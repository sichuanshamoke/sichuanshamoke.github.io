<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="超分辨率," />





  <link rel="alternate" href="/atom.xml" title="沙漠客的学习驿站" type="application/atom+xml" />






<meta name="description" content="文献集合 之 Efficient Network &#x2F; Mechanism Design Methods 来自综述文章：《A systematic survey of deep learning-based single-image super-resolution》，2024,4月 文章链接：A Systematic Survey of Deep Learning-Based Single-Ima">
<meta property="og:type" content="article">
<meta property="og:title" content="文献集合 之 Efficient Network &#x2F; Mechanism Design Methods">
<meta property="og:url" content="http://example.com/2025/03/11/%E6%96%87%E7%8C%AE%E9%9B%86%E5%90%88-%E4%B9%8B-Efficient-Network-Mechanism-Design-Methods/index.html">
<meta property="og:site_name" content="沙漠客的学习驿站">
<meta property="og:description" content="文献集合 之 Efficient Network &#x2F; Mechanism Design Methods 来自综述文章：《A systematic survey of deep learning-based single-image super-resolution》，2024,4月 文章链接：A Systematic Survey of Deep Learning-Based Single-Ima">
<meta property="og:locale">
<meta property="og:image" content="https://s21.ax1x.com/2025/03/11/pEN7HHK.png">
<meta property="og:image" content="https://s21.ax1x.com/2025/03/11/pEN7rXq.png">
<meta property="og:image" content="https://s21.ax1x.com/2025/03/11/pENHk4g.png">
<meta property="og:image" content="https://s21.ax1x.com/2025/03/11/pENbcYF.png">
<meta property="og:image" content="https://s21.ax1x.com/2025/03/11/pENbwyn.png">
<meta property="og:image" content="https://s21.ax1x.com/2025/03/11/pENbxmt.png">
<meta property="og:image" content="https://s21.ax1x.com/2025/03/12/pEUmcbq.png">
<meta property="article:published_time" content="2025-03-11T08:05:03.000Z">
<meta property="article:modified_time" content="2025-03-12T11:36:33.071Z">
<meta property="article:author" content="Shamoke">
<meta property="article:tag" content="超分辨率">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s21.ax1x.com/2025/03/11/pEN7HHK.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/2025/03/11/文献集合-之-Efficient-Network-Mechanism-Design-Methods/"/>





  <title>文献集合 之 Efficient Network / Mechanism Design Methods | 沙漠客的学习驿站</title>
  








<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">
  

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <a target="_blank" rel="noopener" href="https://github.com/sichuanshamoke" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">沙漠客的学习驿站</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Studying and Recording</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            日程表
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>
    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/03/11/%E6%96%87%E7%8C%AE%E9%9B%86%E5%90%88-%E4%B9%8B-Efficient-Network-Mechanism-Design-Methods/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="沙漠客">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="沙漠客的学习驿站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">文献集合 之 Efficient Network / Mechanism Design Methods</h1>
        

        <div class="post-meta">
          
          
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2025-03-11T16:05:03+08:00">
                2025-03-11
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2025-03-12T19:36:33+08:00">
                2025-03-12
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BB%BC%E8%BF%B0%E7%B1%BB/" itemprop="url" rel="index">
                    <span itemprop="name">综述类</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2025/03/11/%E6%96%87%E7%8C%AE%E9%9B%86%E5%90%88-%E4%B9%8B-Efficient-Network-Mechanism-Design-Methods/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2025/03/11/%E6%96%87%E7%8C%AE%E9%9B%86%E5%90%88-%E4%B9%8B-Efficient-Network-Mechanism-Design-Methods/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  28k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1:41
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="文献集合-之-Efficient-Network-Mechanism-Design-Methods"><a href="#文献集合-之-Efficient-Network-Mechanism-Design-Methods" class="headerlink" title="文献集合 之 Efficient Network / Mechanism Design Methods"></a>文献集合 之 Efficient Network / Mechanism Design Methods</h1><blockquote>
<p>来自综述文章：《A systematic survey of deep learning-based single-image super-resolution》，2024,4月</p>
<p>文章链接：<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3659100">A Systematic Survey of Deep Learning-Based Single-Image Super-Resolution | ACM Computing Surveys</a></p>
</blockquote>
<span id="more"></span>
<p>前言：2014年，Dong等人[ 38 ]提出了超分辨率卷积神经网络( Super-resolution Convolutional Neural Network，SRCNN，ECCV )。SRCNN是第一个基于CNN的SISR模型。</p>
<h2 id="Residual-Learning"><a href="#Residual-Learning" class="headerlink" title="Residual Learning"></a><strong>Residual Learning</strong></h2><blockquote>
<p>在SRCNN中，研究人员发现通过增加更多的卷积层来增加感受野可以获得更好的结果。然而，直接堆叠层会导致梯度消失/爆炸和退化问题[64]。同时，增加层数会导致更高的训练误差和更昂贵的计算成本。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">引用符号</th>
<th style="text-align:left"><strong>年份</strong></th>
<th style="text-align:left">人物</th>
<th style="text-align:left">标题</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">65</td>
<td style="text-align:left">2016(CVPR)</td>
<td style="text-align:left">Kaiming He</td>
<td style="text-align:left">Deep residual learning for image recognition</td>
<td style="text-align:left">在 ResNet 中，He 等提出了残差学习框架，期望学习残差映射而非拟合整个潜在映射。在 SISR 中，由于 LR 图像和 HR 图像共享大部分相同信息，很容易对 LR 和 HR 图像之间的残差图像进行显式建模。残差学习使网络可以更深，并缓解了梯度消失和退化问题</td>
</tr>
<tr>
<td style="text-align:left">83</td>
<td style="text-align:left">2016(CVPR)</td>
<td style="text-align:left">Jiwon Kim</td>
<td style="text-align:left">Accurate image super-resolution using very deep convolutional networks</td>
<td style="text-align:left">借助残差学习，Kim 等提出了一种非常深的超分辨率网络，也称为 VDSR</td>
</tr>
<tr>
<td style="text-align:left">108</td>
<td style="text-align:left">2017(CVPRW)</td>
<td style="text-align:left">Bee Lim</td>
<td style="text-align:left">Enhanced deep residual networks for single image super-resolution</td>
<td style="text-align:left">Lim 等指出，在 SISR 任务中，批归一化层消耗更多内存但不会提高模型性能，因此在 SISR 任务中，批归一化层通常被去除</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Global-and-Local-Residual-Learning"><a href="#Global-and-Local-Residual-Learning" class="headerlink" title="Global and Local Residual Learning"></a>Global and Local Residual Learning</h3><blockquote>
<p>全局残差学习是一种从输入到最终重建层的跳跃连接，有助于提高信息从输入到输出的传递，在一定程度上减少了信息的丢失。然而，随着网络的深入，大量的图像细节在经过如此多的层后不可避免地会丢失。因此，提出了局部残差学习，它在每几个堆叠层中执行，而不是从输入到输出。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>引用符号</th>
<th>年份</th>
<th>人物</th>
<th>标题</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>98</td>
<td>2018(ECCV)</td>
<td>Juncheng Li</td>
<td>Multi-scale Residual Network for Image Super-Resolution</td>
<td style="text-align:left">在这种方法中，形成了多路径模式，并携带了丰富的图像细节，同时也有助于梯度流。此外，许多新的特征提取模块引入了局部残差学习，以增强强大的学习能力</td>
</tr>
<tr>
<td>225</td>
<td>2018(ECCV)</td>
<td>Yulun Zhang</td>
<td>Image Super-Resolution Using Very Deep Residual Channel Attention Networks</td>
<td style="text-align:left">在这种方法中，形成了多路径模式，并携带了丰富的图像细节，同时也有助于梯度流。此外，许多新的特征提取模块引入了局部残差学习，以增强强大的学习能力 ；结合局部残差学习和全局残差学习在当前也非常流行</td>
</tr>
<tr>
<td>91</td>
<td>2017(CVPR)</td>
<td>Christian Ledig</td>
<td>Photo-realistic single image super-resolution using a generative adversarial network</td>
<td style="text-align:left">结合局部残差学习和全局残差学习在当前也非常流行</td>
</tr>
<tr>
<td>108</td>
<td>2017(CVPRW)</td>
<td>Bee Lim</td>
<td>Enhanced deep residual networks for single image super-resolution</td>
<td style="text-align:left">结合局部残差学习和全局残差学习在当前也非常流行</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Residual-Scaling"><a href="#Residual-Scaling" class="headerlink" title="Residual Scaling"></a>Residual Scaling</h3><blockquote>
<p>在 EDSR 中，Lim 等发现增加特征图（即通道维度）超过一定水平会使训练过程数值不稳定。为解决此问题，他们采用了残差缩放技术，在将残差添加到主路径之前，将其乘以 0 到 1 之间的常数进行缩放</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>引用符号</th>
<th>年份</th>
<th>人物</th>
<th>标题</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>156</td>
<td>2017(AAAI)</td>
<td>Christian Szegedy</td>
<td>Inception-v4, inception-resnet and the impact of residual connections on learning</td>
<td>在 EDSR 中，Lim 等发现增加特征图（即通道维度）超过一定水平会使训练过程数值不稳定。为解决此问题，他们采用了残差缩放技术，在将残差添加到主路径之前，将其乘以 0 到 1 之间的常数进行缩放</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Dense-Connection"><a href="#Dense-Connection" class="headerlink" title="Dense Connection"></a>Dense Connection</h2><blockquote>
<p>DenseNet [ 72,CVPR ]中提出了一种密集连接机制，近年来被广泛应用于计算机视觉任务中。与只将层次特征发送到最终重构层的结构不同，密集块中的每一层都接收到前面所有层的特征(图6 )。在大多数层之间创建的短路径可以帮助缓解梯度消失/爆炸的问题，并加强通过层的深层信息流，从而进一步提高重建精度。在密集连接机制的帮助下，网络不同深度之间的信息流可以被充分利用，从而产生更好的重建结果。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>引用符号</th>
<th>年份</th>
<th>人物</th>
<th>标题</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>162</td>
<td>2017(ICCV）</td>
<td>Tong Tong</td>
<td>Image super-resolution using dense skip connections</td>
<td>受密集连接机制的启发，Tong 等提出了 SRDenseNet。SRDenseNet 不仅使用层级密集连接，还使用块级密集连接，每个密集块的输出通过密集连接相连。通过这种方式，将低层特征和高层特征结合起来，并充分利用这些特征进行重建。</td>
</tr>
<tr>
<td>228</td>
<td>2018(CVPR)</td>
<td>Yulun Zhang</td>
<td>Residual dense network for image super-resolution</td>
<td>在 RDN 中，密集连接与残差学习相结合，形成了残差密集块（RDB），它允许低频特征通过多个跳跃连接绕过，使主分支专注于学习高频信息</td>
</tr>
<tr>
<td>159</td>
<td>2017(CVPR)</td>
<td>Ying Tai</td>
<td>Memnet: A persistent memory network for image restoration</td>
<td>密集连接也应用于 MemNet 等模型中</td>
</tr>
<tr>
<td>131</td>
<td>2019(IET Image Processing)</td>
<td>Kangfu Mei</td>
<td>Deep residual refining based pseudo-multi-frame network for effective single image super-resolution</td>
<td>RPMNet同上</td>
</tr>
<tr>
<td>150</td>
<td>2019(Multimedia Tools and Applications)</td>
<td>Mingyu Shen</td>
<td>Multipath feedforward network for single image super-resolution</td>
<td>MFNet同上</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Recursive-Learning"><a href="#Recursive-Learning" class="headerlink" title="Recursive Learning"></a>Recursive Learning</h2><blockquote>
<p>为了在不增加模型参数的情况下获得较大的感受野，针对SISR提出了递归学习，在网络中重复应用相同的子模块，并共享相同的参数。换句话说，递归块是递归单元的集合，其中这些递归单元之间的对应结构共享相同的参数。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>引用符号</th>
<th>年份</th>
<th>人物</th>
<th>标题</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>84</td>
<td>2016(CVPR)</td>
<td>Jiwon Kim</td>
<td>Deeply-recursive convolutional network for image super-resolution</td>
<td>为了在不增加模型参数的情况下获得大的感受野，递归学习被提出用于 SISR，例如在 DRCN 中，相同的卷积层被应用 16 次，得到了 41×41 大小的感受野</td>
</tr>
<tr>
<td>158</td>
<td>2017(CVPR)</td>
<td>Ying Tai</td>
<td>Image super-resolution via deep recursive residual network</td>
<td>然而，递归学习模型中过多的堆叠层仍然会导致梯度消失/爆炸的问题。在 DRRN 中，递归块基于残差学习进行，</td>
</tr>
<tr>
<td>159</td>
<td>2017(CVPR)</td>
<td>Ying Tai</td>
<td>Memnet: A persistent memory network for image restoration</td>
<td>近年来，越来越多的模型在递归单元中引入了残差学习策略，如 MemNet</td>
</tr>
<tr>
<td>3</td>
<td>2018(ECCV)</td>
<td>Namhyuk Ahn</td>
<td>Fast, accurate, and lightweight super-resolution with cascading residual network</td>
<td>近年来，越来越多的模型在递归单元中引入了残差学习策略，如 CARN</td>
</tr>
<tr>
<td>99</td>
<td>2019(ICCVW)</td>
<td>Juncheng Li</td>
<td>Lightweight and Accurate Recursive Fractal Network for Image Super-Resolution</td>
<td>近年来，越来越多的模型在递归单元中引入了残差学习策略，如 SRRFN</td>
</tr>
</tbody>
</table>
</div>
<p><img src="https://s21.ax1x.com/2025/03/11/pEN7HHK.png" alt="pEN7HHK.png"></p>
<h2 id="Progressive-Learning"><a href="#Progressive-Learning" class="headerlink" title="Progressive Learning"></a>Progressive Learning</h2><blockquote>
<p>渐进式学习是指逐步增加学习任务的难度。对于一些序列预测任务或顺序决策问题，使用渐进式学习来减少训练时间，提高泛化性能。由于SISR是一个病态问题，由于一些不利条件，如大尺度因子、未知退化核和噪声，总是面临很大的学习困难，因此适合利用渐进式学习来简化学习过程，提高重构效率。<br>在渐进学习的帮助下，复杂问题可以被分解为多个简单任务，从而加速模型收敛并获得更好的重建结果。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>引用符号</th>
<th>年份</th>
<th>人物</th>
<th>标题</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>87</td>
<td>2017(CVPR)</td>
<td>Wei-Sheng Lai</td>
<td>Deep laplacian pyramid networks for fast and accurate super-resolution</td>
<td>在 LapSRN 中，该方法被应用于逐步重建高分辨率图像的子带残差</td>
</tr>
<tr>
<td>180</td>
<td>2018(CVPRW)</td>
<td>Yifan Wang</td>
<td>A fully progressive approach to single-image super-resolution</td>
<td>在 ProSR 中，金字塔的每一层逐渐融合，以减少对先前训练层的影响，并且逐步添加每个尺度的训练对</td>
</tr>
<tr>
<td>106</td>
<td>2019(CVPR)</td>
<td>Zhen Li</td>
<td>Feedback network for image super-resolution</td>
<td>在 SRFBN 中，该策略被应用于解决复杂的退化任务，对不同难度的目标进行排序，以便进行渐进式学习。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Multi-scale-Learning"><a href="#Multi-scale-Learning" class="headerlink" title="Multi-scale Learning"></a>Multi-scale Learning</h2><blockquote>
<p>大量研究工作[ 29、87、157]指出，图像在不同尺度下可能表现出不同的特征，充分利用这些特征可以进一步提高模型性能</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>引用符号</th>
<th>年份</th>
<th>人物</th>
<th>标题</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>29</td>
<td>2017(CVPR)</td>
<td>François Chollet</td>
<td>Xception: Deep learning with depthwise separable convolutions</td>
<td>大量研究工作[ 29、87、157]指出，图像在不同尺度下可能表现出不同的特征，充分利用这些特征可以进一步提高模型性能</td>
</tr>
<tr>
<td>87</td>
<td>2017(CVPR)</td>
<td>Wei-Sheng Lai</td>
<td>Deep laplacian pyramid networks for fast and accurate super-resolution</td>
<td>同上</td>
</tr>
<tr>
<td>157</td>
<td>2016(CVPR)</td>
<td>Christian Szegedy</td>
<td>Rethinking the inception architecture for computer vision</td>
<td>同上</td>
</tr>
<tr>
<td>98</td>
<td>2018(ECCV)</td>
<td>Juncheng Li</td>
<td>Multi-scale Residual Network for Image Super-Resolution</td>
<td>受Inception模块的启发[ 29 ]，Li 等提出了一种多尺度残差块（MSRB）用于特征提取。MSRB 在一个块中集成了不同的卷积核，以自适应地提取不同尺度的图像特征</td>
</tr>
<tr>
<td>97</td>
<td>2020(IEEE Transactions on Circuits and Systems for Video Technology)</td>
<td>Juncheng Li</td>
<td>MDCN: Multi-scale dense cross network for image super-resolution</td>
<td>Li 等进一步优化结构，提出了更精确的多尺度密集交叉块（MDCB）用于特征提取。MDCB 本质上是一个双路径密集网络，可以有效地检测局部和多尺度特征</td>
</tr>
<tr>
<td>141</td>
<td>2020(Neurocomputing)</td>
<td>Jinghui Qin</td>
<td>Multi-scale feature fusion residual network for Single Image Super-Resolution</td>
<td>Qin 等提出了多尺度特征融合残差网络（MSFFRN），以充分利用图像特征进行 SISR</td>
</tr>
<tr>
<td>17</td>
<td>2019(ICASSP)</td>
<td>Chia-Yang Chang</td>
<td>Multi-scale dense network for single-image super-resolution</td>
<td>Chang 等提出了多尺度密集网络（MSDN），将多尺度学习与密集连接相结合</td>
</tr>
<tr>
<td>15</td>
<td>2019(Neurocomputing)</td>
<td>Feilong Cao</td>
<td>Single image super-resolution via multi-scale residual channel attention network</td>
<td>Cao 等开发了一种新的超分辨率方法，称为多尺度残差通道注意力网络（MSRCAN），将通道注意力机制引入到 MSRB 中</td>
</tr>
</tbody>
</table>
</div>
<p><img src="https://s21.ax1x.com/2025/03/11/pEN7rXq.png" alt="pEN7rXq.png"></p>
<h2 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h2><blockquote>
<p>注意力机制可以被认为是一种工具，可以将可用的资源分配给输入中信息最丰富的部分。为了提高学习过程中的效率，提出了一些工作来引导网络更加关注感兴趣的区域。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>引用符号</th>
<th>年份</th>
<th>人物</th>
<th>标题</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>69</td>
<td>2018(CVPR)</td>
<td>Jie Hu</td>
<td>Squeeze-and-excitation networks</td>
<td>Hu 等提出了挤压激励（SE）块，用于在图像分类任务中建模通道间关系</td>
</tr>
<tr>
<td>174</td>
<td>2018(CVPR)</td>
<td>Xiaolong Wang</td>
<td>Non-local neural networks</td>
<td>Wang等人通过结合非局部操作，提出了一种用于视频分类的非局部注意力神经网络</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Channel-Attention"><a href="#Channel-Attention" class="headerlink" title="Channel Attention"></a>Channel Attention</h3><blockquote>
<p>在SISR中，我们主要是想尽可能多地恢复出有价值的高频信息。然而，常见的基于CNN的方法平等地对待通道特征，在处理不同类型的信息时缺乏灵活性。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>引用符号</th>
<th>年份</th>
<th>人物</th>
<th>标题</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>132</td>
<td>2018(NeurIPS)</td>
<td>Kangfu Mei</td>
<td>An Effective Single-Image Super-Resolution Model Using Squeeze-and-Excitation Networks</td>
<td>许多方法在 SISR 模型中引入了 SE 机制</td>
</tr>
<tr>
<td>225</td>
<td>2018(ECCV)</td>
<td>Yulun Zhang</td>
<td>Image Super-Resolution Using Very Deep Residual Channel Attention Networks</td>
<td>Zhang 等提出了一种基于 SE 机制的新模块，名为残差通道注意力块（RCAB）。通过全局平均池化层和 Sigmoid 函数对每个特征通道进行重新缩放，使网络能够专注于更有用的通道，增强判别学习能力</td>
</tr>
<tr>
<td>33</td>
<td>2019(CVPR)</td>
<td>Tao Dai</td>
<td>Second-order attention network for single image super-resolution</td>
<td>在 SAN 中，探索了特征的二阶统计量，以基于协方差归一化进行注意力机制。大量实验表明，二阶通道注意力可以帮助网络获得更具判别力的表示，从而提高重建精度</td>
</tr>
</tbody>
</table>
</div>
<p><img src="https://s21.ax1x.com/2025/03/11/pENHk4g.png" alt="pENHk4g.png"></p>
<p><img src="https://s21.ax1x.com/2025/03/11/pENbcYF.png" alt="pENbcYF.png"></p>
<h3 id="Non-Local-Attention"><a href="#Non-Local-Attention" class="headerlink" title="Non-Local Attention"></a>Non-Local Attention</h3><blockquote>
<p>基于CNN的方法在局部感受野内进行卷积时，忽略了该区域外的上下文信息，而较远区域的特征可能具有较高的相关性，能够提供有效的信息。考虑到这一问题，非局部注意力被提出作为一种滤波算法来计算图像所有像素的加权平均值。通过这种方式，远处的像素也可以对关注位置的响应做出贡献。</p>
<p><strong>Non-Local Attention</strong> 是一种用于捕捉图像中长距离依赖关系的机制，旨在解决传统卷积神经网络（CNN）因局部感受野限制而忽略全局上下文信息的问题。其核心思想是通过计算图像中所有像素之间的相关性（无论远近），生成全局注意力权重，从而允许每个位置的响应不仅受局部区域影响，还能整合整个图像中具有高相关性的远距离信息。</p>
<ol>
<li><strong>全局关联性建模</strong>：对于输入特征图中的每个位置，计算其与所有其他位置的相似性（如通过点积、高斯函数等），生成一个全局注意力图。</li>
<li><strong>加权聚合</strong>：根据注意力权重，对所有位置的特征进行加权平均，使当前位置的特征融合全局上下文信息。</li>
<li><strong>残差连接</strong>：通常将加权后的特征与原始特征相加，保留局部细节的同时增强全局感知能力。</li>
</ol>
<p>参考博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_35975753/article/details/108801408">NON-LOCAL-注意力机制_nonlocal注意力机制-CSDN博客</a></p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>引用符号</th>
<th>年份</th>
<th>人物</th>
<th>标题</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>113</td>
<td>2018(NeurIPS)</td>
<td>Ding Liu</td>
<td>Non-local recurrent network for image restoration</td>
<td>在 NLRN 中，非局部操作在有限的邻域内进行，以提高鲁棒性</td>
</tr>
<tr>
<td>227</td>
<td>2019(arXiv)</td>
<td>Yulun Zhang</td>
<td>Residual non-local attention networks for image restoration</td>
<td>在 RNAN 中，提出了非局部注意力块，在其掩码分支中同时使用通道和空间维度的注意力机制，以更好地指导主干分支中的特征提取</td>
</tr>
<tr>
<td>138</td>
<td>2020(ECCV)</td>
<td>Ben Niu</td>
<td>Single image super-resolution via a holistic attention network</td>
<td>在HAN中提出了一个整体注意力网络，该网络由层注意力模块和通道-空间注意力模块组成，用于建模层、通道和位置之间的整体相互依赖关系。</td>
</tr>
<tr>
<td>134</td>
<td>2020(CVPR)</td>
<td>Yiqun Mei</td>
<td>Image super-resolution with cross-scale non-local attention and exhaustive self-exemplars mining</td>
<td>在 CSNLN 中，提出了跨尺度非局部注意力模块，用于挖掘同一特征图中 LR 特征和大尺度 HR 块之间的长程依赖关系</td>
</tr>
<tr>
<td>197</td>
<td>2022(AAAI)</td>
<td>Bin Xia</td>
<td>Efficient non-local contrastive attention for image super-resolution</td>
<td>为了减轻非局部注意力造成的噪声污染，ENLCA 利用高效的非局部衰减和稀疏聚合，通过对比学习专注于有用信息，分离无关特征</td>
</tr>
</tbody>
</table>
</div>
<p><img src="https://s21.ax1x.com/2025/03/11/pENbwyn.png" alt="pENbwyn.png"></p>
<h2 id="Feedback-Mechanism"><a href="#Feedback-Mechanism" class="headerlink" title="Feedback Mechanism"></a>Feedback Mechanism</h2><blockquote>
<p>反馈机制是指将输出的概念带到先前的状态，允许模型有一个自我修正的过程。值得注意的是，反馈机制不同于递归学习，因为在反馈机制中，模型参数保持自校正且不共享。近年来，反馈机制被广泛应用于许多计算机视觉任务[ 14、16 ]中，这也有利于SR图像重建。具体来说，反馈机制允许网络将高层信息带回前几层，并细化低层信息，从而充分引导LR图像恢复出高质量的SR图像。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>引用符号</th>
<th>年份</th>
<th>人物</th>
<th>标题</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>14</td>
<td>2015(ICCV)</td>
<td>Chunshui Cao</td>
<td>Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks</td>
<td></td>
</tr>
<tr>
<td>16</td>
<td>2016(CVPR)</td>
<td>Joao Carreira</td>
<td>Human pose estimation with iterative error feedback</td>
<td></td>
</tr>
<tr>
<td>62</td>
<td>2020(IEEE Transactions on Pattern Analysis and Machine Intelligence)</td>
<td>Muhammad Haris</td>
<td>Deep back-projectinetworks for single image super-resolution</td>
<td>在 DBPN 中，提供了迭代的上下采样层，以实现每个阶段投影误差的误差反馈机制</td>
</tr>
<tr>
<td>61</td>
<td>2018(CVPR)</td>
<td>Wei Han</td>
<td>Image super-resolution via dual-state recurrent networks</td>
<td>在 DSRN 中，提出了双状态递归网络，其中递归信号通过延迟反馈在两个状态之间双向交换</td>
</tr>
<tr>
<td>106</td>
<td>2019(CVPR)</td>
<td>Zhen Li</td>
<td>Feedback network for image super-resolution</td>
<td>在 SRFBN 中，提出了反馈块，每次迭代的输入是上一次的输出作为反馈信息。随后是几个具有密集跳跃连接的投影组，低级表示被细化并成为更强大的高级表示</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Gating-Mechanism"><a href="#Gating-Mechanism" class="headerlink" title="Gating Mechanism"></a>Gating Mechanism</h2><blockquote>
<p>上述残差学习中的跳跃连接往往会使得输出特征的通道维度极高。如果这样的高维通道在后续层中保持不变，那么计算代价将非常大，因此会影响重建效率和性能。直观上，跳跃连接后的输出特征应该被有效拒绝，而不是简单的拼接。</p>
<p>为了解决这个问题，研究人员建议使用门控机制来自适应地提取和学习更有效的信息。大多数情况下，采用1 × 1的卷积层来完成门控机制，可以降低通道维度并留下更多的有效信息。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>引用符号</th>
<th>年份</th>
<th>人物</th>
<th>标题</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>162</td>
<td>2017(ICCV)</td>
<td>Tong Tong</td>
<td>Image super-resolution using dense skip connections</td>
<td>在 SRDenseNet 中，1×1 卷积层在重建模块前作为瓶颈层</td>
</tr>
<tr>
<td>98</td>
<td>2018(ECCV)</td>
<td>Juncheng Li</td>
<td>Multi-scale Residual Network for Image Super-Resolution</td>
<td>在 MSRN 中，1×1 卷积层在重建模块前作为瓶颈层</td>
</tr>
<tr>
<td>159</td>
<td>2017(CVPR)</td>
<td>Ying Tai</td>
<td>Memnet: A persistent memory network for image restoration</td>
<td>在 MemNet 中，它是每个内存块末尾的门单元，用于控制长期记忆和短期记忆的权重，且全局和局部区域都使用了门控机制。</td>
</tr>
<tr>
<td>4</td>
<td>2018(CVPRW)</td>
<td>Namhyuk Ahn</td>
<td>Image super-resolution via progressive cascading residual network</td>
<td>值得注意的是，门不仅能够作为瓶颈放置在网络的末端，而且能够在网络中持续进行。在CARN 中，全局和局部区域都使用了门控机制。</td>
</tr>
<tr>
<td>97</td>
<td>2020(IEEE Transactions on Circuits and Systems for Video Technology)</td>
<td>Juncheng Li</td>
<td>MDCN: Multi-scale dense cross network for image super-resolution</td>
<td>门控机制也可以结合其他操作，如注意力机制，构建更有效的门模块，实现特征蒸馏。Li 等提出了分层特征蒸馏块(HFDB)，将 1×1 卷积层和注意力机制相结合。</td>
</tr>
</tbody>
</table>
</div>
<p><img src="https://s21.ax1x.com/2025/03/11/pENbxmt.png" alt="pENbxmt.png"></p>
<h2 id="Efficient-Structure"><a href="#Efficient-Structure" class="headerlink" title="Efficient Structure"></a><strong>Efficient Structure</strong></h2><blockquote>
<p>毫无疑问，增加模型的深度是提高模型性能最简单的方法。然而，由于深大模型的计算开销巨大，难以应用于计算能力有限的移动设备。为了解决这个问题，近年来越来越多的轻量高效的SISR方法被提出。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>引用符号</th>
<th>年份</th>
<th>人物</th>
<th>标题</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>2018(ECCV)</td>
<td>Ahn</td>
<td>Fast, accurate, and lightweight super-resolution with cascading residual network</td>
<td>Ahn 等设计了一种架构（CARN），在残差网络上实现了级联机制，实现了快速、准确和轻量级的超分辨率</td>
</tr>
<tr>
<td>75</td>
<td>2018(CVPR)</td>
<td>Hui</td>
<td>Fast and accurate single image super-resolution via information distillation network</td>
<td>Hui等人利用信息蒸馏策略提出了一种具有轻量级参数和计算复杂度的新型信息蒸馏网络( Information Distillation Network，IDN )。<br>注：深度学习中的蒸馏网络（Knowledge Distillation）是一种通过知识迁移实现模型压缩和性能提升的技术。其核心思想是让轻量级学生模型（Student）模仿复杂教师模型（Teacher）的行为，从而在保持或接近教师模型性能的同时降低计算和存储成本</td>
</tr>
<tr>
<td>74</td>
<td>2019(ACMMM)</td>
<td>Hui</td>
<td>Lightweight image super-resolution with information multi-distillation network</td>
<td>Hui 等通过构建级联信息多蒸馏块，进一步提出了轻量级信息多蒸馏网络（IMDN）</td>
</tr>
<tr>
<td>114</td>
<td>2020(ECCVW)</td>
<td>Liu</td>
<td>Residual feature distillation network for lightweight image super-resolution</td>
<td>Liu 等提出了 RFDN，通过结合更轻量级的特征蒸馏连接操作，提高了单图像超分辨率（SISR）的效率</td>
</tr>
<tr>
<td>234</td>
<td>2022(ECCV)</td>
<td>Zhou</td>
<td><strong>Efficient image super-resolution using vast-receptive-field attention</strong></td>
<td>Zhou 等开发了 VapSR，通过优化注意力机制创建了一个更高效的超分辨率网络。<br>注：前置模型：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.01073">[PAN] 使用像素注意力实现高效的图像超分辨率</a>,，ECCV-2020<br>参考博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/amusi1994/article/details/127564103">VapSR：基于超大感受野注意力的超分辨率模型</a></td>
</tr>
<tr>
<td>155</td>
<td>2022(NeurIPS）</td>
<td>Li</td>
<td><strong>Shufflemixer: An efficient convnet for image super-resolution</strong></td>
<td>Li 等引入了 ShuffleMixer，一种研究使用大卷积和通道分割混洗操作的技术，使网络更适合移动设备<br>参考博客：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/523284090">shuffleMixer图像超分论文阅读 - 知乎</a></td>
</tr>
<tr>
<td>105</td>
<td>2022(CVPR)</td>
<td>Li</td>
<td><strong>Blueprint separable residual network for efficient image super-resolution</strong></td>
<td>Li 等提出了 Blueprint Separable Residual Network（BSRN），包含两种高效设计：蓝图可分离卷积和更有效的注意力模块</td>
</tr>
<tr>
<td>101</td>
<td>2023(IEEE Transactions on Multimedia)</td>
<td>Li</td>
<td><strong>Cross-receptive Focused Inference Network for lightweight image super-resolution</strong></td>
<td>CFIN：Li等人[ 101 ]提出了一种新颖的交叉感受野引导变换( Cross-receptive Field Guided Transformer，CFGT )，通过使用调制卷积核来选择重建所需的上下文信息。</td>
</tr>
<tr>
<td>121</td>
<td>2022(ACMMM)</td>
<td>Luo</td>
<td>Adjustable Memory-efficient Image Super-resolution via Individual Kernel Sparsity</td>
<td>Luo 等提出了 Individual Kernel Sparsity（IKS）方法，用于内存高效且可调整稀疏性的图像超分辨率，使深度网络能够部署在内存受限的设备上</td>
</tr>
<tr>
<td>204</td>
<td>2023(ACMMM)</td>
<td>Ye</td>
<td>Hardware-friendly Scalable Image Super Resolution with Progressive Structured Sparsity</td>
<td>Ye 等提出了 Hardware-friendly Scalable SR（HSSR），具有渐进式结构稀疏性。该模型可以通过单个可扩展模型覆盖多个不同大小的 SR 模型，无需额外的重新训练或后处理</td>
</tr>
<tr>
<td>109</td>
<td>2023(CVPR)</td>
<td>Lin</td>
<td>Memory-friendly Scalable Super-resolution via Rewinding Lottery Ticket Hypothesis</td>
<td>Lin 等提出了通过通过回溯彩票假设实现内存友好型可扩展超分辨率的Memory-friendly Scalable dynamic SR（MSSR）轻量级模型，可以轻松推广到不同的 SR 模型</td>
</tr>
<tr>
<td>28</td>
<td>2023(CVPR)</td>
<td>Choi</td>
<td><strong>N-gram in swin transformers for efficient lightweight image super-resolution</strong></td>
<td>Choi 等引入了 NGswin，通过拓宽基于窗口的自注意力方法的感受野来提高 SISR 的性能</td>
</tr>
<tr>
<td>166</td>
<td>2023(CVPR)</td>
<td>Wang</td>
<td><strong>Omni aggregation networks for lightweight image super-resolution</strong></td>
<td>Wang 等提出了 Omni - SR，通过在空间和通道维度上复制像素交互来增强轻量级模型的能力</td>
</tr>
<tr>
<td>102</td>
<td>2023(ICCV)</td>
<td>Li</td>
<td><strong>DLGSANet: lightweight dynamic local and global self-attention networks for image super-resolution</strong></td>
<td>Li 等引入了 DLGSANet，通过采用稀疏全局自注意力模块来确定最相关的相似值，简化了 SISR 的效率</td>
</tr>
</tbody>
</table>
</div>
<p>知识蒸馏模型（distillation network）：</p>
<p>参考博客：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/319880839">深度学习中的知识蒸馏技术 - 知乎</a></p>
<blockquote>
<p>整体架构：</p>
<p>图中有两个主要的神经网络模型：“Teacher”（教师模型）和 “Student”（学生模型）。教师模型是一个已经预训练（pre - trained）好的复杂模型，而学生模型是一个待训练（to be trained）的相对简单的模型，二者同时接收训练数据（Training data）。</p>
<p>具体流程：</p>
<ol>
<li><strong>教师模型输出</strong>：教师模型对训练数据进行处理，输出 “soft labels”（软标签），也叫预测（predictions）。软标签不仅包含了模型预测的类别，还包含了各个类别之间的概率关系等更丰富的信息，即蒸馏知识（distilled knowledge）。</li>
<li><strong>学生模型学习</strong>：学生模型接收同样的训练数据，并在学习过程中，不仅参考真实标签（true label，也叫 hard labels 硬标签），还借鉴教师模型输出的软标签中的信息。通过这种方式，学生模型可以更快地学习，并且在一定程度上达到与教师模型相近的性能，同时模型结构更简单，计算成本更低 。</li>
</ol>
<p>知识蒸馏技术常用于在资源受限的场景下，比如在移动设备或嵌入式设备上部署模型时，通过让轻量级的学生模型学习大型教师模型的知识，来实现高效的模型推理。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pEUmcbq"><img src="https://s21.ax1x.com/2025/03/12/pEUmcbq.png" alt="pEUmcbq.png"></a></p>
<h2 id="Transformer-based-Method（✪）"><a href="#Transformer-based-Method（✪）" class="headerlink" title="Transformer-based Method（✪）"></a><strong>Transformer-based Method</strong>（✪）</h2><blockquote>
<p>Transformer的核心思想是”自注意力”机制，它可以捕获序列元素之间的长期信息。最近，Transformer [ 164 ] 在NLP任务中取得了辉煌的成绩。例如，预训练的深度学习模型(例如, BERT[35]  , GPT [144] )已经显示出优于传统方法的有效性。受此启发，越来越多的研究人员开始探索Transformer在计算机视觉任务中的应用，并在多个任务中取得了突破性的成果。在图像复原中，Transformer常被用来捕捉图像的全局信息，以进一步提高重建图像的质量。</p>
<p>然基于Transformer的方法在性能上有了很大的提升，但是Transform中使用的注意力机制会占用大量的GPU内存。因此，如何进一步降低基于Transformer方法的GPU内存值得进一步探索</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>引用符号</th>
<th>年份</th>
<th>人物</th>
<th>标题</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>164</td>
<td>2017(NeurIPS)</td>
<td>Vaswani</td>
<td>Attention is all you need</td>
<td>Transformer 的关键思想是 “自注意力” 机制，它可以捕获序列元素之间的长期信息</td>
</tr>
<tr>
<td>22</td>
<td>2021(CVPR)</td>
<td>Chen</td>
<td>Pre-trained image processing transformer</td>
<td>近年来，越来越多的基于Transformer的模型被提出。例如，Chen等人提出了在大规模数据集上进行预训练的图像处理转换器( Image Processing Transformer，IPT [ 22 ] )。此外，针对不同的图像处理任务，引入了对比学习。因此，预训练的模型经过微调后可以高效地应用于期望的任务。然而，IPT [ 22 ]依赖于大规模数据集，并且具有大量的参数(超过115 . 5M参数)，这极大地限制了它的应用场景。</td>
</tr>
<tr>
<td>117</td>
<td>2021(CVPR)</td>
<td>Liu</td>
<td>Swin transformer: Hierarchical vision transformer using shifted windows</td>
<td>参考博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_39478403/article/details/120042232">【深度学习】详解 Swin Transformer </a></td>
</tr>
<tr>
<td>107</td>
<td>2021(ICCVW)</td>
<td>Liang</td>
<td>SwinIR: Image restoration using swin transformer</td>
<td>Liang等人在Swin Transformer [ 117 ]的基础上提出了用于图像复原的Swin IR [ 107 ]。具体来说，采用Swin Transformer block ( RSTB )进行特征提取，使用DIV2K + Flickr2K进行训练。</td>
</tr>
<tr>
<td>212</td>
<td>2022(CVPR)</td>
<td>Zamir</td>
<td>Restormer: Efficient transformer for high-resolution image restoration</td>
<td>为了改善SwinIR中不同窗口之间缺乏直接交互的问题。Zamir 等人提出了Restormer，通过在Transformer中嵌入CNN并在多个尺度上进行局部-全局学习来重建高质量的图像。</td>
</tr>
<tr>
<td>27</td>
<td>2022(NeurIPS)</td>
<td>Chen</td>
<td>Cross Aggregation Transformer for Image Restoration</td>
<td>Chen 等提出了 CAT，扩展了注意力区域并跨不同窗口聚合特征</td>
</tr>
<tr>
<td>24</td>
<td>2023(CVPR)</td>
<td>Chen</td>
<td>Activating More Pixels in Image Super-Resolution Transformer</td>
<td>为了激活Transformer关注的更多像素，Chen 等提出了 HAT，通过使用重叠交叉注意力模块结合预训练策略来增强 Transformer 模型的潜力</td>
</tr>
<tr>
<td>103</td>
<td>2023(CVPR)</td>
<td>Li</td>
<td>Efficient and explicit modelling of image hierarchies for image restoration</td>
<td>Li 等提出 <strong>GRL</strong>，通过在 Transformer 中集成各种注意力机制，在全局、区域和局部尺度上显式地对图像层次结构进行建模</td>
</tr>
<tr>
<td>119</td>
<td>2021(CVPRW)</td>
<td>Lu</td>
<td>Transformer for Single Image Super-Resolution</td>
<td>在轻量级SISR模型的应用方面，Lu 等提出了高效超分辨率 Transformer（ESRT），用于快速准确的 SISR，以较少的参数和低计算成本取得了有竞争力的结果</td>
</tr>
<tr>
<td>222</td>
<td>2022(ECCV)</td>
<td>Zhang</td>
<td>Efficient long-range attention network for image super-resolution</td>
<td>Zhang 等提出 ELAN，通过共享自注意力机制降低模型复杂度，加速基于 Transformer 的模型</td>
</tr>
<tr>
<td>191</td>
<td>2022(CVPR)</td>
<td>Wang</td>
<td>Uformer: A general u-shaped transformer for image restoration</td>
<td>Wang 等提出了 Uformer，一种通用且优越的 U 形 Transformer，它可以在捕获局部上下文和多尺度特征的同时，降低高分辨率特征图上的计算复杂度</td>
</tr>
<tr>
<td>212</td>
<td>2022(CVPR)</td>
<td>Zamir</td>
<td>Restormer: Efficient transformer for high-resolution image restoration</td>
<td>Zamir 等提出了一种高效的 Restormer，它可以捕获长距离像素交互，同时适用于大图像</td>
</tr>
<tr>
<td>101</td>
<td>2023(IEEE Transactions on Multimedia)</td>
<td>Li</td>
<td>Cross-receptive Focused Inference Network for lightweight image super-resolution</td>
<td>Li 等提出了 Cross-receptive Focused Inference Network（CFIN），可以结合上下文建模，在有限的计算资源下实现良好的性能</td>
</tr>
<tr>
<td>239</td>
<td>2023(CVPR)</td>
<td>Zhu</td>
<td>Attention Retractable Frequency Fusion Transformer for Image Super Resolution</td>
<td>Zhu 等设计了注意力可伸缩频率融合 Transformer（ARFFT），以增强表示能力并将感受野扩展到整个图像</td>
</tr>
<tr>
<td>100</td>
<td>2023(IEEE Transactions on Circuits and Systems for Video Technology)</td>
<td>Li</td>
<td>Lightweight Image Super-Resolution with Pyramid Clustering Transformer</td>
<td>Li 等提出了一种简洁而强大的金字塔聚类 Transformer 网络（PCTN），用于轻量级 SISR</td>
</tr>
<tr>
<td>26</td>
<td>2023(ICCV)</td>
<td>Chen</td>
<td>Dual aggregation transformer for image super-resolution</td>
<td>Chen 等提出了双聚合 Transformer（DAT）用于 SISR，以块间和块内双重方式跨空间和通道维度聚合特征</td>
</tr>
<tr>
<td>236</td>
<td>2023(ICCV)</td>
<td>Zhou</td>
<td>Srformer: Permuted self-attention for single image super-resolution</td>
<td>Zhou 等提出了 SRFormer，通过有效地整合自注意力通道和空间信息，提升了基于窗口的 Transformer 方法的性能</td>
</tr>
<tr>
<td>218</td>
<td>2024(CVPR)</td>
<td>Zhang</td>
<td>Transcending the Limit of Local Window: Advanced Super-Resolution Transformer with Adaptive Token Dictionary</td>
<td>Zhang 等提出了 ATDSR，通过一组自适应令牌字典丰富了超分辨率 Transformer，从而提高了 SISR 的精度</td>
</tr>
<tr>
<td>120</td>
<td>2024(AAAI)</td>
<td>Luo</td>
<td>AdaFormer: Efficient Transformer with Adaptive Token Sparsification for Image Super-resolution</td>
<td>Luo 等提出的自适应令牌稀疏化 Transformer（AdaFormer）通过结合稀疏性策略加速图像的模型推理</td>
</tr>
</tbody>
</table>
</div>

      
    </div>
    
    
    



    

    

    

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>-------------</div>
    
</div>

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/" rel="tag"><i class="fa fa-tag"></i> 超分辨率</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2025/03/07/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0-SR&Diffusion-model/" rel="next" title="超分辨率第十一章-SR&Diffusion_model">
                <i class="fa fa-chevron-left"></i> 超分辨率第十一章-SR&Diffusion_model
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2025/03/17/%E6%96%87%E7%8C%AE%E9%9B%86%E5%90%88-%E4%B9%8B-Perceptual-Quality-Methods%E3%80%81Information-Utilization-Method/" rel="prev" title="文献集合 之 Perceptual Quality Methods、Information Utilization Method">
                文献集合 之 Perceptual Quality Methods、Information Utilization Method <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>

  </div>
  
  
  
  </article>
  



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">
      
      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/header.gif"
                alt="沙漠客" />
            
              <p class="site-author-name" itemprop="name">沙漠客</p>
              <p class="site-description motion-element" itemprop="description">Action speak louder than words!</p>
          </div>
        <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=100 src="//music.163.com/outchain/player?type=2&id=1396311816&auto=1&height=66"></iframe>
        </iframe>
          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/">
              
                  <span class="site-state-item-count">41</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/sichuanshamoke" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://blog.csdn.net/m0_51960673?spm=1000.2115.3001.5343" target="_blank" title="CSDN">
                      
                        <i class="fa fa-fw fa-fa fa-codiepie"></i>CSDN</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.kaggle.com/sichuanshamoke" target="_blank" title="kaggle">
                      
                        <i class="fa fa-fw fa-stack-overflow"></i>kaggle</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://leetcode.cn/u/amazing-snyderdc5/" target="_blank" title="LeetCode">
                      
                        <i class="fa fa-fw fa-skype"></i>LeetCode</a>
                  </span>
                
            </div>
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <!-- modify icon to fire by szw -->
                <i class="fa fa-history fa-" aria-hidden="true"></i>
                近期文章
              </div>
              <ul class="links-of-blogroll-list">
                
                
                  <li class="recent_posts_li">
                    <a href="/2025/03/17/%E6%96%87%E7%8C%AE%E9%9B%86%E5%90%88-%E4%B9%8B-Perceptual-Quality-Methods%E3%80%81Information-Utilization-Method/" title="文献集合 之 Perceptual Quality Methods、Information Utilization Method" target="_blank">文献集合 之 Perceptual Quality Methods、Information Utilization Method</a>
                  </li>
                
                  <li class="recent_posts_li">
                    <a href="/2025/03/11/%E6%96%87%E7%8C%AE%E9%9B%86%E5%90%88-%E4%B9%8B-Efficient-Network-Mechanism-Design-Methods/" title="文献集合 之 Efficient Network / Mechanism Design Methods" target="_blank">文献集合 之 Efficient Network / Mechanism Design Methods</a>
                  </li>
                
                  <li class="recent_posts_li">
                    <a href="/2025/03/07/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0-SR&Diffusion-model/" title="超分辨率第十一章-SR&Diffusion_model" target="_blank">超分辨率第十一章-SR&Diffusion_model</a>
                  </li>
                
                  <li class="recent_posts_li">
                    <a href="/2024/12/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%85%AD%E7%AB%A0-VAE%E6%9E%B6%E6%9E%84/" title="深度学习第六章-VAE架构" target="_blank">深度学习第六章-VAE架构</a>
                  </li>
                
                  <li class="recent_posts_li">
                    <a href="/2024/12/02/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%AC%AC%E5%8D%81%E7%AB%A0-SR&NormalizingFlow/" title="超分辨率第十章-SR&NormalizingFlow" target="_blank">超分辨率第十章-SR&NormalizingFlow</a>
                  </li>
                
              </ul>
            </div>
          


          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.marxists.org/chinese/maozedong/index.htm" title="五件法宝" target="_blank">五件法宝</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://space.bilibili.com/562700874?spm_id_from=333.1365.0.0" title="风翼飞镰" target="_blank">风翼飞镰</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://mieawentuirba.github.io/" title="风沙侠客" target="_blank">风沙侠客</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%96%87%E7%8C%AE%E9%9B%86%E5%90%88-%E4%B9%8B-Efficient-Network-Mechanism-Design-Methods"><span class="nav-text">文献集合 之 Efficient Network &#x2F; Mechanism Design Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Residual-Learning"><span class="nav-text">Residual Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Global-and-Local-Residual-Learning"><span class="nav-text">Global and Local Residual Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Residual-Scaling"><span class="nav-text">Residual Scaling</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dense-Connection"><span class="nav-text">Dense Connection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Recursive-Learning"><span class="nav-text">Recursive Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Progressive-Learning"><span class="nav-text">Progressive Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-scale-Learning"><span class="nav-text">Multi-scale Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention-Mechanism"><span class="nav-text">Attention Mechanism</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Channel-Attention"><span class="nav-text">Channel Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Non-Local-Attention"><span class="nav-text">Non-Local Attention</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Feedback-Mechanism"><span class="nav-text">Feedback Mechanism</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gating-Mechanism"><span class="nav-text">Gating Mechanism</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Efficient-Structure"><span class="nav-text">Efficient Structure</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer-based-Method%EF%BC%88%E2%9C%AA%EF%BC%89"><span class="nav-text">Transformer-based Method（✪）</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2023 &mdash; <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Shamoke</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">367k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: true,
        appId: 'wsUYJtlLslOYlG6pA3pS9i30-gzGzoHsz',
        appKey: 'd1H2YDHxgJkfTsFw0UhNNMFJ',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

    <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }
 
    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }
 
    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }
 
    .highlight-wrap {
      position: relative;
    }
  </style>
 
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
 
        if(result)$(this).text('复制成功')
        else $(this).text('复制失败')
 
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('复制')
        }, 300)
      }).append(e)
    })
  </script>	
  
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
