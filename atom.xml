<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>懷仁堂</title>
  
  <subtitle>Studying and Recording</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-09-23T13:21:04.982Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Shamoke</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>超分辨率第五章-SRDenseNet</title>
    <link href="http://example.com/2024/09/23/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%AC%AC%E4%BA%94%E7%AB%A0-SRDenseNet/"/>
    <id>http://example.com/2024/09/23/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%AC%AC%E4%BA%94%E7%AB%A0-SRDenseNet/</id>
    <published>2024-09-23T13:21:04.000Z</published>
    <updated>2024-09-23T13:21:04.982Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>超分辨率第四章-VDSR&amp;EDSR</title>
    <link href="http://example.com/2024/09/20/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%AC%AC%E5%9B%9B%E7%AB%A0-VDSR&amp;EDSR/"/>
    <id>http://example.com/2024/09/20/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%AC%AC%E5%9B%9B%E7%AB%A0-VDSR&amp;EDSR/</id>
    <published>2024-09-20T11:21:03.000Z</published>
    <updated>2024-09-23T13:12:19.794Z</updated>
    
    <content type="html"><![CDATA[<h1 id="超分辨率第四章-VDSR-amp-EDSR"><a href="#超分辨率第四章-VDSR-amp-EDSR" class="headerlink" title="超分辨率第四章-VDSR&amp;EDSR"></a>超分辨率第四章-VDSR&amp;EDSR</h1><blockquote><p>VDSR是2016年提出的模型</p><ul><li>论文地址：<a href="https://arxiv.org/abs/1511.04587">Accurate Image Super-Resolution Using Very Deep Convolutional Networks</a></li><li>代码位置：F:\Github下载\pytorch-vdsr-recurrence-main</li></ul><p>EDSR是是SRResNet的增强版本,2017年提出</p></blockquote><span id="more"></span><h2 id="一-VDSR"><a href="#一-VDSR" class="headerlink" title="一.VDSR"></a>一.VDSR</h2><h3 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1.模型介绍"></a>1.模型介绍</h3><ul><li>SRCNN的不足<ul><li>在增加深度之后训练效果较差。</li><li>模型只适用于单个放大因子，若需使用其他放大因子的尺度，需另外训练一个模型。</li></ul></li><li>改进措施<ul><li>基于VGG网络的方法，增强感受野，模型深度达到20层。</li><li>使用残差网络训练模型，避免退化问题。</li><li><strong>使用高学习率（0.1开始）加快收敛速度，并使用一个可调的梯度裁剪，以最大限度地提高速度，同时抑制爆炸梯度。</strong></li><li>SRCNN是针对单一尺度进行训练的，如果需要处理不同尺度的图像，则需要训练多个模型。而VDSR通过训练一个单一的网络来处理多尺度的超分辨率问题，显著降低了参数量并提高了实用性</li></ul></li><li>模型架构：作者使用20个网络层，除第一层和最后一层外，其余层具有相同的类型：64个大小为3x3x64的滤波器，也就是每一层滤波器的输入通道数为64，输出通道数也为64。其中一个滤波器在3*3的空间区域上操作。第一个网络层对输入图像进行操作，最后一个网络层用于图像重建。</li></ul><p><img src="https://s21.ax1x.com/2024/09/23/pAQCOc4.png" alt="pAQCOc4.png"></p><h3 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2.数据集"></a>2.数据集</h3><ul><li>训练集-91张图片（进行数据增强后得到训练集）</li><li>测试集-set5</li></ul><h3 id="3-模型结构"><a href="#3-模型结构" class="headerlink" title="3.模型结构"></a>3.模型结构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置中间层结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv_ReLU_Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Conv_ReLU_Block, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv = nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.conv(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主要网络结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, <span class="variable language_">self</span>).__init__() </span><br><span class="line">        <span class="variable language_">self</span>.residual_layer = <span class="variable language_">self</span>.make_layer(Conv_ReLU_Block, <span class="number">18</span>) <span class="comment">#18个3*3*64的中间层结构</span></span><br><span class="line">        <span class="variable language_">self</span>.<span class="built_in">input</span> = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>) <span class="comment">#输入层</span></span><br><span class="line">        <span class="variable language_">self</span>.output = nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>) <span class="comment">#输出层</span></span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化Conv2d层中的权重，使用高斯分布，标准差根据输入特征数量动态计算  </span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="variable language_">self</span>.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                n = m.kernel_size[<span class="number">0</span>] * m.kernel_size[<span class="number">1</span>] * m.out_channels</span><br><span class="line">                m.weight.data.normal_(<span class="number">0</span>, sqrt(<span class="number">2.</span> / n))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_layer</span>(<span class="params">self, block, num_of_layer</span>): <span class="comment">#创建中间层</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_of_layer):</span><br><span class="line">            layers.append(block())</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x <span class="comment">#原始输入作为残差</span></span><br><span class="line">        out = <span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.<span class="built_in">input</span>(x)) <span class="comment">#依次通过输入层、中间层、输出层 </span></span><br><span class="line">        out = <span class="variable language_">self</span>.residual_layer(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.output(out)</span><br><span class="line">        out = torch.add(out, residual) <span class="comment">#输出与残差相加，实现残差连接</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    modeltest = Net()</span><br><span class="line">    <span class="built_in">print</span>(modeltest)</span><br></pre></td></tr></table></figure><h3 id="4-模型训练"><a href="#4-模型训练" class="headerlink" title="4.模型训练"></a>4.模型训练</h3><p>参数设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Namespace(batchSize=<span class="number">128</span>, nEpochs=<span class="number">50</span>, lr=<span class="number">0.1</span>, step=<span class="number">10</span>, cuda=<span class="literal">True</span>, resume=<span class="string">&#x27;&#x27;</span>, start_epoch=<span class="number">1</span>, clip=<span class="number">0.4</span>, threads=<span class="number">1</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.0001</span>, pretrained=<span class="string">&#x27;none&#x27;</span>, gpus=<span class="string">&#x27;0&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">train_set = DatasetFromHdf5(<span class="string">&quot;data/train.h5&quot;</span>)  </span><br><span class="line">training_data_loader = DataLoader(dataset=train_set, batch_size=opt.batchSize, shuffle=<span class="literal">True</span>) </span><br><span class="line">model = Net() </span><br><span class="line">criterion = nn.MSELoss(reduction=<span class="string">&#x27;sum&#x27;</span>) </span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=opt.lr, momentum=opt.momentum, weight_decay=opt.weight_decay)  <span class="comment"># 设置优化器  </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(opt.start_epoch, opt.nEpochs + <span class="number">1</span>):  </span><br><span class="line">    train(training_data_loader, optimizer, model, criterion, epoch)  <span class="comment"># 调用训练函数  </span></span><br><span class="line">    save_checkpoint(model, epoch)  <span class="comment"># 保存每轮训练后的权重</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">training_data_loader, optimizer, model, criterion, epoch</span>):  </span><br><span class="line">    <span class="comment">#每轮调整学习率</span></span><br><span class="line">    lr = adjust_learning_rate(optimizer, epoch-<span class="number">1</span>)  </span><br><span class="line">    <span class="comment">#遍历优化器的参数组，将每个组的学习率设置为新的学习率  </span></span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:  </span><br><span class="line">        param_group[<span class="string">&quot;lr&quot;</span>] = lr  </span><br><span class="line">    <span class="comment"># 打印当前epoch和对应的学习率  </span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch = &#123;&#125;, lr = &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>]))  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 设置模型为训练模式  </span></span><br><span class="line">    model.train()  </span><br><span class="line">    <span class="keyword">for</span> iteration, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(training_data_loader, <span class="number">1</span>):  </span><br><span class="line">        <span class="built_in">input</span>, target = Variable(batch[<span class="number">0</span>]), Variable(batch[<span class="number">1</span>], requires_grad=<span class="literal">False</span>)    </span><br><span class="line">        <span class="keyword">if</span> opt.cuda:  </span><br><span class="line">            <span class="built_in">input</span> = <span class="built_in">input</span>.cuda()  </span><br><span class="line">            target = target.cuda()  </span><br><span class="line">            </span><br><span class="line">        loss = criterion(model(<span class="built_in">input</span>), target)  </span><br><span class="line">        optimizer.zero_grad()   </span><br><span class="line">        loss.backward()  </span><br><span class="line">        nn.utils.clip_grad_norm_(model.parameters(), opt.clip)   <span class="comment">#梯度裁剪是一种技术，用于控制梯度的更新量，以避免在训练过程中出现梯度爆炸的问题，从而有助于模型训练的稳定性，当调用 nn.utils.clip_grad_norm_(model.parameters(), opt.clip) 时，PyTorch 会计算模型所有参数的梯度的L2范数。如果这个范数大于 opt.clip 指定的值，那么每个参数的梯度将会按比例缩放，使得最终的L2范数等于 opt.clip。</span></span><br><span class="line">        optimizer.step()  </span><br></pre></td></tr></table></figure><h2 id="二-EDSR"><a href="#二-EDSR" class="headerlink" title="二. EDSR"></a>二. EDSR</h2><ul><li>在SRResnet的基础上去除了BN层</li></ul><p><img src="https://s21.ax1x.com/2024/09/23/pAQm0Gn.png" alt="pAQm0Gn.png"></p><p>结构如下：</p><ul><li><img src="https://s21.ax1x.com/2024/09/23/pAQmDx0.png" alt="pAQmDx0.png"></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;超分辨率第四章-VDSR-amp-EDSR&quot;&gt;&lt;a href=&quot;#超分辨率第四章-VDSR-amp-EDSR&quot; class=&quot;headerlink&quot; title=&quot;超分辨率第四章-VDSR&amp;amp;EDSR&quot;&gt;&lt;/a&gt;超分辨率第四章-VDSR&amp;amp;EDSR&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;VDSR是2016年提出的模型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1511.04587&quot;&gt;Accurate Image Super-Resolution Using Very Deep Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;代码位置：F:&#92;Github下载&#92;pytorch-vdsr-recurrence-main&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;EDSR是是SRResNet的增强版本,2017年提出&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="硕士阶段学习笔记(入门阶段)" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/"/>
    
    <category term="模型" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="超分辨率" scheme="http://example.com/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>超分辨率第三章-SRGAN</title>
    <link href="http://example.com/2024/09/17/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%AC%AC%E4%B8%89%E7%AB%A0-SRGAN/"/>
    <id>http://example.com/2024/09/17/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%AC%AC%E4%B8%89%E7%AB%A0-SRGAN/</id>
    <published>2024-09-17T07:33:29.000Z</published>
    <updated>2024-09-19T13:56:30.064Z</updated>
    
    <content type="html"><![CDATA[<h1 id="超分辨率第三章-SRGAN"><a href="#超分辨率第三章-SRGAN" class="headerlink" title="超分辨率第三章-SRGAN"></a>超分辨率第三章-SRGAN</h1><blockquote><p>SRGNN是2017年提出的模型，首次使用GAN在超分辨领域。</p><p>参考文献：<a href="https://arxiv.org/abs/1609.04802">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</a></p><p>参考博客：<a href="https://blog.csdn.net/qq_47071847/article/details/130859675">基于pytorch的SRGAN实现(全网最细!!!)</a></p></blockquote><span id="more"></span><h2 id="一-模型介绍"><a href="#一-模型介绍" class="headerlink" title="一.模型介绍"></a>一.模型介绍</h2><ul><li><p>先前超分辨率模型的局限性：虽然具有较高的峰值信噪比，但它们通常缺乏高频细节，并且在感知上不令人满意。</p></li><li><p>SRGAN中的生成网络就是SRResNet网络，其以ResNet块为基本结构，是一个具有深度的SR网络。生成网络使用感知损失进行训练，而不是传统的MSE方法，它使用预训练之后的VGG-16网络产生的feature map级进行计算，再加上本身生成网络带有的对抗损失。此外判别器也需要去训练，两个网络结合起来就是我们的SRGAN网络。</p></li><li><p>SRGNN提出了感知损失函数（Perceptual loss function），包括对抗损失与内容损失，在感知质量方面有了极大改进。MOS（平均意见得分）很高。</p></li></ul><h3 id="1-感知损失函数（Perceptual-loss-function）"><a href="#1-感知损失函数（Perceptual-loss-function）" class="headerlink" title="1.感知损失函数（Perceptual loss function）"></a>1.感知损失函数（Perceptual loss function）</h3><ul><li><script type="math/tex; mode=display">l ^ { S R } = l _ { X } ^ { S R } + 1 0 ^ { - 3 } l _ { G e n } ^ { S R }</script><ul><li>由基于VGG-16的内容损失函数和GAN的对抗损失函数组成</li></ul></li><li>内容损失函数<ul><li><script type="math/tex; mode=display">I _ { X } ^ { S R } = I _ { V G G / ( i , j ) } ^ { S R } = \frac { 1 } { W _ { i , j } H _ { i , j } } \sum _ { x = 1 } ^ { W _ { i , j } } ( \phi _ { i , j } ( G _ { 0 , i } ( I ^ { L R } ) ) _ { x , y } - \phi _ { i , j } ( I ^ { H R } ) _ { x , y } ) ^ { 2 } .</script></li><li>采用预训练好的VGG-16网络的特征向量，使得生成网络的结果<script type="math/tex">G( I ^ { L R })</script>通过VGG某一层之后产生的feature map和原始高分辨率图像<script type="math/tex">I ^ { H R }</script>通过VGG-16网络产生的feature map做loss，作者指出这种loss更能反应图片之间的感知相似度。</li></ul></li><li>对抗损失函数<ul><li><script type="math/tex; mode=display">I _ { G e n } ^ { S R } = \sum _ { n = 1 } ^ { N } - \log D _ { \theta _ { D } } ( G _ { \theta _ { G } } ( I ^ { L R } ) ) .</script></li></ul></li></ul><h3 id="2-论文贡献"><a href="#2-论文贡献" class="headerlink" title="2.论文贡献"></a>2.论文贡献</h3><ul><li>深度RESNet（SRRESNet）针对MSE进行了优化，通过PSNR和结构相似度(SSIM)来测量图像SR的高放大因子</li><li>SRGAN，是一种基于GAN的网络，针对一种新的感知损失进行了优化。用在VGG网络的特征映射上计算的损失来代替基于MSE的内容损失，该特征映射对像素空间的变化更加不变，这样相较于原来像素损失超分的图像更具有纹理等高频细节.</li><li>对来自三个公共基准数据集的图像进行广泛的平均意见得分(MOS)测试，证实SRGAN在很大程度上是高放大因子(4×)的照片真实感SR图像估计的最新技术, 即超分后的图像更加接近自然图像.</li></ul><h3 id="3-模型结构"><a href="#3-模型结构" class="headerlink" title="3.模型结构"></a>3.模型结构</h3><p><img src="https://s21.ax1x.com/2024/09/19/pAKsjR1.png" alt="pAKsjR1.png"></p><h2 id="二-数据集"><a href="#二-数据集" class="headerlink" title="二.数据集"></a>二.数据集</h2><ul><li>训练集使用：VOC2012（训练数据集包含16700张图片，验证数据集包含425张图片）</li><li>测试集使用：Set5 Set14 BSD100 Urban100 SunHays80</li></ul><h2 id="三-模型搭建"><a href="#三-模型搭建" class="headerlink" title="三.模型搭建"></a>三.模型搭建</h2><h3 id="1-生成器结构"><a href="#1-生成器结构" class="headerlink" title="1.生成器结构"></a>1.生成器结构</h3><ul><li>输入一张低分辨率图片，生成高分辨率图片</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, scale_factor</span>):</span><br><span class="line">        upsample_block_num = <span class="built_in">int</span>(math.log(scale_factor, <span class="number">2</span>)) <span class="comment">#计算上采样块的数量，输入放大因子为4，则有两个上采样块</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(Generator, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.block1 = nn.Sequential( <span class="comment">#首先放大维度，特征提取</span></span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">9</span>, padding=<span class="number">4</span>),</span><br><span class="line">            nn.PReLU()</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.block2 = ResidualBlock(<span class="number">64</span>) <span class="comment">#5个残差网络块，特征提取</span></span><br><span class="line">        <span class="variable language_">self</span>.block3 = ResidualBlock(<span class="number">64</span>)</span><br><span class="line">        <span class="variable language_">self</span>.block4 = ResidualBlock(<span class="number">64</span>)</span><br><span class="line">        <span class="variable language_">self</span>.block5 = ResidualBlock(<span class="number">64</span>)</span><br><span class="line">        <span class="variable language_">self</span>.block6 = ResidualBlock(<span class="number">64</span>)</span><br><span class="line">        <span class="variable language_">self</span>.block7 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">        )</span><br><span class="line">        block8 = [UpsampleBLock(<span class="number">64</span>, <span class="number">2</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(upsample_block_num)] <span class="comment">#定义了一个列表，进行上采样两次提高分辨率，每次提高2倍，共提升4倍</span></span><br><span class="line">        block8.append(nn.Conv2d(<span class="number">64</span>, <span class="number">3</span>, kernel_size=<span class="number">9</span>, padding=<span class="number">4</span>))<span class="comment">#向该列表添加一个卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.block8 = nn.Sequential(*block8)<span class="comment">#将这些层组合成一个顺序模型</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        block1 = <span class="variable language_">self</span>.block1(x)</span><br><span class="line">        block2 = <span class="variable language_">self</span>.block2(block1)</span><br><span class="line">        block3 = <span class="variable language_">self</span>.block3(block2)</span><br><span class="line">        block4 = <span class="variable language_">self</span>.block4(block3)</span><br><span class="line">        block5 = <span class="variable language_">self</span>.block5(block4)</span><br><span class="line">        block6 = <span class="variable language_">self</span>.block6(block5)</span><br><span class="line">        block7 = <span class="variable language_">self</span>.block7(block6)</span><br><span class="line">        block8 = <span class="variable language_">self</span>.block8(block1 + block7) <span class="comment">#特征融合相加</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (torch.tanh(block8) + <span class="number">1</span>) / <span class="number">2</span> <span class="comment">#使用torch.tanh函数将输出值映射到[-1, 1]区间，并通过(torch.tanh(block8) + 1) / 2将其缩放到[0, 1]区间，这是图像数据常见的归一化范围。</span></span><br></pre></td></tr></table></figure><h3 id="2-判别器结构"><a href="#2-判别器结构" class="headerlink" title="2.判别器结构"></a>2.判别器结构</h3><ul><li>输入图片，判断真假</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.net = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line"></span><br><span class="line">            nn.AdaptiveAvgPool2d(<span class="number">1</span>), <span class="comment">#一个自适应平均池化层，它该层都会将其空间维度（高度和宽度）压缩到1x1，而保持通道数不变</span></span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">1024</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">1024</span>, <span class="number">1</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(<span class="variable language_">self</span>.net(x).view(batch_size)) <span class="comment">#显示输入图片为真实的概率，将最终的输出（原本是一个形状为(batch_size, 1, 1, 1)的四维张量）展平成一个一维张量，其长度为批次大小，其元素对应于批次中每个样本的判别结果</span></span><br></pre></td></tr></table></figure><h3 id="3-resnet结构"><a href="#3-resnet结构" class="headerlink" title="3.resnet结构"></a>3.resnet结构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(ResidualBlock, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(channels, channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn1 = nn.BatchNorm2d(channels)</span><br><span class="line">        <span class="variable language_">self</span>.prelu = nn.PReLU()</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(channels, channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn2 = nn.BatchNorm2d(channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        residual = <span class="variable language_">self</span>.bn1(residual)</span><br><span class="line">        residual = <span class="variable language_">self</span>.prelu(residual)</span><br><span class="line">        residual = <span class="variable language_">self</span>.conv2(residual)</span><br><span class="line">        residual = <span class="variable language_">self</span>.bn2(residual)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x + residual</span><br></pre></td></tr></table></figure><h3 id="4-上采样结构"><a href="#4-上采样结构" class="headerlink" title="4.上采样结构"></a>4.上采样结构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UpsampleBLock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, up_scale</span>):</span><br><span class="line">        <span class="built_in">super</span>(UpsampleBLock, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv = nn.Conv2d(in_channels, in_channels * up_scale ** <span class="number">2</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)<span class="comment">#up_scale ** 2是因为之后的像素重排（pixel shuffle）操作会将通道数重排成空间维度，以达到上采样的效果，此时增加维度可以保持整体维度不变。</span></span><br><span class="line">        <span class="variable language_">self</span>.pixel_shuffle = nn.PixelShuffle(up_scale) <span class="comment">#这个层将输入特征图的通道数重新排列成空间维度，以实现上采样（新的通道数将是原始通道数除以up_scale^2，而高度和宽度将会乘以up_scale）</span></span><br><span class="line">        <span class="variable language_">self</span>.prelu = nn.PReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.conv(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.pixel_shuffle(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.prelu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="四-损失函数"><a href="#四-损失函数" class="headerlink" title="四.损失函数"></a>四.损失函数</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision.models.vgg <span class="keyword">import</span> vgg16</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GeneratorLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(GeneratorLoss, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment">#使用预训练的 VGG16 模型来构建特征提取网络</span></span><br><span class="line">        vgg = vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#选择 VGG16 模型的前 31 层作为损失网络，并将其设置为评估模式（不进行梯度更新）</span></span><br><span class="line">        loss_network = nn.Sequential(*<span class="built_in">list</span>(vgg.features)[:<span class="number">31</span>]).<span class="built_in">eval</span>()</span><br><span class="line">        <span class="comment">#冻结其参数，不进行梯度更新</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> loss_network.parameters():</span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br><span class="line">        <span class="comment">#定义VGG16网络</span></span><br><span class="line">        <span class="variable language_">self</span>.loss_network = loss_network</span><br><span class="line">        <span class="comment">#定义均方误差损失函数，计算生成器生成图像与目标图像之间的均方误差损失</span></span><br><span class="line">        <span class="variable language_">self</span>.mse_loss = nn.MSELoss()</span><br><span class="line">        <span class="comment">#定义总变差损失函数，计算生成器生成图像的总变差损失，用于平滑生成的图像</span></span><br><span class="line">        <span class="variable language_">self</span>.tv_loss = TVLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, out_labels, out_images, target_images</span>): <span class="comment">#分别传入判别器判定概率，伪高分辨率图像，真图像</span></span><br><span class="line">        <span class="comment"># Adversarial Loss（对抗损失）：使生成的图像更接近真实图像，目标是最小化生成器对图像的判别结果的平均值与 1(真实值)的差距</span></span><br><span class="line">        adversarial_loss = torch.mean(<span class="number">1</span> - out_labels)</span><br><span class="line">        <span class="comment"># Perception Loss（感知损失）：计算生成图像和目标图像在vgg-16网络中提取的特征之间的均方误差损失</span></span><br><span class="line">        perception_loss = <span class="variable language_">self</span>.mse_loss(<span class="variable language_">self</span>.loss_network(out_images), <span class="variable language_">self</span>.loss_network(target_images))</span><br><span class="line">        <span class="comment"># Image Loss（图像损失）：计算生成图像和目标图像之间的均方误差损失</span></span><br><span class="line">        image_loss = <span class="variable language_">self</span>.mse_loss(out_images, target_images)</span><br><span class="line">        <span class="comment"># TV Loss（总变差损失）：计算生成图像的总变差损失，用于平滑生成的图像</span></span><br><span class="line">        tv_loss = <span class="variable language_">self</span>.tv_loss(out_images)</span><br><span class="line">        <span class="comment"># 返回生成器的总损失，四个损失项加权求和</span></span><br><span class="line">        <span class="keyword">return</span> image_loss + <span class="number">0.001</span> * adversarial_loss + <span class="number">0.006</span> * perception_loss + <span class="number">2e-8</span> * tv_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 总变差损失衡量的是图像中相邻像素之间的差异程度</span></span><br><span class="line"><span class="comment"># 在模型训练过程中，将总变差损失作为损失函数的一部分，可以引导模型在优化过程中考虑图像的空间连续性，从而生成更加符合人类直觉的图像。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TVLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tv_loss_weight=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TVLoss, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.tv_loss_weight = tv_loss_weight</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch_size = x.size()[<span class="number">0</span>]</span><br><span class="line">        h_x = x.size()[<span class="number">2</span>]</span><br><span class="line">        w_x = x.size()[<span class="number">3</span>]</span><br><span class="line">        count_h = <span class="variable language_">self</span>.tensor_size(x[:, :, <span class="number">1</span>:, :])</span><br><span class="line">        count_w = <span class="variable language_">self</span>.tensor_size(x[:, :, :, <span class="number">1</span>:])</span><br><span class="line">        <span class="comment"># 计算水平方向上的总变差损失</span></span><br><span class="line">        h_tv = torch.<span class="built_in">pow</span>((x[:, :, <span class="number">1</span>:, :] - x[:, :, :h_x - <span class="number">1</span>, :]), <span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">        <span class="comment"># 计算垂直方向上的总变差损失</span></span><br><span class="line">        w_tv = torch.<span class="built_in">pow</span>((x[:, :, :, <span class="number">1</span>:] - x[:, :, :, :w_x - <span class="number">1</span>]), <span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">        <span class="comment"># 返回总变差损失</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.tv_loss_weight * <span class="number">2</span> * (h_tv / count_h + w_tv / count_w) / batch_size</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tensor_size</span>(<span class="params">t</span>):</span><br><span class="line">        <span class="comment"># 返回张量的尺寸大小，即通道数乘以高度乘以宽度</span></span><br><span class="line">        <span class="keyword">return</span> t.size()[<span class="number">1</span>] * t.size()[<span class="number">2</span>] * t.size()[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    g_loss = GeneratorLoss()</span><br><span class="line">    <span class="built_in">print</span>(g_loss)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="五-模型训练"><a href="#五-模型训练" class="headerlink" title="五.模型训练"></a>五.模型训练</h2><h3 id="1-载入数据集与初始化网络"><a href="#1-载入数据集与初始化网络" class="headerlink" title="1.载入数据集与初始化网络"></a>1.载入数据集与初始化网络</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">opt = parser.parse_args()<span class="comment">#用户可以在命令行中指定一些参数（如裁剪大小、放大因子、训练轮数等），这些参数将被存储在opt对象中</span></span><br><span class="line"></span><br><span class="line">CROP_SIZE = opt.crop_size</span><br><span class="line">UPSCALE_FACTOR = opt.upscale_factor</span><br><span class="line">NUM_EPOCHS = opt.num_epochs</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建训练和验证数据集</span></span><br><span class="line">train_set = TrainDatasetFromFolder(<span class="string">&#x27;data/VOC2012/train&#x27;</span>, crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)</span><br><span class="line">val_set = ValDatasetFromFolder(<span class="string">&#x27;data/VOC2012/val&#x27;</span>, upscale_factor=UPSCALE_FACTOR)</span><br><span class="line"><span class="comment">#创建数据加载器</span></span><br><span class="line">train_loader = DataLoader(dataset=train_set, num_workers=<span class="number">4</span>, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_loader = DataLoader(dataset=val_set, num_workers=<span class="number">4</span>, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化生成器（netG）和判别器（netD）网络，并打印生成器和判别器的参数数量</span></span><br><span class="line">netG = Generator(UPSCALE_FACTOR)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;# generator parameters:&#x27;</span>, <span class="built_in">sum</span>(param.numel() <span class="keyword">for</span> param <span class="keyword">in</span> netG.parameters()))</span><br><span class="line">netD = Discriminator()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;# discriminator parameters:&#x27;</span>, <span class="built_in">sum</span>(param.numel() <span class="keyword">for</span> param <span class="keyword">in</span> netD.parameters()))</span><br><span class="line"></span><br><span class="line">generator_criterion = GeneratorLoss()<span class="comment">#定义生成器的内容损失函数，此处会引入VGG16网络进行计算</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    netG.cuda()</span><br><span class="line">    netD.cuda()</span><br><span class="line">    generator_criterion.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化优化器</span></span><br><span class="line">optimizerG = optim.Adam(netG.parameters())</span><br><span class="line">optimizerD = optim.Adam(netD.parameters())</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化一个字典results，用于存储训练过程中的各种指标（如判别器和生成器的损失、评分、PSNR、SSIM等）。这些指标将用于评估训练过程中的模型性能。</span></span><br><span class="line">results = &#123;<span class="string">&#x27;d_loss&#x27;</span>: [], <span class="string">&#x27;g_loss&#x27;</span>: [], <span class="string">&#x27;d_score&#x27;</span>: [], <span class="string">&#x27;g_score&#x27;</span>: [], <span class="string">&#x27;psnr&#x27;</span>: [], <span class="string">&#x27;ssim&#x27;</span>: []&#125;</span><br></pre></td></tr></table></figure><h3 id="2-训练阶段"><a href="#2-训练阶段" class="headerlink" title="2.训练阶段"></a>2.训练阶段</h3><ul><li>生成器（Generator）和判别器（Discriminator）交替训练</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, NUM_EPOCHS + <span class="number">1</span>):</span><br><span class="line">    train_bar = tqdm(train_loader)</span><br><span class="line">    running_results = &#123;<span class="string">&#x27;batch_sizes&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;d_loss&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;g_loss&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;d_score&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;g_score&#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">    netG.train()</span><br><span class="line">    netD.train()</span><br><span class="line">    <span class="keyword">for</span> data, target <span class="keyword">in</span> train_bar:</span><br><span class="line">        g_update_first = <span class="literal">True</span></span><br><span class="line">        batch_size = data.size(<span class="number">0</span>)</span><br><span class="line">        running_results[<span class="string">&#x27;batch_sizes&#x27;</span>] += batch_size</span><br><span class="line"></span><br><span class="line">        <span class="comment">#先训练判别器，输入就是真图片、假图片和它们对应的标签。</span></span><br><span class="line">        <span class="comment"># (1) Update D network: maximize D(x)-1-D(G(z))</span></span><br><span class="line">        real_img = target <span class="comment">#真实图片</span></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            real_img = real_img.<span class="built_in">float</span>().cuda()</span><br><span class="line">        z = data <span class="comment">#低分辨率图片</span></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            z = z.<span class="built_in">float</span>().cuda()</span><br><span class="line"></span><br><span class="line">        fake_img = netG(z) <span class="comment">#通过生成器生成高分辨率伪图片</span></span><br><span class="line">        optimizerD.zero_grad() <span class="comment">#清除判别器的梯度</span></span><br><span class="line">        real_out = netD(real_img).mean() <span class="comment">#通过判别器对真实图像进行前向传播，并计算其输出的平均值</span></span><br><span class="line">        fake_out = netD(fake_img).mean() <span class="comment">#通过判别器对伪图像进行前向传播，并计算其输出的平均值</span></span><br><span class="line">        d_loss = <span class="number">1</span> - real_out + fake_out <span class="comment">#计算判别器的损失</span></span><br><span class="line">        d_loss.backward(retain_graph=<span class="literal">True</span>) <span class="comment">#反向传播，计算判别器的梯度，并保留计算图以进行后续优化步骤</span></span><br><span class="line">        optimizerD.step() <span class="comment">#对判别器网络梯度进行更新</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#再训练生成器，在训练生成器的时候我们希望生成器可以生成极为真实的假图片。因此我们在训练生成器需要知道判别器认为什么图片是真图片</span></span><br><span class="line">        <span class="comment"># (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss</span></span><br><span class="line">        optimizerG.zero_grad() <span class="comment">#清除生成器的梯度</span></span><br><span class="line">        fake_img = netG(z) <span class="comment">#通过生成器生成高分辨率伪图片</span></span><br><span class="line">        fake_out = netD(fake_img).mean() <span class="comment">#通过判别器对伪图像进行前向传播，并计算其输出的平均值</span></span><br><span class="line">        g_loss = generator_criterion(fake_out, fake_img, real_img)<span class="comment"># 计算生成器的损失，包括对抗损失、感知损失、图像损失和TV损失</span></span><br><span class="line">        g_loss.backward() <span class="comment">#反向传播，计算生成器的梯度</span></span><br><span class="line">        optimizerG.step() <span class="comment">#对生成器网络梯度进行更新</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># loss for current batch before optimization</span></span><br><span class="line">        <span class="comment">#累加当前批次生成器的损失值乘以批次大小，用于计算平均损失</span></span><br><span class="line">        running_results[<span class="string">&#x27;g_loss&#x27;</span>] += g_loss.item() * batch_size</span><br><span class="line">        <span class="comment">#累加当前批次判别器的损失值乘以批次大小，用于计算平均损失</span></span><br><span class="line">        running_results[<span class="string">&#x27;d_loss&#x27;</span>] += d_loss.item() * batch_size</span><br><span class="line">        <span class="comment">#累加当前批次真实图像在判别器的输出得分乘以批次大小，用于计算平均得分</span></span><br><span class="line">        running_results[<span class="string">&#x27;d_score&#x27;</span>] += real_out.item() * batch_size</span><br><span class="line">        <span class="comment">#累加当前批次伪图像在判别器的输出得分乘以批次大小，用于计算平均得分</span></span><br><span class="line">        running_results[<span class="string">&#x27;g_score&#x27;</span>] += fake_out.item() * batch_size</span><br><span class="line"></span><br><span class="line">        <span class="comment">#更新训练进度条的描述信息</span></span><br><span class="line">        train_bar.set_description(desc=<span class="string">&#x27;[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f&#x27;</span> % (</span><br><span class="line">            epoch, NUM_EPOCHS, running_results[<span class="string">&#x27;d_loss&#x27;</span>] / running_results[<span class="string">&#x27;batch_sizes&#x27;</span>],</span><br><span class="line">            running_results[<span class="string">&#x27;g_loss&#x27;</span>] / running_results[<span class="string">&#x27;batch_sizes&#x27;</span>],</span><br><span class="line">            running_results[<span class="string">&#x27;d_score&#x27;</span>] / running_results[<span class="string">&#x27;batch_sizes&#x27;</span>],</span><br><span class="line">            running_results[<span class="string">&#x27;g_score&#x27;</span>] / running_results[<span class="string">&#x27;batch_sizes&#x27;</span>]))</span><br></pre></td></tr></table></figure><h3 id="3-验证阶段"><a href="#3-验证阶段" class="headerlink" title="3.验证阶段"></a>3.验证阶段</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">netG.<span class="built_in">eval</span>() <span class="comment">#生成器验证模式</span></span><br><span class="line">out_path = <span class="string">&#x27;training_results/SRF_&#x27;</span> + <span class="built_in">str</span>(UPSCALE_FACTOR) + <span class="string">&#x27;/&#x27;</span> <span class="comment">#创建用于保存训练结果的目录</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(out_path):</span><br><span class="line">    os.makedirs(out_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    val_bar = tqdm(val_loader) <span class="comment">#验证集进度条</span></span><br><span class="line">    valing_results = &#123;<span class="string">&#x27;mse&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;ssims&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;psnr&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;ssim&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;batch_sizes&#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line">    val_images = []</span><br><span class="line">    <span class="keyword">for</span> val_lr, val_hr_restore, val_hr <span class="keyword">in</span> val_bar: <span class="comment">#遍历验证数据集(低分辨率图 恢复的高分辨率图 高分辨率图）</span></span><br><span class="line">        batch_size = val_lr.size(<span class="number">0</span>)</span><br><span class="line">        valing_results[<span class="string">&#x27;batch_sizes&#x27;</span>] += batch_size</span><br><span class="line">        lr = val_lr</span><br><span class="line">        hr = val_hr</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            lr = lr.<span class="built_in">float</span>().cuda()</span><br><span class="line">            hr = hr.<span class="built_in">float</span>().cuda()</span><br><span class="line">        sr = netG(lr) <span class="comment">#生成超分辨率图像</span></span><br><span class="line"></span><br><span class="line">        batch_mse = ((sr - hr) ** <span class="number">2</span>).data.mean() <span class="comment">#计算批量图像的均方误差，这里应该使用.mean()而不是.data.mean()，后者在PyTorch新版本中已不推荐</span></span><br><span class="line">        valing_results[<span class="string">&#x27;mse&#x27;</span>] += batch_mse * batch_size <span class="comment">#累加均方误差</span></span><br><span class="line">        batch_ssim = pytorch_ssim.ssim(sr, hr).item() <span class="comment">#计算批量图像的结构相似度指数</span></span><br><span class="line">        valing_results[<span class="string">&#x27;ssims&#x27;</span>] += batch_ssim * batch_size <span class="comment">#累加结构相似度指数</span></span><br><span class="line">        <span class="comment">#计算平均峰值信噪比</span></span><br><span class="line">        valing_results[<span class="string">&#x27;psnr&#x27;</span>] = <span class="number">10</span> * log10((hr.<span class="built_in">max</span>()**<span class="number">2</span>) / (valing_results[<span class="string">&#x27;mse&#x27;</span>] / valing_results[<span class="string">&#x27;batch_sizes&#x27;</span>]))</span><br><span class="line">        <span class="comment">#计算平均结构相似度指数</span></span><br><span class="line">        valing_results[<span class="string">&#x27;ssim&#x27;</span>] = valing_results[<span class="string">&#x27;ssims&#x27;</span>] / valing_results[<span class="string">&#x27;batch_sizes&#x27;</span>]</span><br><span class="line">        <span class="comment">#更新训练进度条的描述信息</span></span><br><span class="line">        val_bar.set_description(</span><br><span class="line">            desc=<span class="string">&#x27;[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f&#x27;</span> % (</span><br><span class="line">                valing_results[<span class="string">&#x27;psnr&#x27;</span>], valing_results[<span class="string">&#x27;ssim&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="comment">#将验证图像添加到列表中，用于后续保存</span></span><br><span class="line">        val_images.extend(</span><br><span class="line">            [display_transform()(val_hr_restore.squeeze(<span class="number">0</span>)), display_transform()(hr.data.cpu().squeeze(<span class="number">0</span>)),</span><br><span class="line">             display_transform()(sr.data.cpu().squeeze(<span class="number">0</span>))])</span><br><span class="line">    val_images = torch.stack(val_images)  <span class="comment">#将验证图像列表堆叠为张量</span></span><br><span class="line">    val_images = torch.chunk(val_images, val_images.size(<span class="number">0</span>) // <span class="number">15</span>) <span class="comment">#将堆叠后的张量分割为多个小块，每个小块包含15张图像</span></span><br><span class="line">    val_save_bar = tqdm(val_images, desc=<span class="string">&#x27;[saving training results]&#x27;</span>) <span class="comment">#创建保存图像进度条，并设置描述为“[saving training results]”</span></span><br><span class="line">    index = <span class="number">1</span></span><br><span class="line">    <span class="comment">#遍历图像批次并保存</span></span><br><span class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> val_save_bar:</span><br><span class="line">        image = utils.make_grid(image, nrow=<span class="number">3</span>, padding=<span class="number">5</span>) <span class="comment">#将小块中的图像创建为一个网格，每行显示3张图像，图像之间有5个像素的间隔</span></span><br><span class="line">        utils.save_image(image, out_path + <span class="string">&#x27;epoch_%d_index_%d.png&#x27;</span> % (epoch, index), padding=<span class="number">5</span>)</span><br><span class="line">        index += <span class="number">1</span></span><br></pre></td></tr></table></figure><h4 id="4-保存文件"><a href="#4-保存文件" class="headerlink" title="4.保存文件"></a>4.保存文件</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将判别器和生成器的参数保存到指定文件</span></span><br><span class="line">torch.save(netG.state_dict(), <span class="string">&#x27;epochs/netG_epoch_%d_%d.pth&#x27;</span> % (UPSCALE_FACTOR, epoch))</span><br><span class="line">torch.save(netD.state_dict(), <span class="string">&#x27;epochs/netD_epoch_%d_%d.pth&#x27;</span> % (UPSCALE_FACTOR, epoch))</span><br><span class="line"><span class="comment"># save loss\scores\psnr\ssim</span></span><br><span class="line">results[<span class="string">&#x27;d_loss&#x27;</span>].append(running_results[<span class="string">&#x27;d_loss&#x27;</span>] / running_results[<span class="string">&#x27;batch_sizes&#x27;</span>])</span><br><span class="line">results[<span class="string">&#x27;g_loss&#x27;</span>].append(running_results[<span class="string">&#x27;g_loss&#x27;</span>] / running_results[<span class="string">&#x27;batch_sizes&#x27;</span>])</span><br><span class="line">results[<span class="string">&#x27;d_score&#x27;</span>].append(running_results[<span class="string">&#x27;d_score&#x27;</span>] / running_results[<span class="string">&#x27;batch_sizes&#x27;</span>])</span><br><span class="line">results[<span class="string">&#x27;g_score&#x27;</span>].append(running_results[<span class="string">&#x27;g_score&#x27;</span>] / running_results[<span class="string">&#x27;batch_sizes&#x27;</span>])</span><br><span class="line">results[<span class="string">&#x27;psnr&#x27;</span>].append(valing_results[<span class="string">&#x27;psnr&#x27;</span>])</span><br><span class="line">results[<span class="string">&#x27;ssim&#x27;</span>].append(valing_results[<span class="string">&#x27;ssim&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span> <span class="keyword">and</span> epoch != <span class="number">0</span>:</span><br><span class="line">    out_path = <span class="string">&#x27;statistics/&#x27;</span></span><br><span class="line">    <span class="comment"># 创建一个DataFrame对象，用于存储训练结果数据</span></span><br><span class="line">    data_frame = pd.DataFrame(</span><br><span class="line">        data=&#123;<span class="string">&#x27;Loss_D&#x27;</span>: results[<span class="string">&#x27;d_loss&#x27;</span>], <span class="string">&#x27;Loss_G&#x27;</span>: results[<span class="string">&#x27;g_loss&#x27;</span>], <span class="string">&#x27;Score_D&#x27;</span>: results[<span class="string">&#x27;d_score&#x27;</span>],</span><br><span class="line">              <span class="string">&#x27;Score_G&#x27;</span>: results[<span class="string">&#x27;g_score&#x27;</span>], <span class="string">&#x27;PSNR&#x27;</span>: results[<span class="string">&#x27;psnr&#x27;</span>], <span class="string">&#x27;SSIM&#x27;</span>: results[<span class="string">&#x27;ssim&#x27;</span>]&#125;,</span><br><span class="line">        index=<span class="built_in">range</span>(<span class="number">1</span>, epoch + <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 将DataFrame对象保存为CSV文件</span></span><br><span class="line">    data_frame.to_csv(out_path + <span class="string">&#x27;srf_&#x27;</span> + <span class="built_in">str</span>(UPSCALE_FACTOR) + <span class="string">&#x27;_train_results.csv&#x27;</span>, index_label=<span class="string">&#x27;Epoch&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="五-模型测试"><a href="#五-模型测试" class="headerlink" title="五.模型测试"></a>五.模型测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log10</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.utils <span class="keyword">as</span> utils</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pytorch_ssim</span><br><span class="line"><span class="keyword">from</span> data_utils <span class="keyword">import</span> TestDatasetFromFolder, display_transform</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> Generator</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;Test Benchmark Datasets&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--upscale_factor&#x27;</span>, default=<span class="number">4</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, <span class="built_in">help</span>=<span class="string">&#x27;super resolution upscale factor&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--model_name&#x27;</span>, default=<span class="string">&#x27;netG_epoch_4_150.pth&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, <span class="built_in">help</span>=<span class="string">&#x27;generator model epoch name&#x27;</span>)</span><br><span class="line">opt = parser.parse_args()</span><br><span class="line"></span><br><span class="line">UPSCALE_FACTOR = opt.upscale_factor</span><br><span class="line">MODEL_NAME = opt.model_name</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存每个测试数据集的结果</span></span><br><span class="line">results = &#123;<span class="string">&#x27;Set5&#x27;</span>: &#123;<span class="string">&#x27;psnr&#x27;</span>: [], <span class="string">&#x27;ssim&#x27;</span>: []&#125;, <span class="string">&#x27;Set14&#x27;</span>: &#123;<span class="string">&#x27;psnr&#x27;</span>: [], <span class="string">&#x27;ssim&#x27;</span>: []&#125;, <span class="string">&#x27;BSD100&#x27;</span>: &#123;<span class="string">&#x27;psnr&#x27;</span>: [], <span class="string">&#x27;ssim&#x27;</span>: []&#125;,</span><br><span class="line">           <span class="string">&#x27;Urban100&#x27;</span>: &#123;<span class="string">&#x27;psnr&#x27;</span>: [], <span class="string">&#x27;ssim&#x27;</span>: []&#125;, <span class="string">&#x27;SunHays80&#x27;</span>: &#123;<span class="string">&#x27;psnr&#x27;</span>: [], <span class="string">&#x27;ssim&#x27;</span>: []&#125;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 Generator 对象</span></span><br><span class="line">model = Generator(UPSCALE_FACTOR).<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model = model.cuda()</span><br><span class="line"><span class="comment"># 加载训练好的模型参数</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;epochs/&#x27;</span> + MODEL_NAME))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载测试数据集</span></span><br><span class="line">test_set = TestDatasetFromFolder(<span class="string">&#x27;data/test&#x27;</span>, upscale_factor=UPSCALE_FACTOR)</span><br><span class="line">test_loader = DataLoader(dataset=test_set, num_workers=<span class="number">4</span>, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 创建一个用于 test_loader 的 tqdm 进度条</span></span><br><span class="line">test_bar = tqdm(test_loader, desc=<span class="string">&#x27;[testing benchmark datasets]&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试结果输出路径</span></span><br><span class="line">out_path = <span class="string">&#x27;benchmark_results/SRF_&#x27;</span> + <span class="built_in">str</span>(UPSCALE_FACTOR) + <span class="string">&#x27;/&#x27;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(out_path):</span><br><span class="line">    os.makedirs(out_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> image_name, lr_image, hr_restore_img, hr_image <span class="keyword">in</span> test_bar:</span><br><span class="line">    <span class="comment"># 由于 image_name 是一个包含单个元素的列表，所以将其取出</span></span><br><span class="line">    image_name = image_name[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 将 lr_image 转换为 Variable 对象，并设置 volatile=True</span></span><br><span class="line">    <span class="comment"># volatile=True 表示不会计算梯度，这在推理阶段通常是需要的</span></span><br><span class="line">    lr_image = Variable(lr_image, volatile=<span class="literal">True</span>)</span><br><span class="line">    hr_image = Variable(hr_image, volatile=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        lr_image = lr_image.cuda()</span><br><span class="line">        hr_image = hr_image.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成超分变率图像</span></span><br><span class="line">    sr_image = model(lr_image)</span><br><span class="line"></span><br><span class="line">    mse = ((hr_image - sr_image) ** <span class="number">2</span>).data.mean()</span><br><span class="line">    <span class="comment"># 计算峰值信噪比（Peak Signal-to-Noise Ratio）</span></span><br><span class="line">    psnr = <span class="number">10</span> * log10(<span class="number">255</span> ** <span class="number">2</span> / mse)</span><br><span class="line">    <span class="comment"># 计算结构相似性指数（Structural Similarity Index）</span></span><br><span class="line">    <span class="comment"># 使用 pytorch_ssim 库中的 ssim 函数计算 SSIM</span></span><br><span class="line">    ssim = pytorch_ssim.ssim(sr_image, hr_image).data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建一个包含三张图像的张量，分别是原始恢复的高分辨率图像、原始高分辨率图像和生成的超分辨率图像</span></span><br><span class="line">    <span class="comment"># 将每张图像应用 display_transform() 转换，并通过 squeeze(0) 去除批次维度</span></span><br><span class="line">    test_images = torch.stack(</span><br><span class="line">        [display_transform()(hr_restore_img.squeeze(<span class="number">0</span>)), display_transform()(hr_image.data.cpu().squeeze(<span class="number">0</span>)),</span><br><span class="line">         display_transform()(sr_image.data.cpu().squeeze(<span class="number">0</span>))])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用 make_grid 函数将三张图像拼接成一张大图像</span></span><br><span class="line">    <span class="comment"># nrow=3 表示每行显示 3 张图像，padding=5 表示图像之间的间距为 5</span></span><br><span class="line">    image = utils.make_grid(test_images, nrow=<span class="number">3</span>, padding=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用 save_image 函数将合成的图像保存到指定路径</span></span><br><span class="line">    utils.save_image(image, out_path + image_name.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>] + <span class="string">&#x27;_psnr_%.4f_ssim_%.4f.&#x27;</span> % (psnr, ssim) +</span><br><span class="line">                     image_name.split(<span class="string">&#x27;.&#x27;</span>)[-<span class="number">1</span>], padding=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将对应数据集的PSNR和SSIM保存到对应的字典当中</span></span><br><span class="line">    results[image_name.split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>]][<span class="string">&#x27;psnr&#x27;</span>].append(psnr)</span><br><span class="line">    results[image_name.split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>]][<span class="string">&#x27;ssim&#x27;</span>].append(ssim)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最终结果保存路径</span></span><br><span class="line">out_path = <span class="string">&#x27;statistics/&#x27;</span></span><br><span class="line">saved_results = &#123;<span class="string">&#x27;psnr&#x27;</span>: [], <span class="string">&#x27;ssim&#x27;</span>: []&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历 results 字典中的每个值</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> results.values():</span><br><span class="line">    <span class="comment"># 获取 PSNR 和 SSIM 的列表</span></span><br><span class="line">    psnr = np.array(item[<span class="string">&#x27;psnr&#x27;</span>])</span><br><span class="line">    ssim = np.array(item[<span class="string">&#x27;ssim&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果列表为空，将 PSNR 和 SSIM 设置为 &#x27;No data&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">len</span>(psnr) == <span class="number">0</span>) <span class="keyword">or</span> (<span class="built_in">len</span>(ssim) == <span class="number">0</span>):</span><br><span class="line">        psnr = <span class="string">&#x27;No data&#x27;</span></span><br><span class="line">        ssim = <span class="string">&#x27;No data&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果列表不为空，计算 PSNR 和 SSIM 的均值</span></span><br><span class="line">        psnr = psnr.mean()</span><br><span class="line">        ssim = ssim.mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将计算得到的 PSNR 和 SSIM 添加到 saved_results 字典的相应列表中</span></span><br><span class="line">    saved_results[<span class="string">&#x27;psnr&#x27;</span>].append(psnr)</span><br><span class="line">    saved_results[<span class="string">&#x27;ssim&#x27;</span>].append(ssim)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 DataFrame 对象，使用 saved_results 字典作为数据，以 results.keys() 作为列标签</span></span><br><span class="line">data_frame = pd.DataFrame(saved_results, results.keys())</span><br><span class="line"><span class="comment"># 将 DataFrame 对象保存为 CSV 文件</span></span><br><span class="line"><span class="comment"># 文件路径由 out_path、&#x27;srf_&#x27;、UPSCALE_FACTOR 值和 &#x27;_test_results.csv&#x27; 组成</span></span><br><span class="line"><span class="comment"># index_label=&#x27;DataSet&#x27; 表示使用 &#x27;DataSet&#x27; 作为索引标签</span></span><br><span class="line">data_frame.to_csv(out_path + <span class="string">&#x27;srf_&#x27;</span> + <span class="built_in">str</span>(UPSCALE_FACTOR) + <span class="string">&#x27;_test_results.csv&#x27;</span>, index_label=<span class="string">&#x27;DataSet&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;超分辨率第三章-SRGAN&quot;&gt;&lt;a href=&quot;#超分辨率第三章-SRGAN&quot; class=&quot;headerlink&quot; title=&quot;超分辨率第三章-SRGAN&quot;&gt;&lt;/a&gt;超分辨率第三章-SRGAN&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;SRGNN是2017年提出的模型，首次使用GAN在超分辨领域。&lt;/p&gt;
&lt;p&gt;参考文献：&lt;a href=&quot;https://arxiv.org/abs/1609.04802&quot;&gt;Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;参考博客：&lt;a href=&quot;https://blog.csdn.net/qq_47071847/article/details/130859675&quot;&gt;基于pytorch的SRGAN实现(全网最细!!!)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="硕士阶段学习笔记(入门阶段)" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/"/>
    
    <category term="模型" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="超分辨率" scheme="http://example.com/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>超分辨率第二章-FSRCNN</title>
    <link href="http://example.com/2024/09/13/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%AC%AC%E4%BA%8C%E7%AB%A0-FSRCNN/"/>
    <id>http://example.com/2024/09/13/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%AC%AC%E4%BA%8C%E7%AB%A0-FSRCNN/</id>
    <published>2024-09-13T08:00:41.000Z</published>
    <updated>2024-09-14T09:50:15.351Z</updated>
    
    <content type="html"><![CDATA[<h1 id="超分辨率第二章-FSRCNN"><a href="#超分辨率第二章-FSRCNN" class="headerlink" title="超分辨率第二章-FSRCNN"></a>超分辨率第二章-FSRCNN</h1><blockquote><p>FSRCNN是2016年提出的超分辨率模型，使用后端上采样（转置卷积的方法），在一定程度上解决了SRCNN的问题。</p><p><a href="https://blog.csdn.net/zzy_pphz/article/details/108408933">参考博客</a></p><p><a href="https://arxiv.org/abs/1608.00367">论文地址</a></p><p>模型位置：F:\Github下载\FSRCNN-pytorch-master</p></blockquote><span id="more"></span><h2 id="一-模型介绍"><a href="#一-模型介绍" class="headerlink" title="一.模型介绍"></a>一.模型介绍</h2><ul><li><p>FSRCNN改进了SRCNN在速度上存在的缺陷</p><ul><li><p>SRCNN在将低分辨率图像送进网络之前，<strong>会先使用双三次插值法进行插值上采样操作</strong>，产生与groundtruth大小一致的低分辨率图像，这样会增加了计算复杂度，<strong>因为插值后的图像相比原始的低分辨率图像更大，于是在输入网络后各个卷积层的计算代价会增大，从而限制了网络的整体速度</strong>。</p></li><li><p><strong>SRCNN非线性映射层的计算代价太高，参数过多(高维度下的映射)。</strong></p></li></ul></li><li><p>FSRCNN在SRCNN基础上做了如下改变：</p><ul><li><p><strong>FSRCNN直接采用低分辨的图像作为输入，不同于SRCNN需要先对低分辨率的图像进行双三次插值然后作为输入，FSRCNN在网络的最后采用反卷积层实现上采样，小尺寸的图像在映射学习阶段可以有效地提升运算速度</strong></p></li><li><p><strong>FSRCNN中没有非线性映射，相应地出现了收缩层、映射（多个卷积核为3*3的层）和扩展层，在低维空间中进行映射学习可以有效地提升运算速度</strong></p></li><li><p><strong>FSRCNN选择更小尺寸的滤波器和更深的网络结构。</strong></p></li><li><p><strong>所有卷积层（反卷积层除外）都可以由不同放大因子的网络共享，能够在保持恢复质量（即图像超分辨率重建后的质量）不降低的前提下，通过迁移（或共享）其卷积层来快速地对不同放大因子（upscaling factors）的图像进行训练和测试。</strong></p><p>具体来说，我们可以从以下几个方面来理解这句话：</p><ol><li><strong>卷积层的迁移</strong>：在深度学习中，迁移学习是一种常见的技术，它允许将一个已经训练好的模型的部分（如网络层）用于另一个相关的任务中，以减少训练时间和提高模型性能。<strong>FSRCNN利用这一思想，将网络中用于特征提取的卷积层设计为与放大因子无关。这意味着，无论是将图像放大2倍、3倍还是4倍，这些卷积层都可以保持不变，而不需要为每个放大因子重新训练。</strong></li><li><strong>快速训练和测试</strong>：由于卷积层可以跨不同的放大因子进行迁移，因此FSRCNN在训练和测试时可以节省大量时间。对于一个新的放大因子，我们只需要重新训练或调整网络的反卷积层，该层直接负责根据给定的放大因子来重建图像。这种方式大大减少了训练成本，并加快了测试速度。</li><li><strong>无损失的恢复质量</strong>：尽管FSRCNN通过迁移卷积层来简化训练和测试过程，但它并没有牺牲图像超分辨率重建的质量。这得益于其网络架构的精心设计，特别是反卷积层能够根据目标放大因子有效地重建图像细节。因此，无论是在哪个放大因子下，FSRCNN都能提供高质量的图像恢复结果。</li></ol></li></ul></li><li><p>对比</p><ul><li><p>SRCNN结构（飞镖型维度变化：小-大-小，前端上采样）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SRCNN</span>(nn.Module): <span class="comment">#输入的图像是经过双三次插值后上采样放大尺寸的低分辨率图像</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_channels=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SRCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(num_channels, <span class="number">64</span>, kernel_size=<span class="number">9</span>, padding=<span class="number">9</span> // <span class="number">2</span>) <span class="comment">#特征提取层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">64</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">5</span> // <span class="number">2</span>) <span class="comment">#映射层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv3 = nn.Conv2d(<span class="number">32</span>, num_channels, kernel_size=<span class="number">5</span>, padding=<span class="number">5</span> // <span class="number">2</span>) <span class="comment">#重建层</span></span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.conv1(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.conv2(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.conv3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></li><li><p>FSRCNN结构（沙漏型维度变化：大-小-大，后端上采样）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, scale_factor, num_channels=<span class="number">1</span>, d=<span class="number">56</span>, s=<span class="number">12</span>, m=<span class="number">4</span></span>):</span><br><span class="line">       <span class="built_in">super</span>(FSRCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">       <span class="variable language_">self</span>.first_part = nn.Sequential(</span><br><span class="line">           nn.Conv2d(num_channels, d, kernel_size=<span class="number">5</span>, padding=<span class="number">5</span>//<span class="number">2</span>),</span><br><span class="line">           nn.PReLU(d)</span><br><span class="line">       )</span><br><span class="line">       <span class="variable language_">self</span>.mid_part = [nn.Conv2d(d, s, kernel_size=<span class="number">1</span>), nn.PReLU(s)]</span><br><span class="line">       <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">           <span class="variable language_">self</span>.mid_part.extend([nn.Conv2d(s, s, kernel_size=<span class="number">3</span>, padding=<span class="number">3</span>//<span class="number">2</span>), nn.PReLU(s)])<span class="comment">#进行m次s个通道到s个通道的映射</span></span><br><span class="line">       <span class="variable language_">self</span>.mid_part.extend([nn.Conv2d(s, d, kernel_size=<span class="number">1</span>), nn.PReLU(d)])</span><br><span class="line">       <span class="variable language_">self</span>.mid_part = nn.Sequential(*<span class="variable language_">self</span>.mid_part)<span class="comment">#把中间层的卷积都封装起来</span></span><br><span class="line">       <span class="variable language_">self</span>.last_part = nn.ConvTranspose2d(d, num_channels, kernel_size=<span class="number">9</span>, stride=scale_factor, padding=<span class="number">9</span>//<span class="number">2</span>,</span><br><span class="line">                                           output_padding=scale_factor-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">       <span class="variable language_">self</span>._initialize_weights()</span><br><span class="line">    </span><br></pre></td></tr></table></figure></li><li><p>图片:conv(卷积核大小，输出维度，输入维度)</p><ul><li><img src="https://s21.ax1x.com/2024/09/06/pAZz6IA.png" alt="pAZz6IA.png"></li></ul></li></ul></li></ul><h2 id="二-数据集"><a href="#二-数据集" class="headerlink" title="二.数据集"></a>二.数据集</h2><p><strong>以img-91作为训练集，Set5作为测试集。</strong></p><h2 id="三-模型搭建"><a href="#三-模型搭建" class="headerlink" title="三.模型搭建"></a>三.模型搭建</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FSRCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, scale_factor, num_channels=<span class="number">1</span>, d=<span class="number">56</span>, s=<span class="number">12</span>, m=<span class="number">4</span></span>): <span class="comment">#缩放因子，输入维度，模型各个部分的超参数</span></span><br><span class="line">        <span class="built_in">super</span>(FSRCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment">#模型的第一部分：特征提取层</span></span><br><span class="line">        <span class="variable language_">self</span>.first_part = nn.Sequential(</span><br><span class="line">            nn.Conv2d(num_channels, d, kernel_size=<span class="number">5</span>, padding=<span class="number">5</span>//<span class="number">2</span>),</span><br><span class="line">            nn.PReLU(d)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#模型的第二部分：</span></span><br><span class="line">        <span class="variable language_">self</span>.mid_part = [nn.Conv2d(d, s, kernel_size=<span class="number">1</span>), nn.PReLU(s)] <span class="comment">#收缩层，缩小维度</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m):<span class="comment">#执行4次，卷积核大小为3*3的非线性映射层</span></span><br><span class="line">            <span class="variable language_">self</span>.mid_part.extend([nn.Conv2d(s, s, kernel_size=<span class="number">3</span>, padding=<span class="number">3</span>//<span class="number">2</span>), nn.PReLU(s)])</span><br><span class="line">        <span class="variable language_">self</span>.mid_part.extend([nn.Conv2d(s, d, kernel_size=<span class="number">1</span>), nn.PReLU(d)]) <span class="comment">#扩充层，恢复到原来的维度</span></span><br><span class="line">        <span class="variable language_">self</span>.mid_part = nn.Sequential(*<span class="variable language_">self</span>.mid_part) <span class="comment">#通过nn.Sequential(*self.mid_part)转换为一个Sequential模块</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#模型的第三部分：转置卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.last_part = nn.ConvTranspose2d(d, num_channels, kernel_size=<span class="number">9</span>, stride=scale_factor, padding=<span class="number">9</span>//<span class="number">2</span>,</span><br><span class="line">                                            output_padding=scale_factor-<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#在输出特征图的边缘额外添加的零填充的层数。这个参数在转置卷积中非常重要，因为它允许我们更精确地控制输出特征图的大小。</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>._initialize_weights()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>): <span class="comment">#权重初始化</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="variable language_">self</span>.first_part:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.normal_(m.weight.data, mean=<span class="number">0.0</span>, std=math.sqrt(<span class="number">2</span>/(m.out_channels*m.weight.data[<span class="number">0</span>][<span class="number">0</span>].numel())))</span><br><span class="line">                nn.init.zeros_(m.bias.data)</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="variable language_">self</span>.mid_part:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.normal_(m.weight.data, mean=<span class="number">0.0</span>, std=math.sqrt(<span class="number">2</span>/(m.out_channels*m.weight.data[<span class="number">0</span>][<span class="number">0</span>].numel())))</span><br><span class="line">                nn.init.zeros_(m.bias.data)</span><br><span class="line">        nn.init.normal_(<span class="variable language_">self</span>.last_part.weight.data, mean=<span class="number">0.0</span>, std=<span class="number">0.001</span>)</span><br><span class="line">        nn.init.zeros_(<span class="variable language_">self</span>.last_part.bias.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.first_part(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.mid_part(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.last_part(x)</span><br><span class="line">        <span class="keyword">return</span> x*</span><br></pre></td></tr></table></figure><h2 id="四-模型训练"><a href="#四-模型训练" class="headerlink" title="四.模型训练"></a>四.模型训练</h2><ul><li><p>代码与上一章内容相同</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py --train-file=data_set/train_set/<span class="number">91</span>-image_x3.h5 --<span class="built_in">eval</span>-file=data_set/eval_set/Set5_x3.h5 --outputs-<span class="built_in">dir</span>=outputs <span class="comment">#运行命令</span></span><br></pre></td></tr></table></figure></li></ul><p>结果：<img src="https://s21.ax1x.com/2024/09/14/pAuVdT1.png" alt="pAuVdT1.png"></p><h2 id="五-模型测试"><a href="#五-模型测试" class="headerlink" title="五.模型测试"></a>五.模型测试</h2><blockquote><p>这里将低分辨率进行超分处理后的预测Y通道与双三次插值后图像的 Cb、Cr通道的结合的意义：</p><p>YCbCr色彩空间中的Y代表亮度信息，而Cb和Cr代表色度信息（蓝色和红色的色度差）。RGB色彩空间则直接由红（R）、绿（G）、蓝（B）三个颜色通道组成。</p><p>在图像处理中，由于人眼对亮度的敏感度高于对色度的敏感度，因此很多图像处理算法会选择在YCbCr色彩空间中进行处理，尤其是在超分辨率等任务中，可以独立地对亮度（Y通道）和色度（Cb、Cr通道）进行处理，以达到更好的视觉效果和计算效率。</p><p>在超分辨率任务中，由于亮度信息（Y通道）包含了图像的主要结构信息，因此通常会通过深度学习等算法对低分辨率图像的Y通道进行预测，以获得高分辨率的Y通道。而色度信息（Cb、Cr通道）则相对简单，可以通过传统的插值方法（如双三次插值）从低分辨率图像中直接获得高分辨率的版本。</p><p>将预测的Y通道与bicubic插值得到的Cb、Cr通道结合，实际上是在保持色度信息不变的同时，仅对亮度信息进行增强或修正。这样做的好处是可以在保持图像颜色自然性的同时，显著提高图像的清晰度和细节表现。</p></blockquote><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">cudnn.benchmark = <span class="literal">True</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">model = FSRCNN(scale_factor=args.scale).to(device)</span><br><span class="line"></span><br><span class="line">state_dict = model.state_dict()</span><br><span class="line"><span class="comment">#加载模型权重文件</span></span><br><span class="line"><span class="keyword">for</span> n, p <span class="keyword">in</span> torch.load(args.weights_file, map_location=<span class="keyword">lambda</span> storage, loc: storage).items():</span><br><span class="line">    <span class="keyword">if</span> n <span class="keyword">in</span> state_dict.keys():</span><br><span class="line">        state_dict[n].copy_(p)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> KeyError(n)</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 计算调整后的图像尺寸，确保它是缩放因子的整数倍</span></span><br><span class="line">image = pil_image.<span class="built_in">open</span>(args.image_file).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">image_width = (image.width // args.scale) * args.scale</span><br><span class="line">image_height = (image.height // args.scale) * args.scale</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成高分辨率（HR）、低分辨率（LR）和双三次插值放大后的图像（bicubic）  </span></span><br><span class="line">hr = image.resize((image_width, image_height), resample=pil_image.BICUBIC)</span><br><span class="line">lr = hr.resize((hr.width // args.scale, hr.height // args.scale), resample=pil_image.BICUBIC) <span class="comment">#下采样降低分辨率</span></span><br><span class="line">bicubic = lr.resize((lr.width * args.scale, lr.height * args.scale), resample=pil_image.BICUBIC) <span class="comment">#双三次插值增大分辨率</span></span><br><span class="line">bicubic.save(args.image_file.replace(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;_bicubic_x&#123;&#125;.&#x27;</span>.<span class="built_in">format</span>(args.scale)))</span><br><span class="line"></span><br><span class="line"><span class="comment">#将RGB图像转换为YCbCr色彩空间，并返回处理后的Y通道和原始的YCbCr图像，高分辨率和低分辨率图像接受Y通道，而双三次插值图像接受色度通道（（Cb、Cr通道）</span></span><br><span class="line">lr, _ = preprocess(lr, device)</span><br><span class="line">hr, _ = preprocess(hr, device)</span><br><span class="line">_, ycbcr = preprocess(bicubic, device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    preds = model(lr).clamp(<span class="number">0.0</span>, <span class="number">1.0</span>) <span class="comment">#此处只需输入原始低分辨率图像，不需要输入双三次插值后的图像，此时进行将超分处理。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算峰值信噪比</span></span><br><span class="line">psnr = calc_psnr(hr, preds)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;PSNR: &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(psnr))</span><br><span class="line"></span><br><span class="line">preds = preds.mul(<span class="number">255.0</span>).cpu().numpy().squeeze(<span class="number">0</span>).squeeze(<span class="number">0</span>)<span class="comment">#将模型预测结果缩放到0-255范围，并转换为NumPy数组，去除不必要的维度。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将预测的Y通道与bicubic图像的Cb和Cr通道结合，之后转换为RGB格式，裁剪到0-255范围，转换为PIL图像，并保存。</span></span><br><span class="line">output = np.array([preds, ycbcr[..., <span class="number">1</span>], ycbcr[..., <span class="number">2</span>]]).transpose([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line">output = np.clip(convert_ycbcr_to_rgb(output), <span class="number">0.0</span>, <span class="number">255.0</span>).astype(np.uint8)</span><br><span class="line">output = pil_image.fromarray(output)</span><br><span class="line">output.save(args.image_file.replace(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;_fsrcnn_x&#123;&#125;.&#x27;</span>.<span class="built_in">format</span>(args.scale)))</span><br><span class="line"></span><br><span class="line"><span class="comment">#python test.py --weights-file=outputs/x3/best.pth --image-file=data/car.bmp</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;超分辨率第二章-FSRCNN&quot;&gt;&lt;a href=&quot;#超分辨率第二章-FSRCNN&quot; class=&quot;headerlink&quot; title=&quot;超分辨率第二章-FSRCNN&quot;&gt;&lt;/a&gt;超分辨率第二章-FSRCNN&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;FSRCNN是2016年提出的超分辨率模型，使用后端上采样（转置卷积的方法），在一定程度上解决了SRCNN的问题。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/zzy_pphz/article/details/108408933&quot;&gt;参考博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1608.00367&quot;&gt;论文地址&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;模型位置：F:&#92;Github下载&#92;FSRCNN-pytorch-master&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="硕士阶段学习笔记(入门阶段)" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/"/>
    
    <category term="模型" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="超分辨率" scheme="http://example.com/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>基于参考图像的超分辨率-综述</title>
    <link href="http://example.com/2024/09/09/%E5%9F%BA%E4%BA%8E%E5%8F%82%E8%80%83%E5%9B%BE%E5%83%8F%E7%9A%84%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87-%E7%BB%BC%E8%BF%B0/"/>
    <id>http://example.com/2024/09/09/%E5%9F%BA%E4%BA%8E%E5%8F%82%E8%80%83%E5%9B%BE%E5%83%8F%E7%9A%84%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87-%E7%BB%BC%E8%BF%B0/</id>
    <published>2024-09-09T08:05:20.000Z</published>
    <updated>2024-09-09T13:22:21.184Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基于参考图像的超分辨率-综述"><a href="#基于参考图像的超分辨率-综述" class="headerlink" title="基于参考图像的超分辨率-综述"></a>基于参考图像的超分辨率-综述</h1><blockquote><p>参考文献：<a href="https://kns.cnki.net/kcms2/article/abstract?v=-4s28oSk478lMupsZHFTRSDE7aArCjQPFtapPdcSezJKaonTlxOG4MtmhRxxWeFwj3rVlAtCBbYqf-RpkIRuaI4yEOkQLZVycoNwfeD0FW1fE8xfDpyBhZyfF1CqRgwDJ0BcE_ZKE9JZw7tiLkd2LRrG5V93dZAhGDzOTeIfu9sLZLlxk2cQW3fgHfb83_NtXHT9F7iUTpH7FAhPxZowhKalw5i1BSKcAGX5dXZTpjBFE2-AYcZvxBqb_q-t7-uXP2mxU5s4TJvdkWCzxBTTPt2ziqoXmFwMWiRiNA9bTsmm9P2Mma7B5Gd50JmDLFPaPuSmU7uRDH0=&amp;uniplatform=NZKPT&amp;language=CHS">基于参考图像的超分辨率重建算法综述</a></p></blockquote><span id="more"></span><h2 id="一-RefSR介绍"><a href="#一-RefSR介绍" class="headerlink" title="一.RefSR介绍"></a>一.RefSR介绍</h2><ul><li><p>将已有的高分辨率图像作为参考， 利用<strong>参考图</strong>中丰富的纹理来补偿低分辨率图像缺失的细节信息， 从而缓解单帧图像超分辨重建的不适定性</p></li><li><p>基于参考图的超分辨率重建， 与普通的单帧重建不同， 除输入低分辨率图像外， <strong>还需额外输入一张或多张与低分辨率图像内容或纹理相似的高分辨率参考图像</strong></p></li></ul><h2 id="二-RefSR的主要方法"><a href="#二-RefSR的主要方法" class="headerlink" title="二.RefSR的主要方法"></a>二.RefSR的主要方法</h2><ul><li><p>依据Ref图像与LR图像的对应方式， 可以将RefSR分为两大类： 基于图像对齐的方法和基于图像块匹配的方法</p></li><li><p>基于图像对齐的RefSR方法是利用光流、 可变形卷积等模型将输入的LR图像与Ref图像进行全局配准， 再将对齐后的Ref图像的纹理用于LR图像重建， 其代表模型有SSEN、Cross-Net</p></li><li>而基于图像块匹配的RfSR方法则是将输入图像分割为若干个块，对每个块进行相似度匹配，再利用匹配后的LR/Rf图像进行重建。该方法代表模型有SRNTT、TTSR、MASA等。</li><li><img src="https://s21.ax1x.com/2024/09/09/pAmETsA.png" alt="pAmETsA.png"></li></ul><h3 id="1-基于图像对齐的方法"><a href="#1-基于图像对齐的方法" class="headerlink" title="1.基于图像对齐的方法"></a>1.基于图像对齐的方法</h3><ul><li><img src="https://s21.ax1x.com/2024/09/09/pAmAfjs.png" alt="pAmAfjs.png"></li></ul><h4 id="①CrossNet（跨尺度端到端的网络）"><a href="#①CrossNet（跨尺度端到端的网络）" class="headerlink" title="①CrossNet（跨尺度端到端的网络）"></a>①CrossNet（跨尺度端到端的网络）</h4><ul><li><p>其中图像对齐使用了<strong>光流法</strong>。</p><ul><li>光流：光流的概念是指在连续的两帧图像中由于图像中的物体移动或者摄像头的移动导致的图像中目标像素的移动</li></ul></li><li><p>该网络利用SISR方法对LR图像进行上采样，得到与Ref图像相同大小的上采样图，再分别提取出上采样图和Rf图像的多尺度特征。</p></li><li><p><strong>然后，利用改进的FlowNetS模型学习不同尺度LR/Ref图像特征的光流信息来更新Ref图像，从而实现图像对齐操作</strong>。</p><ul><li><blockquote><ul><li>数据预处理<ul><li>准备LR（低分辨率）图像和Ref（参考或高分辨率）图像对。这些图像对可能是从不同来源或在不同条件下获取的，因此它们之间可能存在位移、旋转、缩放等差异。</li><li>可能需要将图像对缩放到不同的尺度，以便在多个尺度上分析光流信息。这有助于捕捉不同尺度的运动模式，从而提高光流估计的准确性。</li></ul></li><li>改进的FlowNetS模型<ul><li>FlowNetS是一个基于卷积神经网络（CNN）的光流估计模型，<strong>它接受一对图像作为输入，并输出一个与输入图像相同大小的光流场。光流场中的每个像素值表示该像素在另一张图像中的对应位置偏移。</strong></li><li>为了适应特定的任务需求，FlowNetS模型可能经过了改进，比如增加了更多的卷积层、使用了更高效的特征提取器、或者修改了损失函数等。</li></ul></li><li>光流信息学习<ul><li>将LR和Ref图像对输入到改进的FlowNetS模型中。</li><li>模型会分析图像对之间的特征差异，并学习如何将这些差异转化为光流信息。在多个尺度上进行这个操作可以捕获更丰富的运动模式。</li></ul></li><li>Ref图像更新<ul><li><strong>利用学习到的光流信息，可以对Ref图像进行变换，以使其与LR图像在视觉上对齐。这通常涉及到对Ref图像中的每个像素位置进行偏移，偏移量由光流场中的相应像素值给出。</strong></li><li>由于光流信息可能包含噪声或估计误差，因此在执行更新之前，可能需要对光流场进行平滑处理或滤波操作。</li></ul></li><li>图像对齐操作<ul><li>经过Ref图像的更新后，LR和Ref图像在视觉上变得更加对齐。这种对齐可以用于多种应用场景，比如图像超分辨率、视频帧插值、图像融合等。</li><li>需要注意的是，由于光流估计的局限性（如遮挡、大位移、快速运动等），在某些情况下，图像对齐可能无法达到完美的效果。</li></ul></li></ul></blockquote></li></ul></li><li><p>最后，通过融合操作重建出HR图像。该方法假设两幅图像具有较强的相似性，所以当LR/Rf图像相关性不强时，效果会有下降。</p></li></ul><h4 id="②SSEN"><a href="#②SSEN" class="headerlink" title="②SSEN"></a>②SSEN</h4><ul><li><strong>SSEN利用可变形卷积来寻找LR/Rf图像的对应关系，且用动态偏移估计器对可变形卷积的偏移量进行估计。</strong><ul><li>首先，通过特征提取网络（如卷积神经网络）从LR和Ref图像中提取特征图。</li><li>然后，将特征图输入到动态偏移估计器中，计算得到每个卷积核采样点的偏移量。</li><li>最后，使用这些偏移量来指导可变形卷积操作，从而生成新的特征图或超分辨率图像</li></ul></li><li>同时，为了捕获特征内部和特征之间的全局相关性，在动态偏移估计器中加入了非局部块。该方法能够处理非刚性变换的图像，计算量较小，但是无法解决长距离对应问题。</li></ul><h3 id="2-基于图像块匹配的方法"><a href="#2-基于图像块匹配的方法" class="headerlink" title="2.基于图像块匹配的方法"></a>2.基于图像块匹配的方法</h3><p><img src="https://s21.ax1x.com/2024/09/09/pAmENV0.png" alt="pAmENV0.png"></p><ul><li>图像块匹配：Rf图像下采样后，计算其一阶、二阶梯度，利用欧式距离在梯度特征图中寻找与LR图像最相邻的9个块，然后加权平均得到融合结果。</li></ul><h4 id="①CC-Net-SS-Net"><a href="#①CC-Net-SS-Net" class="headerlink" title="①CC-Net+SS-Net"></a>①CC-Net+SS-Net</h4><ul><li>使用CC-Net(cross-scale correspondence network)模块，通过卷积神经网络提取LR/Rf图像特征，利用内积计算相似度，选择相似度最高的图像块作为匹配<br>对</li><li>然后，将匹配好的图像块送入到SS-Net(super-resolution synthesis network)模块进行多尺度融合，最后得到SR图像。</li></ul><h4 id="②SRNTT（super-resolution-by-neural-texture-transfer）"><a href="#②SRNTT（super-resolution-by-neural-texture-transfer）" class="headerlink" title="②SRNTT（super-resolution by neural texture transfer）"></a>②SRNTT（super-resolution by neural texture transfer）</h4><ul><li>首先， 对提取的图像特征进行密集块匹配， 然后将匹配后的Ref图像进行纹理迁移， 从而使得重建的SR图像拥有丰富的纹理信息。 </li></ul><h4 id="③TTSR（texture-transformer-network-for-image-su-per-resolution）"><a href="#③TTSR（texture-transformer-network-for-image-su-per-resolution）" class="headerlink" title="③TTSR（texture transformer network for image su-per-resolution）"></a>③TTSR（texture transformer network for image su-per-resolution）</h4><ul><li>该网络通过使用注意力机制挖掘深层次的特征对应关系，再将匹配好的特征送入跨尺度融合模块，最后得到SR图像。</li></ul><h4 id="④MASA"><a href="#④MASA" class="headerlink" title="④MASA"></a>④MASA</h4><ul><li>首先对LR/Rf图像进行一个大尺度的图像块匹配，再利用图像的局部相似性，对大尺度图形块分块进行小尺度匹配。</li></ul><h4 id="⑤AMSA"><a href="#⑤AMSA" class="headerlink" title="⑤AMSA"></a>⑤AMSA</h4><ul><li>针对小规模的不对齐问题，提出了动态融合模块；针对大尺度不对齐问题，提出了多规模融合模块。这两个融合模块相互配合，产生了很好的融合效果。</li></ul><h4 id="⑥-C-2-Matching"><a href="#⑥-C-2-Matching" class="headerlink" title="⑥$C^2$-Matching"></a>⑥$C^2$-Matching</h4><ul><li>cross transformation and cross resolution matching技术，用于跨变换和跨分辨率的关系匹配。</li><li>对于变换差距，利用对比网络拉近匹配对之间的距离，疏远不匹配对之间的距离</li><li>对于分辨率差距，提出教师学生关系蒸馏网络，与传统的知识蒸馏网络不同，此网络用HR-HR匹配来指导相对困难的LR-HR匹配。然后，通过设计的动态融合模块来解决潜在的错位问题。</li><li>该技术在目前常用的数据集中，都显现出了极强的泛化能力，以及对大尺度和旋转变换的鲁棒性。</li></ul><h2 id="三-损失函数"><a href="#三-损失函数" class="headerlink" title="三.损失函数"></a>三.损失函数</h2><h3 id="1-重建损失"><a href="#1-重建损失" class="headerlink" title="1.重建损失"></a>1.重建损失</h3><ul><li>可分为L1,L2损失</li><li><script type="math/tex; mode=display">L _ { r e c - l 1 } ( I ^ { S R } , I ) = \frac { 1 } { h w c } \sum _ { i , j , k } | I _ { i , j , k } ^ { s R } - I _ { i , j , k } | ,</script></li><li><script type="math/tex; mode=display">L _ { r e c - l2 } ( I ^ { S R } , I ) = \frac { 1 } { h w c } \sum _ { i , j , k } ( I _ { i , j , k } ^ { s R } - I _ { i , j , k } ) ^ { 2 } 。</script></li><li>h，w和c分别表示图像的长、宽和通道数；ISR表示生成的HR图像；I表示真实图像。</li></ul><h3 id="2-感知损失"><a href="#2-感知损失" class="headerlink" title="2.感知损失"></a>2.感知损失</h3><ul><li><script type="math/tex; mode=display">L _ { p e r } = | | \varphi _ { i } ( I ^ { S R } ) - \varphi _ { i } ( I ) | | _ { 2 } 。</script></li></ul><h3 id="3-对抗损失"><a href="#3-对抗损失" class="headerlink" title="3.对抗损失"></a>3.对抗损失</h3><ul><li><script type="math/tex; mode=display">L _ { g a n - c e - g } = - \lg D ( I ^ { S R } ) ,</script></li></ul><h3 id="4-纹理损失"><a href="#4-纹理损失" class="headerlink" title="4.纹理损失"></a>4.纹理损失</h3><ul><li><script type="math/tex; mode=display">L _ { t e x } = \sum _ { l } \lambda _ { l } | | G _ { r } ( \varphi _ { l } ( I ^ { S R } ) \odot S _ { l } ^ { * } ) - G _ { r } ( \varphi _ { l } ( I ^ { R e f } ) \odot S _ { l } ^ { * } ) | _ { F } 。</script></li></ul><h2 id="四-总结"><a href="#四-总结" class="headerlink" title="四.总结"></a>四.总结</h2><ul><li><img src="https://s21.ax1x.com/2024/09/09/pAmn5DJ.png" alt="pAmn5DJ.png"></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;基于参考图像的超分辨率-综述&quot;&gt;&lt;a href=&quot;#基于参考图像的超分辨率-综述&quot; class=&quot;headerlink&quot; title=&quot;基于参考图像的超分辨率-综述&quot;&gt;&lt;/a&gt;基于参考图像的超分辨率-综述&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;参考文献：&lt;a href=&quot;https://kns.cnki.net/kcms2/article/abstract?v=-4s28oSk478lMupsZHFTRSDE7aArCjQPFtapPdcSezJKaonTlxOG4MtmhRxxWeFwj3rVlAtCBbYqf-RpkIRuaI4yEOkQLZVycoNwfeD0FW1fE8xfDpyBhZyfF1CqRgwDJ0BcE_ZKE9JZw7tiLkd2LRrG5V93dZAhGDzOTeIfu9sLZLlxk2cQW3fgHfb83_NtXHT9F7iUTpH7FAhPxZowhKalw5i1BSKcAGX5dXZTpjBFE2-AYcZvxBqb_q-t7-uXP2mxU5s4TJvdkWCzxBTTPt2ziqoXmFwMWiRiNA9bTsmm9P2Mma7B5Gd50JmDLFPaPuSmU7uRDH0=&amp;amp;uniplatform=NZKPT&amp;amp;language=CHS&quot;&gt;基于参考图像的超分辨率重建算法综述&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="学术论文总结" scheme="http://example.com/categories/%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="超分辨率" scheme="http://example.com/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>超分辨率第一章-SRCNN</title>
    <link href="http://example.com/2024/09/08/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%AC%AC%E4%B8%80%E7%AB%A0-SRCNN/"/>
    <id>http://example.com/2024/09/08/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%AC%AC%E4%B8%80%E7%AB%A0-SRCNN/</id>
    <published>2024-09-08T08:58:27.000Z</published>
    <updated>2024-09-20T13:04:53.071Z</updated>
    
    <content type="html"><![CDATA[<h1 id="超分辨率第一章-SRCNN"><a href="#超分辨率第一章-SRCNN" class="headerlink" title="超分辨率第一章-SRCNN"></a>超分辨率第一章-SRCNN</h1><blockquote><p>第一个超分辨率模型-SRCNN （SISR），2014年提出</p><p>参考网址：<a href="https://blog.csdn.net/zhanjuex/article/details/124344864?spm=1001.2014.3001.5506">【超分辨率】【深度学习】SRCNN pytorch代码（附详细注释和数据集）_srcnn代码-CSDN博客</a></p><p><a href="https://blog.csdn.net/weixin_52261094/article/details/128389448">SRCNN超分辨率Pytorch实现，代码逐行讲解，附源码</a></p><p>模型位置：F:\Github下载\SRCNN_Pytorch_1.0-master</p></blockquote><span id="more"></span><h2 id="一-模型介绍"><a href="#一-模型介绍" class="headerlink" title="一.模型介绍"></a>一.模型介绍</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#运行程序</span></span><br><span class="line">python train.py --train-file=data_set/train_set/<span class="number">91</span>-image_x3.h5 --<span class="built_in">eval</span>-file=data_set/eval_set/Set5_x3.h5 --outputs-<span class="built_in">dir</span>=outputs</span><br></pre></td></tr></table></figure><p><strong>SRCNN（2014年Dong等人提出，前端上采样框架 ）</strong></p><ul><li>先将图片下采样预处理得到低分辨率图像 </li><li>再利用双三次插值法将图片放大到目标分辨率（基于插值的上采样方法）</li><li>再用卷积核大小分别为 9×9、1×1、5×5的三个卷积层，分别进行特征提取，拟合 LR-HR 图像对之间的非线性映射以及将网络模型的输出结果进行重建，得到最后的高分辨率图像</li><li>图示：<img src="https://s21.ax1x.com/2024/09/06/pAZzAbQ.png" alt="pAZzAbQ.png"></li></ul><h2 id="二-数据集"><a href="#二-数据集" class="headerlink" title="二.数据集"></a>二.数据集</h2><p><strong>以img-91作为训练集，Set5作为测试集。</strong></p><h2 id="三-模型搭建"><a href="#三-模型搭建" class="headerlink" title="三.模型搭建"></a>三.模型搭建</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SRCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_channels=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SRCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(num_channels, <span class="number">64</span>, kernel_size=<span class="number">9</span>, padding=<span class="number">9</span> // <span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">64</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">5</span> // <span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv3 = nn.Conv2d(<span class="number">32</span>, num_channels, kernel_size=<span class="number">5</span>, padding=<span class="number">5</span> // <span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.conv1(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.conv2(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.conv3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="四-模型训练"><a href="#四-模型训练" class="headerlink" title="四.模型训练"></a>四.模型训练</h2><h3 id="1-调用库"><a href="#1-调用库" class="headerlink" title="1.调用库"></a>1.调用库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse <span class="comment">#argparse用于编写用户友好的命令行接口。程序通过定义它期望从命令行接收的参数，然后 argparse 会自动从 sys.argv 解析出那些参数。这允许你的程序更加灵活和可配置</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment">#cudnn 是 NVIDIA 提供的深度神经网络加速库（cuDNN）的 PyTorch 接口。它可以提高深度学习模型的计算速度和效率，特别是在使用 NVIDIA GPU 时。</span></span><br><span class="line"><span class="keyword">import</span> torch.backends.cudnn <span class="keyword">as</span> cudnn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataloader <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="comment">#进度条</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> SRCNN</span><br><span class="line"><span class="comment">#从 datasets 模块中导入了 TrainDataset 和 EvalDataset 两个类。这两个类很可能分别用于加载训练数据集和评估数据集。</span></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> TrainDataset, EvalDataset</span><br><span class="line"><span class="comment"># utils 模块中导入了 AverageMeter 和 calc_psnr 两个工具或函数。AverageMeter 可能是一个用于计算平均值的工具类，而 calc_psnr 函数则用于计算峰值信噪比（Peak Signal-to-Noise Ratio），这是一种常用的图像质量评估指标。</span></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> AverageMeter, calc_psnr</span><br></pre></td></tr></table></figure><h3 id="2-命令行参数设定"><a href="#2-命令行参数设定" class="headerlink" title="2.命令行参数设定"></a>2.命令行参数设定</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始参数设定，argparse是Python标准库中的一个模块，用于编写用户友好的命令行接口。程序定义了它期望从命令行接收的参数，然后argparse会自动从sys.argv解析出那些参数。</span></span><br><span class="line">parser = argparse.ArgumentParser() <span class="comment">#parser = argparse.ArgumentParser() 创建了一个ArgumentParser对象。这个对象将包含将命令行解析成Python数据类型所需的全部信息。</span></span><br><span class="line"><span class="comment">#通过调用parser.add_argument()方法，可以向解析器添加命令行参数。每个add_argument()调用都指定了一个命令行选项（如--train-file），并可能包含一些额外的参数（如type=str，required=True等），这些参数定义了命令行选项应该如何被解析。</span></span><br><span class="line"><span class="comment">#--train-file, --eval-file, --outputs-dir：这些参数被标记为required=True，意味着它们在命令行中必须被提供。其他参数有默认值。</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--train-file&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, required=<span class="literal">True</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--eval-file&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, required=<span class="literal">True</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--outputs-dir&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, required=<span class="literal">True</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--scale&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">3</span>) <span class="comment">#图片上采样放大尺寸倍数</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">1e-4</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--batch-size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">16</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--num-workers&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--num-epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">400</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">123</span>)</span><br><span class="line">args = parser.parse_args() <span class="comment">#解析命令行参数，并将结果存储在名为args的命名空间中。之后，你可以通过args.参数名的方式来访问这些参数的值。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#保存输出到相应目录下</span></span><br><span class="line">args.outputs_dir = os.path.join(args.outputs_dir, <span class="string">&#x27;x&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(args.scale))</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(args.outputs_dir):</span><br><span class="line">    os.makedirs(args.outputs_dir)</span><br></pre></td></tr></table></figure><h3 id="3-加载数据集并进行预处理"><a href="#3-加载数据集并进行预处理" class="headerlink" title="3.加载数据集并进行预处理"></a>3.加载数据集并进行预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预处理训练集</span></span><br><span class="line">train_dataset = TrainDataset(args.train_file)</span><br><span class="line">train_dataloader = DataLoader(</span><br><span class="line">            dataset=train_dataset,</span><br><span class="line">            batch_size=args.batch_size,</span><br><span class="line">            shuffle=<span class="literal">True</span>,</span><br><span class="line">            num_workers=args.num_workers,</span><br><span class="line">            pin_memory=<span class="literal">True</span>,</span><br><span class="line">            drop_last=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#预处理验证集</span></span><br><span class="line">eval_dataset = EvalDataset(args.eval_file)</span><br><span class="line">eval_dataloader = DataLoader(dataset=eval_dataset, batch_size=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="4-设置训练参数"><a href="#4-设置训练参数" class="headerlink" title="4.设置训练参数"></a>4.设置训练参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cudnn.benchmark = <span class="literal">True</span> <span class="comment">#开启cudnn的benchmark模式，用于加速计算。但请注意，这可能会导致每次运行程序时，前馈计算的结果有细微差异，因为cudnn会寻找最优的卷积算法。</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">torch.manual_seed(args.seed) <span class="comment">#随机数种子</span></span><br><span class="line">model = SRCNN().to(device)</span><br><span class="line">criterion = nn.MSELoss() <span class="comment">#代价函数MSE</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义优化器为Adam，并为模型的不同部分设置不同的学习率。这里，conv3层的学习率是conv1和conv2层学习率的十分之一。</span></span><br><span class="line">optimizer = optim.Adam([</span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>: model.conv1.parameters()&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>: model.conv2.parameters()&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>: model.conv3.parameters(), <span class="string">&#x27;lr&#x27;</span>: args.lr*<span class="number">0.1</span>&#125;</span><br><span class="line">], lr=args.lr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练开始前，复制当前模型的最佳权重（这里初始化为当前模型的权重）。这些权重将在验证过程中根据性能进行更新。  </span></span><br><span class="line">best_weights = copy.deepcopy(model.state_dict())</span><br><span class="line">best_epoch = <span class="number">0</span></span><br><span class="line">best_psnr = <span class="number">0.0</span></span><br></pre></td></tr></table></figure><h3 id="5-模型训练与验证"><a href="#5-模型训练与验证" class="headerlink" title="5.模型训练与验证"></a>5.模型训练与验证</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.num_epochs):</span><br><span class="line">    <span class="comment">#训练模式</span></span><br><span class="line">    model.train()</span><br><span class="line">    epoch_losses = AverageMeter() <span class="comment">#初始化一个AverageMeter对象来跟踪当前epoch的损失平均值</span></span><br><span class="line">    <span class="comment">#创建一个进度条，并设置进度条描述（当前轮/总批次）</span></span><br><span class="line">    <span class="keyword">with</span> tqdm(total=(<span class="built_in">len</span>(train_dataset) - <span class="built_in">len</span>(train_dataset) % args.batch_size)) <span class="keyword">as</span> t:</span><br><span class="line">        t.set_description(<span class="string">&#x27;epoch:&#123;&#125;/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, args.num_epochs))</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">            inputs, labels = data</span><br><span class="line">            inputs = inputs.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line">            preds = model(inputs)</span><br><span class="line">            loss = criterion(preds, labels)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            epoch_losses.update(loss.item(), <span class="built_in">len</span>(inputs)) <span class="comment">#更新当前的平均损失，并在进度条上显示</span></span><br><span class="line">            t.set_postfix(loss=<span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch_losses.avg))</span><br><span class="line">            t.update(<span class="built_in">len</span>(inputs))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#验证模式</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    epoch_psnr = AverageMeter() <span class="comment">#初始化一个AverageMeter对象来跟踪当前epoch的psnr平均值</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> eval_dataloader:</span><br><span class="line">        inputs, labels = data</span><br><span class="line">        inputs = inputs.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            preds = model(inputs).clamp(<span class="number">0.0</span>, <span class="number">1.0</span>) <span class="comment">#对预测值进行裁剪</span></span><br><span class="line">        epoch_psnr.update(calc_psnr(preds, labels), <span class="built_in">len</span>(inputs)) <span class="comment">#计算PSNR值并更新当前epoch的PSNR平均值。calc_psnr函数用于计算预测值和真实值之间的PSNR。</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;eval psnr: &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch_psnr.avg))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#保存最优epoch的权重文件</span></span><br><span class="line">    <span class="keyword">if</span> epoch_psnr.avg &gt; best_psnr:</span><br><span class="line">        best_epoch = epoch</span><br><span class="line">        best_psnr = epoch_psnr.avg</span><br><span class="line">        best_weights = copy.deepcopy(model.state_dict())</span><br><span class="line">        </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;best epoch: &#123;&#125;, psnr: &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(best_epoch, best_psnr))</span><br><span class="line">torch.save(best_weights, os.path.join(args.outputs_dir, <span class="string">&#x27;best.pth&#x27;</span>))</span><br></pre></td></tr></table></figure><h2 id="五-模型测试"><a href="#五-模型测试" class="headerlink" title="五.模型测试"></a>五.模型测试</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#运行程序</span></span><br><span class="line">python test.py --weights-file=outputs/x3/best.pth --image-file=data/car.bmp</span><br></pre></td></tr></table></figure><h3 id="1-命令行参数设定"><a href="#1-命令行参数设定" class="headerlink" title="1.命令行参数设定"></a>1.命令行参数设定</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--weights-file&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, required=<span class="literal">True</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--image-file&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, required=<span class="literal">True</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--scale&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">3</span>) </span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure><h3 id="2-加载预训练权重"><a href="#2-加载预训练权重" class="headerlink" title="2.加载预训练权重"></a>2.加载预训练权重</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cudnn.benchmark = <span class="literal">True</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">model = SRCNN().to(device)</span><br><span class="line"></span><br><span class="line">state_dict = model.state_dict() <span class="comment">#加载模型的参数状态  </span></span><br><span class="line"><span class="comment">#加载预训练权重，并映射到GPU</span></span><br><span class="line"><span class="keyword">for</span> n, p <span class="keyword">in</span> torch.load(args.weights_file, map_location=<span class="keyword">lambda</span> storage, loc: storage).items():</span><br><span class="line">    <span class="keyword">if</span> n <span class="keyword">in</span> state_dict.keys():<span class="comment"># 如果预训练权重中的键在模型的参数状态中存在 </span></span><br><span class="line">        state_dict[n].copy_(p)<span class="comment"># 则用预训练权重替换模型参数</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> KeyError(n)</span><br></pre></td></tr></table></figure><h3 id="3-双三次插值（BICUBIC）调整图片尺寸"><a href="#3-双三次插值（BICUBIC）调整图片尺寸" class="headerlink" title="3.双三次插值（BICUBIC）调整图片尺寸"></a>3.双三次插值（BICUBIC）调整图片尺寸</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">image = pil_image.<span class="built_in">open</span>(args.image_file).convert(<span class="string">&#x27;RGB&#x27;</span>)<span class="comment"># 加载图像，并转换为RGB格式</span></span><br><span class="line"><span class="comment"># 根据args.scale调整图像尺寸，使其为scale的整数倍 </span></span><br><span class="line">image_width = (image.width // args.scale) * args.scale</span><br><span class="line">image_height = (image.height // args.scale) * args.scale</span><br><span class="line"><span class="comment"># 调整至scale的倍数</span></span><br><span class="line">image = image.resize((image_width, image_height), resample=pil_image.BICUBIC)</span><br><span class="line"><span class="comment"># 将原始图片进行下采样，缩小到原始尺寸的1/scale，得到低分辨率图片</span></span><br><span class="line">image = image.resize((image.width // args.scale, image.height // args.scale), resample=pil_image.BICUBIC)</span><br><span class="line"><span class="comment"># 将低分辨率图片进行上采样，放大到规定的尺寸</span></span><br><span class="line">image = image.resize((image.width * args.scale, image.height * args.scale), resample=pil_image.BICUBIC)</span><br><span class="line"><span class="comment"># 保存处理后的图像</span></span><br><span class="line">image.save(args.image_file.replace(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;_bicubic_x&#123;&#125;.&#x27;</span>.<span class="built_in">format</span>(args.scale)))</span><br></pre></td></tr></table></figure><h3 id="4-调整色彩空间并进行超分辨率重建"><a href="#4-调整色彩空间并进行超分辨率重建" class="headerlink" title="4.调整色彩空间并进行超分辨率重建"></a>4.调整色彩空间并进行超分辨率重建</h3><ul><li><strong>将调整后的图像从RGB色彩空间转换到YCbCr色彩空间，并仅对Y分量进行超分辨率重建（SRCNN等模型通常只处理亮度分量）</strong>。</li><li><strong>YCbCr是一种色彩空间，其中Y代表亮度分量（Luminance），Cb和Cr代表蓝色和红色的色度分量（Chrominance）。YCbCr色彩空间是YUV色彩空间的一种变种，广泛应用于视频压缩和图像处理中。</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">image = np.array(image).astype(np.float32)<span class="comment"># 将图像转换为numpy数组，并转换为float32类型 </span></span><br><span class="line">ycbcr = convert_rgb_to_ycbcr(image)<span class="comment"># 将RGB图像转换为YCbCr色彩空间</span></span><br><span class="line"></span><br><span class="line">y = ycbcr[..., <span class="number">0</span>] <span class="comment">#提取Y分量</span></span><br><span class="line">y /= <span class="number">255.</span> <span class="comment">#归一化Y分量到[0, 1] </span></span><br><span class="line">y = torch.from_numpy(y).to(device) <span class="comment">#将numpy数组转换为torch张量，并移动到指定设备（如GPU）</span></span><br><span class="line">y = y.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>) <span class="comment">#增加一个批次维度和一个通道维度</span></span><br></pre></td></tr></table></figure><h3 id="5-使用模型进行重建"><a href="#5-使用模型进行重建" class="headerlink" title="5.使用模型进行重建"></a>5.使用模型进行重建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():<span class="comment"># 关闭梯度计算，进行前向传播</span></span><br><span class="line">    preds = model(y).clamp(<span class="number">0.0</span>, <span class="number">1.0</span>) <span class="comment">#使用模型进行预测，并限制输出值在[0, 1]之间</span></span><br><span class="line"></span><br><span class="line">psnr = calc_psnr(y, preds)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;PSNR: &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(psnr))</span><br><span class="line"></span><br><span class="line">preds = preds.mul(<span class="number">255.0</span>).cpu().numpy().squeeze(<span class="number">0</span>).squeeze(<span class="number">0</span>)<span class="comment">#将预测结果转换回uint8类型，并去除批次和通道维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将预测的Y分量与原始的Cb、Cr分量合并，然后转换回RGB色彩空间  </span></span><br><span class="line">output = np.array([preds, ycbcr[..., <span class="number">1</span>], ycbcr[..., <span class="number">2</span>]]).transpose([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line">output = np.clip(convert_ycbcr_to_rgb(output), <span class="number">0.0</span>, <span class="number">255.0</span>).astype(np.uint8)</span><br><span class="line"></span><br><span class="line">output = pil_image.fromarray(output) <span class="comment">#将numpy数组转换回PIL图像 </span></span><br><span class="line">output.save(args.image_file.replace(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;_srcnn_x&#123;&#125;.&#x27;</span>.<span class="built_in">format</span>(args.scale))) <span class="comment">#保存最终的图像 </span></span><br></pre></td></tr></table></figure><h3 id="6-结果"><a href="#6-结果" class="headerlink" title="6.结果"></a>6.结果</h3><p><img src="https://s21.ax1x.com/2024/09/13/pAn4tSJ.png" alt="pAn4tSJ.png"></p><h2 id="六-以单幅低分辨率图像实现超分辨率"><a href="#六-以单幅低分辨率图像实现超分辨率" class="headerlink" title="六.以单幅低分辨率图像实现超分辨率"></a>六.以单幅低分辨率图像实现超分辨率</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将单幅低分辨率图像以原尺寸规模进行超分辨率处理</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.backends.cudnn <span class="keyword">as</span> cudnn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> pil_image</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> SRCNN</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> convert_rgb_to_ycbcr, convert_ycbcr_to_rgb, calc_psnr</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--weights-file&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, required=<span class="literal">True</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--image-file&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, required=<span class="literal">True</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--scale&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">3</span>) </span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    cudnn.benchmark = <span class="literal">True</span></span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    model = SRCNN().to(device)</span><br><span class="line"></span><br><span class="line">    state_dict = model.state_dict()</span><br><span class="line">    <span class="keyword">for</span> n, p <span class="keyword">in</span> torch.load(args.weights_file, map_location=<span class="keyword">lambda</span> storage, loc: storage).items():</span><br><span class="line">        <span class="keyword">if</span> n <span class="keyword">in</span> state_dict.keys():</span><br><span class="line">            state_dict[n].copy_(p)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(n)</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    image = pil_image.<span class="built_in">open</span>(args.image_file).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">    image_width = (image.width // args.scale) * args.scale</span><br><span class="line">    image_height = (image.height // args.scale) * args.scale</span><br><span class="line">    image = image.resize((image_width, image_height), resample=pil_image.BICUBIC) </span><br><span class="line">    <span class="comment">#不进行下采样得到低分辨率图像的操作，从而也不需要上采样恢复尺寸</span></span><br><span class="line">    <span class="comment"># image = image.resize((image.width // args.scale, image.height // args.scale), resample=pil_image.BICUBIC)</span></span><br><span class="line">    <span class="comment"># image = image.resize((image.width * args.scale, image.height * args.scale), resample=pil_image.BICUBIC)</span></span><br><span class="line">    image.save(args.image_file.replace(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;_bicubic_x&#123;&#125;.&#x27;</span>.<span class="built_in">format</span>(args.scale)))</span><br><span class="line"></span><br><span class="line">    image = np.array(image).astype(np.float32)</span><br><span class="line">    ycbcr = convert_rgb_to_ycbcr(image)</span><br><span class="line"></span><br><span class="line">    y = ycbcr[..., <span class="number">0</span>]</span><br><span class="line">    y /= <span class="number">255.</span></span><br><span class="line">    y = torch.from_numpy(y).to(device)</span><br><span class="line">    y = y.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        preds = model(y).clamp(<span class="number">0.0</span>, <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># psnr = calc_psnr(y, preds)</span></span><br><span class="line">    <span class="comment"># print(&#x27;PSNR: &#123;:.2f&#125;&#x27;.format(psnr))</span></span><br><span class="line"></span><br><span class="line">    preds = preds.mul(<span class="number">255.0</span>).cpu().numpy().squeeze(<span class="number">0</span>).squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    output = np.array([preds, ycbcr[..., <span class="number">1</span>], ycbcr[..., <span class="number">2</span>]]).transpose([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line">    output = np.clip(convert_ycbcr_to_rgb(output), <span class="number">0.0</span>, <span class="number">255.0</span>).astype(np.uint8)</span><br><span class="line">    output = pil_image.fromarray(output)</span><br><span class="line">    output.save(args.image_file.replace(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;_srcnn_x&#123;&#125;.&#x27;</span>.<span class="built_in">format</span>(args.scale)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#python make.py --weights-file=outputs/x3/best.pth --image-file=data/test.bmp</span></span><br></pre></td></tr></table></figure><ul><li>个人评价：该模型实现效果很差。若不提供原始高分辨率图像，几乎不能将低分辨率图像变为高分辨率图像。</li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;超分辨率第一章-SRCNN&quot;&gt;&lt;a href=&quot;#超分辨率第一章-SRCNN&quot; class=&quot;headerlink&quot; title=&quot;超分辨率第一章-SRCNN&quot;&gt;&lt;/a&gt;超分辨率第一章-SRCNN&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;第一个超分辨率模型-SRCNN （SISR），2014年提出&lt;/p&gt;
&lt;p&gt;参考网址：&lt;a href=&quot;https://blog.csdn.net/zhanjuex/article/details/124344864?spm=1001.2014.3001.5506&quot;&gt;【超分辨率】【深度学习】SRCNN pytorch代码（附详细注释和数据集）_srcnn代码-CSDN博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_52261094/article/details/128389448&quot;&gt;SRCNN超分辨率Pytorch实现，代码逐行讲解，附源码&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;模型位置：F:&#92;Github下载&#92;SRCNN_Pytorch_1.0-master&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="硕士阶段学习笔记(入门阶段)" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/"/>
    
    <category term="模型" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="超分辨率" scheme="http://example.com/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>超分辨率-综述</title>
    <link href="http://example.com/2024/09/06/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87-%E7%BB%BC%E8%BF%B0/"/>
    <id>http://example.com/2024/09/06/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87-%E7%BB%BC%E8%BF%B0/</id>
    <published>2024-09-06T08:55:56.000Z</published>
    <updated>2024-09-09T12:50:05.829Z</updated>
    
    <content type="html"><![CDATA[<h1 id="超分辨率-综述"><a href="#超分辨率-综述" class="headerlink" title="超分辨率-综述"></a>超分辨率-综述</h1><blockquote><p>参考文献：<a href="http://fcst.ceaj.org/CN/10.3778/j.issn.1673-9418.2202063">深度学习的图像超分辨率重建技术综述</a></p></blockquote><span id="more"></span><h2 id="关于分辨率（resolution）"><a href="#关于分辨率（resolution）" class="headerlink" title="关于分辨率（resolution）"></a>关于分辨率（resolution）</h2><h3 id="1-分辨率的概念"><a href="#1-分辨率的概念" class="headerlink" title="1.分辨率的概念"></a>1.分辨率的概念</h3><ul><li>分辨率描述了图像或视频中像素的数量和密度，以及显示设备能够呈现的细节水平</li><li>具体可以细分为<a href="https://baike.baidu.com/item/显示分辨率/3431933?fromModule=lemma_inlink">显示分辨率</a>、<a href="https://baike.baidu.com/item/图像分辨率/872374?fromModule=lemma_inlink">图像分辨率</a><ul><li>显示分辨率<ul><li>在显示器领域，分辨率通常指屏幕水平和垂直方向有多少像素点。分辨率大小使用乘积表示，如1920×1080。当屏幕分辨率高时，屏幕上显示的像素多，单个像素尺寸比较小。2k:1920<em>1080；2.5k:2560\</em>1440；4k:3840 x 2160</li></ul></li><li>图像分辨率<ul><li>图像分辨率指的是图像中存储的信息量，通常为单位英寸中包含的像素点数。随着图像技术的发展，理论上图像分辨率会越来越大。</li><li>Pixels Per Inch(PPI)也叫像素密度单位，所表示的是每英寸所拥有的像素数量。因此PPI数值越高，即代表显示屏能够以越高的密度显示图像。当然，显示的密度越高，拟真度就越高</li></ul></li></ul></li></ul><h3 id="2-疑问解答"><a href="#2-疑问解答" class="headerlink" title="2.疑问解答"></a>2.疑问解答</h3><ul><li><strong>问题一：在基于深度学习的超分辨率重建图像方法中，分为两个模块，上采样和非线性映射学习模块；上采样是将图片进行放大，这个过程提升了图像的分辨率吗？（按我个人的理解是上采样是将图片的尺寸进行放大，但是并没有将先前模糊的图片变清晰，而非线性学习映射才是将低分辨率图片变为高分辨率图片的过程）</strong></li></ul><blockquote><p>答：在基于深度学习的超分辨率重建图像方法中，确实包含上采样和非线性映射学习两个关键模块。针对您提出的问题，我们可以从以下几个方面进行详细分析：</p><p><strong>1.上采样模块的作用</strong></p><p><strong>上采样</strong>模块的主要作用是将图像的尺寸进行放大，这个过程确实提升了图像的分辨率。分辨率是指图像中像素点的数量，通过上采样，图像的像素点数量增加，从而在物理尺寸上实现了图像的放大。然而，<strong>仅仅通过上采样并不能直接改善图像的质量，特别是当原始图像是低分辨率或模糊的时。这是因为上采样本身并不增加图像中的新信息，只是通过插值等方法在现有像素之间生成新的像素值。</strong></p><p><strong>2.非线性映射学习模块的作用</strong></p><p><strong>非线性映射学习</strong>模块则是超分辨率重建中的核心部分。它负责学习低分辨率（LR）图像到高分辨率（HR）图像之间的映射关系。在训练过程中，该模块通过大量的LR-HR图像对进行学习，使得模型能够掌握从LR图像中提取有效特征并重建出HR图像的能力。这个过程中，<strong>模型会学习到如何填补LR图像中缺失的细节信息，从而生成更加清晰、高分辨率的图像。</strong></p><p><strong>3.两者结合的作用</strong></p><p>在超分辨率重建中，上采样模块和非线性映射学习模块是相辅相成的。上采样模块为非线性映射学习模块提供了必要的图像尺寸放大功能，使得模型能够在更大的图像尺寸上进行特征提取和重建。而非线性映射学习模块则通过其强大的学习能力，弥补了上采样过程中可能产生的模糊和失真问题，最终生成出高质量的高分辨率图像。</p><p><strong>4.对问题的回答</strong></p><ul><li><strong>上采样是否提升了图像的分辨率？</strong> 是的，上采样通过增加图像的像素点数量，实现了图像尺寸的放大和分辨率的提升。</li><li><strong>上采样是否将先前模糊的图片变清晰？</strong> 单独来看，上采样并不能直接使模糊的图片变清晰。它只是改变了图像的物理尺寸和分辨率，而没有增加图像中的新信息或改善图像的质量。要使模糊的图片变清晰，需要依赖非线性映射学习模块等后续处理步骤。</li><li><strong>非线性学习映射是否将低分辨率图片变为高分辨率图片？</strong> 是的，非线性学习映射模块通过其强大的学习能力，能够从低分辨率图像中提取有效特征并重建出高分辨率图像。这个过程中，模型会填补低分辨率图像中缺失的细节信息，从而生成出更加清晰、高分辨率的图像。</li></ul><p>综上所述，在基于深度学习的超分辨率重建图像方法中，上采样模块和非线性映射学习模块各自承担着重要的角色，并通过协同工作实现了从低分辨率图像到高分辨率图像的转换</p></blockquote><ul><li><strong>问题二：上采样提升了分辨率，不是将图片的分辨率提高了吗，为什么图片还是模糊的呢，不是说分辨率越高的图片越清晰吗？</strong></li></ul><blockquote><p>您提出的问题非常深入，涉及到了图像处理中分辨率、清晰度以及上采样过程之间的复杂关系。首先，我们来澄清几个概念，并解释为什么上采样虽然提升了图像的分辨率，但并不一定能使图片变得清晰。</p><p><strong>1.分辨率与清晰度的关系</strong></p><p>分辨率确实是衡量图像质量的一个重要指标，它指的是图像中像素的数量。在数字图像中，分辨率越高，意味着图像包含的像素点越多，从而在物理尺寸上能够展现更多的细节。然而，<strong>清晰度（sharpness）则更多地与图像的细节表现力、边缘的锐利程度以及图像的噪声水平等因素有关。</strong></p><p><strong>2.上采样的作用与限制</strong></p><p>上采样是一个将图像尺寸放大的过程，它通过插值算法在原有像素之间生成新的像素值，从而增加图像的像素数量，提升分辨率。但是，这个过程并不会增加图像中的实际信息内容。换句话说，上采样无法创造出原本不存在的图像细节。</p><p><strong>3.为什么上采样后的图片可能仍然模糊？</strong></p><ol><li><strong>信息缺失</strong>：如果原始图像本身就是低分辨率或模糊的，那么它包含的细节信息就有限。上采样虽然能在物理尺寸上放大图像，但无法填补这些缺失的细节。</li><li><strong>插值算法的影响</strong>：上采样过程中使用的插值算法会影响新生成像素的质量。一些简单的插值算法（如最近邻插值）可能会产生明显的锯齿效应或模糊效果。而更复杂的算法（如双线性插值、双三次插值等）虽然能在一定程度上改善这个问题，但仍然无法完全恢复原始图像中不存在的细节。</li><li><strong>噪声放大</strong>：在上采样过程中，图像中的噪声也可能会被放大。如果原始图像就包含较多的噪声，那么上采样后的图像可能会显得更加模糊或颗粒感更强。</li></ol><p><strong>结论</strong></p><p>因此，虽然上采样能够提升图像的分辨率，但并不一定能使图片变得清晰。<strong>要获得清晰的图像，还需要从源头上保证图像的质量，并在后续处理过程中采用合适的算法和技术来尽可能地保留和恢复图像的细节信息。</strong>在某些情况下，可能需要结合超分辨率重建等高级技术来进一步提升图像的质量。</p></blockquote><h2 id="一-单图像超分辨率重建方法-SISR"><a href="#一-单图像超分辨率重建方法-SISR" class="headerlink" title="一.单图像超分辨率重建方法(SISR)"></a>一.单图像超分辨率重建方法(SISR)</h2><h3 id="1-发展情况"><a href="#1-发展情况" class="headerlink" title="1.发展情况"></a>1.发展情况</h3><ul><li>SISR 方法输入一张低分辨率图像，利用深度神经网络学习 LR-HR 图像对之间的映射关系，最终将 LR 图像重建为一张高分辨率图像。</li><li>2014提出<strong>SRCNN</strong>模型：提出<strong>峰值信噪比</strong>（peak signal-tonoise ratio，<strong>PSNR</strong>）和<strong>结构相似度</strong>（structure similarity index measure，<strong>SSIM</strong>）指标。这个阶段的 SISR 方法的改进方向主要以增加神经网络的深度，从而提高PSNR和SSIM指标为导向。</li><li>2017年提出<strong>SRGNN</strong>模型：首次提出要提高图像的感官质量，引入了<strong>感知损失函数</strong>。随后提出的模型开始以<strong>优化重建图像纹理细节</strong>为目标。</li><li>发展历程<ul><li><img src="https://s21.ax1x.com/2024/09/06/pAZqM01.png" alt="pAZqM01.png"></li></ul></li></ul><h3 id="2-超分框架（按照上采样模块位置区分）"><a href="#2-超分框架（按照上采样模块位置区分）" class="headerlink" title="2.超分框架（按照上采样模块位置区分）"></a>2.超分框架（按照上采样模块位置区分）</h3><ul><li><strong>非线性映射学习模块</strong><ul><li><strong>负责完成低分辨率图像到高分辨率图像的映射，这个过程中利用损失函数来进行引导和监督学习的进程</strong></li></ul></li><li><strong>上采样模块</strong><ul><li><strong>实现图片的放大</strong></li></ul></li></ul><h4 id="①前端上采样超分框架"><a href="#①前端上采样超分框架" class="headerlink" title="①前端上采样超分框架"></a>①前端上采样超分框架</h4><ul><li><p><strong>先将图片放大，再进行低分辨率到高分辨率的映射。</strong></p></li><li><p>前端上采样可以<strong>避免在低维空间上进行低维到高维的映射学习</strong>，降低了学习难度，是一 种简单易行的方法。</p></li><li><strong>但是同时噪声和模糊等也被增强，并且在高维空间进行卷积运算将会增加模型计算量，消耗更多的计算资源</strong></li><li><img src="https://s21.ax1x.com/2024/09/06/pAZqBAP.md.png" alt="pAZqBAP.md.png"></li></ul><h4 id="②后端上采样超分框架"><a href="#②后端上采样超分框架" class="headerlink" title="②后端上采样超分框架"></a>②后端上采样超分框架</h4><ul><li><p><strong>先进行低分辨率到高分辨率的映射，再将图片放大。</strong></p></li><li><p>该框架下的大部分卷积计算在低维空间进行，最后再利用端到端可学习的上采样层，<strong>如转置卷积和亚像素卷积</strong> ，进行上采样放大。</p></li><li>进一步释放了卷积的计算能力， 降低模型复杂度。</li><li><img src="https://s21.ax1x.com/2024/09/06/pAZq5NV.md.png" alt="pAZq5NV.md.png"></li></ul><h4 id="③渐进式上采样超分框架"><a href="#③渐进式上采样超分框架" class="headerlink" title="③渐进式上采样超分框架"></a>③渐进式上采样超分框架</h4><ul><li>先进行低分辨率到高分辨的映射，之后逐级进行图片放大，中途生成的图像继续输入后续模块（低分辨率到高分辨率的映射），直到达到目标分辨率。</li><li><p>常用方法是采用卷积级联或者 Laplace 金字塔的方式，再结合多级监督等学习策略</p></li><li><p><img src="https://s21.ax1x.com/2024/09/06/pAZqXH1.png" alt="pAZqXH1.png"></p></li></ul><h4 id="④升降采样迭代式超分框架"><a href="#④升降采样迭代式超分框架" class="headerlink" title="④升降采样迭代式超分框架"></a>④升降采样迭代式超分框架</h4><ul><li>交替使用上、下采样，结合得到的所有特征图来完成低分辨率图像的重建</li><li><img src="https://s21.ax1x.com/2024/09/06/pAZqz4K.md.png" alt="pAZqz4K.md.png"></li></ul><h3 id="3-上采样方法"><a href="#3-上采样方法" class="headerlink" title="3.上采样方法"></a>3.上采样方法</h3><h4 id="①基于插值法的上采样方法"><a href="#①基于插值法的上采样方法" class="headerlink" title="①基于插值法的上采样方法"></a>①基于插值法的上采样方法</h4><ul><li>利用一定的数学策略，从相关点中计算出待扩展点的像素值</li><li><strong>但是由于插值函数本身的连续性，导致了重建图像较为<u>平滑而模糊。</u>但图像纹理处常常是<u>各种突变</u>，这与插值函数的连续性互为矛盾</strong></li></ul><h4 id="②端到端可学习的上采样方法"><a href="#②端到端可学习的上采样方法" class="headerlink" title="②端到端可学习的上采样方法"></a>②端到端可学习的上采样方法</h4><h5 id="1-转置卷积"><a href="#1-转置卷积" class="headerlink" title="(1)转置卷积"></a>(1)转置卷积</h5><ul><li><p>设卷积核大小为k*k，输入为方形矩阵（将转置卷积过程转变为普通卷积过程）</p><ol><li>对输入进行四边补零，单边补零的数量为k-1</li><li>将卷积核旋转180°，在新的输入上进行直接卷积</li></ol></li><li><p>图示：</p><ul><li><img src="https://s21.ax1x.com/2024/09/06/pAZvnds.png" alt="pAZvnds.png"></li></ul></li></ul><h5 id="2-亚像素卷积"><a href="#2-亚像素卷积" class="headerlink" title="(2)亚像素卷积"></a>(2)亚像素卷积</h5><ul><li>亚像素的概念：在相机成像的过程中，获得的图像数据是将图像进行了离散化的处理，由于感光元件本身的能力限制，到成像面上每个像素只代表附近的颜色。例如两个感官原件上的像素之间有4.5um的间距，宏观上它们是连在一起的，微观上它们之间还有无数微小的东西存在，这些存在于两个实际物理像素之间的像素，就被称为“亚像素”。</li><li>利用卷积计算对图像进行特征提取，再对不同通道间的特征图进行重组，从而得到更高分辨率的特征图</li><li>图示：<img src="https://s21.ax1x.com/2024/09/06/pAZx5NR.png" alt="pAZx5NR.png"></li></ul><h3 id="4-非线性映射学习模块"><a href="#4-非线性映射学习模块" class="headerlink" title="4.非线性映射学习模块"></a>4.非线性映射学习模块</h3><ul><li>非线性映射学习模块在训练过程中利用 “<strong>LR-HR 图像对</strong>“进行学习，使模型获得从低分辨率图像到高分辨率图像的映射能力</li><li>一共分为四种模型<ul><li><img src="https://s21.ax1x.com/2024/09/06/pAZzRRP.png" alt="pAZzRRP.png"></li></ul></li></ul><h4 id="①基于CNN的超分模型"><a href="#①基于CNN的超分模型" class="headerlink" title="①基于CNN的超分模型"></a>①基于CNN的超分模型</h4><h5 id="1-SRCNN（2014年Dong等人提出，前端上采样框架-）"><a href="#1-SRCNN（2014年Dong等人提出，前端上采样框架-）" class="headerlink" title="(1)SRCNN（2014年Dong等人提出，前端上采样框架 ）"></a>(1)SRCNN（2014年Dong等人提出，前端上采样框架 ）</h5><ul><li>先将图片下采样预处理得到低分辨率图像</li><li>再利用双三次插值法将图片放大到目标分辨率（基于插值的上采样方法）</li><li>再用卷积核大小分别为 9×9、1×1、5×5的三个卷积层，分别进行特征提取，拟合 LR-HR 图像对之间的非线性映射以及将网络模型的输出结果进行重建，得到最后的高分辨率图像</li><li>图示：<img src="https://s21.ax1x.com/2024/09/06/pAZzAbQ.png" alt="pAZzAbQ.png"></li></ul><h5 id="2-FSRCNN-2016年由Dong改进，后端上采样框架"><a href="#2-FSRCNN-2016年由Dong改进，后端上采样框架" class="headerlink" title="(2)FSRCNN (2016年由Dong改进，后端上采样框架)"></a>(2)FSRCNN (2016年由Dong改进，后端上采样框架)</h5><ul><li>改进点：<ul><li>直接用LR图像作为输入，降低特征维度</li><li>使用比SRCNN 更小的滤波器，网络结构加深</li><li>采用后端上采样超分框架，在网络最后加入反卷积层来将图像放大至目标分辨率。</li><li>FSRCNN采用更小的卷积核、更深的网络层数，训练速度提高，重建的HR图像质量效果进一步得到提高</li></ul></li><li>图示：<img src="https://s21.ax1x.com/2024/09/06/pAZz6IA.png" alt="pAZz6IA.png"></li></ul><h4 id="②基于GNN的超分模型"><a href="#②基于GNN的超分模型" class="headerlink" title="②基于GNN的超分模型"></a>②基于GNN的超分模型</h4><ul><li>解决问题：基于CNN的超分辨率模型，尽管重建出来的高分辨率图像的 PSNR/SSIM 指标越来越高，但是生成的图像过于平滑，高频纹理信息丢失，重建图像缺乏人眼感官上的照片真实感， 并且在工业界的实际使用效果依然很差</li></ul><h5 id="1-SRGNN-2017Ledig等人提出"><a href="#1-SRGNN-2017Ledig等人提出" class="headerlink" title="(1) SRGNN (2017Ledig等人提出)"></a>(1) SRGNN (2017Ledig等人提出)</h5><ul><li>最早开始将超分研究的注意力从 PSNR/SSIM 指标上转移到图像感知质量上。</li><li>利用 VGG 网络提取出来的特征计算损失函数作为内容损失，内容损失加上对抗网络本身的对抗损失，共同构成了感知损失函数</li></ul><h4 id="③基于深度强化学习的超分模型"><a href="#③基于深度强化学习的超分模型" class="headerlink" title="③基于深度强化学习的超分模型"></a>③基于深度强化学习的超分模型</h4><ul><li>强化学习概念：<ul><li>强化学习在现有数据的基础上，循环利用学习得到的新的数据，不断提高模型的学习能力。 该方法已经被证明在不监督每一步的情况下对序列模型进行全局优化的有效性</li></ul></li></ul><h4 id="④基于Transformer的超分模型"><a href="#④基于Transformer的超分模型" class="headerlink" title="④基于Transformer的超分模型"></a>④基于Transformer的超分模型</h4><ul><li>2020 年 Yang 等人最早将 Transformer 引入图像超分领域，提出了基于 Transformer 网络结构的 TTSR 超分算法。</li><li>为了充分利用参考图像的纹理信息，Yang 等人在TTSR中提出了特征融合机制，利用上采样方式实现不同层级间的特征互相融合。</li></ul><h3 id="5-损失函数"><a href="#5-损失函数" class="headerlink" title="5.损失函数"></a>5.损失函数</h3><ul><li>损失函数在非线性映射学习模块的学习过程中，指导着超分模型向着预期的方向学习和前进，通过损失函数的变化可以知道当前模型的训练与预期之间的差距，同时调控模型学习方向。</li></ul><h4 id="①像素损失函数"><a href="#①像素损失函数" class="headerlink" title="①像素损失函数"></a>①像素损失函数</h4><ul><li>表示重建图像与目标图像之间的像素损失</li></ul><h5 id="1-MSE损失（均方误差）函数"><a href="#1-MSE损失（均方误差）函数" class="headerlink" title="(1)MSE损失（均方误差）函数"></a>(1)MSE损失（均方误差）函数</h5><ul><li><script type="math/tex; mode=display">M S E = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } ( y _ { i } - y _ { i } ^ { * } ) ^ { 2 }</script></li><li>其中，n 表示像素点个数，yi 表示预测值，<script type="math/tex">y _ { i } ^ { * }</script>表示目标值。</li><li>缺点（基于平方项的影响）<ul><li>MSE 损失函数（L2 loss）中，在误差已经很小的情况下，MSE损失函数仍然会促使模型继续优化这些误差，即使这样做带来的收益可能微乎其微。</li><li>当误差大于1时，MSE会将误差进一步放大，因此它对数据中的异常值（即那些与大多数数据点显著不同的点）非常敏感。模型可能会为了适应这些异常值而做出较大的调整，这可能导致模型在整体数据上的泛化能力下降。这就使得最终重建图像更为平滑、模糊，缺乏高频的纹理细节。</li><li>在图像处理或重建任务中，如果使用MSE作为损失函数，模型可能会为了最小化MSE而倾向于生成平滑、模糊的图像。这是因为高频纹理细节（如边缘、锐角等）在像素级别上通常与周围像素有较大的差异，这些差异在MSE损失函数下会被视为大误差。为了减小这些误差，模型可能会倾向于将这些高频细节平滑化，从而导致重建图像缺乏细节和锐度。</li></ul></li></ul><h5 id="2-L1损失函数（平均绝对误差）"><a href="#2-L1损失函数（平均绝对误差）" class="headerlink" title="(2)L1损失函数（平均绝对误差）"></a>(2)L1损失函数（平均绝对误差）</h5><ul><li><script type="math/tex; mode=display">L _ { 1 } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } | y _ { i } - y _ { i } ^ { * } |</script></li><li>在实践中，L1 损失函数的实际效果要比 MSE更好，更能提高模型性能，得到更高的指标</li></ul><h4 id="②内容损失"><a href="#②内容损失" class="headerlink" title="②内容损失"></a>②内容损失</h4><ul><li><p>相对像素损失来说，内容损失不再要求像素层面上的精确，而是追求人眼感官层面的相似。为了提升感知质量，利用神经网络中生成的图像特征与真实图像特征之间的距离来进行计算</p></li><li><script type="math/tex; mode=display">L _ { e o n t e n t } = \frac { 1 } { n _ { l } } \sqrt { \sum _ { i , j } ( \phi _ { i , j } ^ { ( 0 ) } ( I ) - \phi _ { i , j } ^ { ( 0 ) } ( \widehat { I } ) ) ^ { 2 } }</script></li><li><script type="math/tex">n _ { l }</script> 表示第 L 层特征图对应的像素点个数，<script type="math/tex">\phi _ { i , j } ^ { ( l ) } ( I )</script>和<script type="math/tex">\phi _ { i , j } ^ { ( l ) } ( \widehat { I } )</script>分别表示重建图像和原始高分辨率图像在第 I 层中第 i 个最大池化层之前经过第 j 次卷积得到的特征图。</li></ul><h4 id="③对抗损失"><a href="#③对抗损失" class="headerlink" title="③对抗损失"></a>③对抗损失</h4><ul><li><script type="math/tex; mode=display">L _ { G } ( I _ { S R } ) = - D ( I _ { S R } )</script></li><li><script type="math/tex; mode=display">L _ { D } ( I _ { S R } , I _ { H R } ) = D ( I _ { S R } ) - D ( I _ { H R } ) + \lambda ( | V _ { I _ { S R } } D ( I _ { S R } ) _ { 2 } - 1 | | ) ^ { 2 }</script></li></ul><h4 id="④感知损失"><a href="#④感知损失" class="headerlink" title="④感知损失"></a>④感知损失</h4><ul><li>内容损失和对抗损失的加权和<ul><li><script type="math/tex; mode=display">L ^ { S R } = L _ { c o n t e n t } + 1 0 ^ { - 3 } L _ { G } ( I _ { S R } )</script></li></ul></li></ul><h2 id="二-基于参考的图像超分辨率重建（略读）"><a href="#二-基于参考的图像超分辨率重建（略读）" class="headerlink" title="二.基于参考的图像超分辨率重建（略读）"></a>二.基于参考的图像超分辨率重建（略读）</h2><ul><li>定义<ul><li><strong>RefSR 方法借助引入的参考图像，将相似度最高的参考图像中的信息转移到低分辨率图像中并进行两者的信息融合，从而重建出纹理细节更清晰的高分辨率图像。</strong></li></ul></li><li>步骤：<ul><li>第一步将参考图像中有用的信息与输入图像中的信息进行匹配，能准确对应两者的信息是重建令人满意的细节纹理的关键。</li><li>第二步将匹配到的信息进行提取，并与输入图像进行融合，进而重建出满意的图像。</li></ul></li><li>决定性因素就是 LR 图像与高分辨率参考图像之间的匹配和融合的准确性</li><li><img src="https://s21.ax1x.com/2024/09/08/pAeRGgx.png" alt="pAeRGgx.png"></li></ul><h3 id="1-像素对齐"><a href="#1-像素对齐" class="headerlink" title="1.像素对齐"></a>1.像素对齐</h3><ul><li>先从 LR 图像中检测稀疏的特征，再在参考图像中进行特征匹配， 最后基于这些匹配特征将原LR图像映射到另一个图像中，从而实现图像对齐</li><li><p><strong>Landmark</strong> 通过全局配准来将参考图像与上采样后的LR图像进行对齐，从而识别出这些图像中各自对应的区域， 减少失配或错配的情况</p><ul><li><img src="https://s21.ax1x.com/2024/09/08/pAeRF4s.png" alt="pAeRF4s.png"></li></ul></li><li><p>2018年的<strong>CrossNet</strong>模型是一种端到端的完全卷积的深度神经网络，通过预测光流量来进行跨尺度变换</p><ul><li><img src="https://s21.ax1x.com/2024/09/08/pAeR2qS.png" alt="pAeR2qS.png"></li></ul></li><li>2018年Zhao等人提出了<strong>高频补偿超分辨率（highfrequency compensation super-resolution，HCSR）模型</strong>，需要计算从参考图像到所有LR光场图像的多个视图差，然后利用混合策略对精化的视差图进行融合，最后得到高质量的超分图像</li><li>2020年Shim 等人在堆叠的可变性卷积的基础上提出了可实现<strong>端到端的新颖高效的参考图像特征提取模块——相似性搜索与提取网络（similairity search and extraction network，SSEN</strong>），可以从参考图像中提取对齐的相关特征，并且可以插入到任何现有的超分辨率网络中。</li></ul><h3 id="2-Patch匹配"><a href="#2-Patch匹配" class="headerlink" title="2.Patch匹配"></a>2.Patch匹配</h3><ul><li>2017 年 Zheng 等人利用 Patch 匹配的方法，提出了 <strong>SS-Net 模型</strong>。具体来说，SS-Net 首先设计了一个跨尺度对应网络来表示参考图像和低分辨率图像之间的跨尺度 Patch匹配。在多个尺度上对低分辨率图像的Patch与参考图像的Patch进行融合，最终合成HR图像并输出。</li><li><p>2019 年 Zheng 等人提出了<strong>端到端可学习的 SRNTT（superresolution by neural texture transfer）网络模型</strong>，SRNTT预先训练的 VGG 中提取的参考特征与 LR 特征在自然空间中进行多级匹配，促进了多尺度神经传输。</p><ul><li><img src="https://s21.ax1x.com/2024/09/08/pAeRzGR.png" alt="pAeRzGR.png"></li></ul></li><li><p>2020年Yang等人[16] 进一步将Transformer架构引入RefSR任务，提出了<strong>TTSR 模型</strong></p><ul><li><img src="https://s21.ax1x.com/2024/09/08/pAeWnzt.png" alt="pAeWnzt.png"></li></ul></li><li><p>2021年Zhou等人从解决实际多尺度相机系统中的 RefSR问题出发，受到多平面图像（multiplane image，MPI）表示的启发，提出了一个 端到端可学习的 RefSR 网络模型——Cross-MPI 模型</p></li><li>2021年Lu等人[56] 提出了 MASA （matching acceleration and spatial adaptation）模型</li><li>2021年Jiang 等人[57] 提出了 C2 -Matching 模型</li></ul><h2 id="三-超分数据集和图像质量评估"><a href="#三-超分数据集和图像质量评估" class="headerlink" title="三.超分数据集和图像质量评估"></a>三.超分数据集和图像质量评估</h2><h3 id="1-常用数据集"><a href="#1-常用数据集" class="headerlink" title="1.常用数据集"></a>1.常用数据集</h3><ul><li><img src="https://s21.ax1x.com/2024/09/08/pAeWNzq.png" alt="pAeWNzq.png"></li></ul><h3 id="2-图像质量评估"><a href="#2-图像质量评估" class="headerlink" title="2.图像质量评估"></a>2.图像质量评估</h3><h4 id="①峰值信噪比（PSNR）"><a href="#①峰值信噪比（PSNR）" class="headerlink" title="①峰值信噪比（PSNR）"></a>①峰值信噪比（PSNR）</h4><ul><li><script type="math/tex; mode=display">P S N R = 1 0 \lg \frac { M A X _ { 1 } ^ { 2 } } { M S E }</script></li><li><p>其中，MSE 为均方误差，MAX指表示图像点颜色的最大数值，图像的最大像素值由二进制位数决定，如8位二进制表示的图像的最大像素值就是 255。</p></li></ul><h4 id="②结构相似度（SSIM）"><a href="#②结构相似度（SSIM）" class="headerlink" title="②结构相似度（SSIM）"></a>②结构相似度（SSIM）</h4><ul><li>SSIM 从人类视觉系统中 获得灵感，将图像的组成分为亮度、对比度以及结构三部分，并用均值作为亮度的估计，标准差作为对比度估计，协方差作为结构相似程度估计</li><li><script type="math/tex; mode=display">S S I M ( x , y ) = \frac { ( 2 \mu _ { x } \mu _ { y } + c _ { 1 } ) ( \sigma _ { x y } + c _ { 2 } ) } { ( \mu _ { x } ^ { 2 } + \mu _ { y } ^ { 2 } + c _ { 1 } ) ( \sigma _ { x } ^ { 2 } + \sigma _ { y } ^ { 2 } + c _ { 2 } ) }</script></li></ul><h4 id="③平均意见评分（mean-opinion-score，MOS）"><a href="#③平均意见评分（mean-opinion-score，MOS）" class="headerlink" title="③平均意见评分（mean opinion score，MOS）"></a>③平均意见评分（mean opinion score，MOS）</h4><ul><li>一种常用的主观图像质量评估的方法，通过邀请接受过训练的普通人以及未接受过训练的普通人来对重建的图像进行评分，并且两者人数大致均衡，通过给重建图像打分，再对最后的得分进行平均</li></ul><h2 id="四-模型分析"><a href="#四-模型分析" class="headerlink" title="四.模型分析"></a>四.模型分析</h2><h3 id="1-SOTA模型统计"><a href="#1-SOTA模型统计" class="headerlink" title="1. SOTA模型统计"></a>1. SOTA模型统计</h3><h4 id="①SISR模型统计"><a href="#①SISR模型统计" class="headerlink" title="①SISR模型统计"></a>①SISR模型统计</h4><p><img src="https://s21.ax1x.com/2024/09/08/pAeW6Y9.png" alt="pAeW6Y9.png"></p><h4 id="②RefSR模型统计"><a href="#②RefSR模型统计" class="headerlink" title="②RefSR模型统计"></a>②RefSR模型统计</h4><p><img src="https://s21.ax1x.com/2024/09/08/pAeWcWR.png" alt="pAeWcWR.png"></p><h3 id="2-两种方式对比"><a href="#2-两种方式对比" class="headerlink" title="2.两种方式对比"></a>2.两种方式对比</h3><p><img src="https://s21.ax1x.com/2024/09/08/pAeWRQx.png" alt="pAeWRQx.png"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;超分辨率-综述&quot;&gt;&lt;a href=&quot;#超分辨率-综述&quot; class=&quot;headerlink&quot; title=&quot;超分辨率-综述&quot;&gt;&lt;/a&gt;超分辨率-综述&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;参考文献：&lt;a href=&quot;http://fcst.ceaj.org/CN/10.3778/j.issn.1673-9418.2202063&quot;&gt;深度学习的图像超分辨率重建技术综述&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="学术论文总结" scheme="http://example.com/categories/%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="超分辨率" scheme="http://example.com/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>深度学习第五章-ResNet网络</title>
    <link href="http://example.com/2024/09/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%94%E7%AB%A0-ResNet%E7%BD%91%E7%BB%9C/"/>
    <id>http://example.com/2024/09/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%94%E7%AB%A0-ResNet%E7%BD%91%E7%BB%9C/</id>
    <published>2024-09-04T08:10:38.000Z</published>
    <updated>2024-09-19T09:00:41.477Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度学习第五章-ResNet网络"><a href="#深度学习第五章-ResNet网络" class="headerlink" title="深度学习第五章-ResNet网络"></a>深度学习第五章-ResNet网络</h1><blockquote><p>本模型位于：E:\python文件\deep-learning-for-image-processing-master\pytorch_classification\Test5_resnet</p></blockquote><span id="more"></span><h2 id="一-模型介绍"><a href="#一-模型介绍" class="headerlink" title="一.模型介绍"></a>一.模型介绍</h2><blockquote><p>特点：</p><ul><li>超深的网络结构(突破1000层)</li><li>提出residual模块</li><li>使用Batch Normalization加速训练(丢弃dropout)</li></ul></blockquote><h3 id="1-残差模块"><a href="#1-残差模块" class="headerlink" title="1.残差模块"></a>1.残差模块</h3><p>（左边是18层、34层的残差块结构；右边是50层、101层、152层的残差块结构）</p><ul><li><img src="https://s21.ax1x.com/2024/09/04/pAZFLVO.png" alt="pAZFLVO.png"></li></ul><h3 id="2-实线残差结构与虚线残差结构"><a href="#2-实线残差结构与虚线残差结构" class="headerlink" title="2.实线残差结构与虚线残差结构"></a>2.实线残差结构与虚线残差结构</h3><p>从conv3层开始需使用虚线所示的残差网络结构（初始层有下采样）</p><h3 id="3-BN处理方法"><a href="#3-BN处理方法" class="headerlink" title="3.BN处理方法"></a>3.BN处理方法</h3><ul><li><p>通常在卷积层或全连接层之后添加</p></li><li><p>Batch Normalization的目的是使我们的一批(Batch) 特征层满足均值为0，方差为1的分布规律</p></li></ul><h3 id="4-模型参数"><a href="#4-模型参数" class="headerlink" title="4.模型参数"></a>4.模型参数</h3><ul><li><img src="https://s21.ax1x.com/2024/09/04/pAZVWO1.png" alt="pAZVWO1.png"></li></ul><h2 id="二-数据集-花分类集"><a href="#二-数据集-花分类集" class="headerlink" title="二.数据集-花分类集"></a>二.数据集-花分类集</h2><ul><li>如同前两章</li></ul><h2 id="三-模型搭建"><a href="#三-模型搭建" class="headerlink" title="三.模型搭建"></a>三.模型搭建</h2><h3 id="1-定义18层、24层网络残差块结构"><a href="#1-定义18层、24层网络残差块结构" class="headerlink" title="1.定义18层、24层网络残差块结构"></a>1.定义18层、24层网络残差块结构</h3><ul><li><p>图示，左边结构：conv2_x层、右边结构：conv3_x层及之后</p><ul><li><img src="https://s21.ax1x.com/2024/09/05/pAZ0SEV.png" alt="pAZ0SEV.png"></li></ul></li><li><p>代码</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module): <span class="comment">#定义残差块（18层、34层网络使用）</span></span><br><span class="line">    expansion = <span class="number">1</span> <span class="comment">#表示残差块中前后维度的倍数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn1 = nn.BatchNorm2d(out_channel) <span class="comment">#bath normalizition层</span></span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn2 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        <span class="variable language_">self</span>.downsample = downsample <span class="comment">#定义下采样方法</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x <span class="comment">#输入数据作为原数据记录</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment">#判断是否为虚线分支结构，若是则将原输入进行下采样</span></span><br><span class="line">            identity = <span class="variable language_">self</span>.downsample(x) </span><br><span class="line"></span><br><span class="line">        out = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        out = <span class="variable language_">self</span>.bn1(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.relu(out)</span><br><span class="line"></span><br><span class="line">        out = <span class="variable language_">self</span>.conv2(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.bn2(out)</span><br><span class="line"></span><br><span class="line">        out += identity <span class="comment">#最终输出由主分支加原输入组成</span></span><br><span class="line">        out = <span class="variable language_">self</span>.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h3 id="2-定义50层、101层、152层残差块网络结构"><a href="#2-定义50层、101层、152层残差块网络结构" class="headerlink" title="2.定义50层、101层、152层残差块网络结构"></a>2.定义50层、101层、152层残差块网络结构</h3><ul><li>图示：左边结构：conv2_x层、右边结构：conv3_x层及之后<ul><li><img src="https://s21.ax1x.com/2024/09/04/pAZEI2j.png" alt="pAZEI2j.png" style="zoom:67%;"></li></ul></li><li>代码</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module): <span class="comment">#定义残差块 （50层、101层、152层网络使用）</span></span><br><span class="line">    expansion = <span class="number">4</span> <span class="comment">#表示残差块中前后维度的倍数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,</span><br><span class="line">                               kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)  <span class="comment"># squeeze channels</span></span><br><span class="line">        <span class="variable language_">self</span>.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        <span class="comment"># -----------------------------------------</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, stride=stride, bias=<span class="literal">False</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn2 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        <span class="comment"># -----------------------------------------</span></span><br><span class="line">        <span class="variable language_">self</span>.conv3 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel*<span class="variable language_">self</span>.expansion,</span><br><span class="line">                               kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)  <span class="comment"># unsqueeze channels</span></span><br><span class="line">        <span class="variable language_">self</span>.bn3 = nn.BatchNorm2d(out_channel*<span class="variable language_">self</span>.expansion)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.downsample = downsample</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment">#判断是否为虚线分支结构,若是则将原输入进行下采样</span></span><br><span class="line">            identity = <span class="variable language_">self</span>.downsample(x)</span><br><span class="line"></span><br><span class="line">        out = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        out = <span class="variable language_">self</span>.bn1(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.relu(out)</span><br><span class="line"></span><br><span class="line">        out = <span class="variable language_">self</span>.conv2(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.bn2(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.relu(out)</span><br><span class="line"></span><br><span class="line">        out = <span class="variable language_">self</span>.conv3(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.bn3(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = <span class="variable language_">self</span>.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h3 id="3-构建网络模型"><a href="#3-构建网络模型" class="headerlink" title="3.构建网络模型"></a>3.构建网络模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module): <span class="comment">#定义网络模型结构</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,block,blocks_num,num_classes=<span class="number">1000</span>,include_top=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ResNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.include_top = include_top</span><br><span class="line">        <span class="variable language_">self</span>.in_channel = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="variable language_">self</span>.in_channel,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding=<span class="number">3</span>,bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn1 = nn.BatchNorm2d(<span class="variable language_">self</span>.in_channel)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.layer1 = <span class="variable language_">self</span>._make_layer(block, <span class="number">64</span>, blocks_num[<span class="number">0</span>]) <span class="comment">#blocks_num为重复的残差块数量，一般这一层还不需要在初始位置进行下采样。</span></span><br><span class="line">        <span class="variable language_">self</span>.layer2 = <span class="variable language_">self</span>._make_layer(block, <span class="number">128</span>, blocks_num[<span class="number">1</span>], stride=<span class="number">2</span>)<span class="comment">#这一层开始需要进行下采样</span></span><br><span class="line">        <span class="variable language_">self</span>.layer3 = <span class="variable language_">self</span>._make_layer(block, <span class="number">256</span>, blocks_num[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.layer4 = <span class="variable language_">self</span>._make_layer(block, <span class="number">512</span>, blocks_num[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.include_top:</span><br><span class="line">            <span class="variable language_">self</span>.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))  <span class="comment">#添加了一个自适应平均池化层（Adaptive Average Pooling Layer），其输出尺寸为(1, 1)，高和宽变为1.</span></span><br><span class="line">            <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">512</span> * block.expansion, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="variable language_">self</span>.modules(): <span class="comment">#对卷积层进行初始化操作</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, channel, block_num, stride=<span class="number">1</span></span>): <span class="comment">#生成各层残差块</span></span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> <span class="variable language_">self</span>.in_channel != channel * block.expansion: <span class="comment">#判断残差块初始输入时是否需要进行下采样</span></span><br><span class="line">       <span class="comment">#如果需要下采样，downsample 被初始化为一个顺序模型（nn.Sequential），其中包含一个1x1的卷积层（用于调整通道数和（或）进行下采样），以及一个批量归一化层（nn.BatchNorm2d）。这个顺序模型用于调整输入数据，使其能够与残差块的输出进行相加（残差网络的关键操作之一）。</span></span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(<span class="variable language_">self</span>.in_channel, channel * block.expansion, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(channel * block.expansion))</span><br><span class="line"></span><br><span class="line">        layers = [] <span class="comment">#方法创建一个空列表 layers，用于存储要构建的残差块</span></span><br><span class="line">        <span class="comment">#首先，将第一个残差块添加到 layers 列表中。这个残差块可能带有下采样层（如果 downsample 不为 None），且其输入通道数为 self.in_channel，输出通道数为 channel。</span></span><br><span class="line">        layers.append(block(<span class="variable language_">self</span>.in_channel,channel,downsample=downsample,stride=stride))</span><br><span class="line">        <span class="variable language_">self</span>.in_channel = channel * block.expansion <span class="comment">#更新通道数的变化</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block_num): <span class="comment">#将剩余残差块的层进行添加</span></span><br><span class="line">            layers.append(block(<span class="variable language_">self</span>.in_channel,channel))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.bn1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.layer1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.layer2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.layer3(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.layer4(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.include_top:</span><br><span class="line">            x = <span class="variable language_">self</span>.avgpool(x)</span><br><span class="line">            x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">            x = <span class="variable language_">self</span>.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="4-定义各版本的Resnet网络"><a href="#4-定义各版本的Resnet网络" class="headerlink" title="4.定义各版本的Resnet网络"></a>4.定义各版本的Resnet网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resnet34</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>): <span class="comment">#本次训练选择的模型</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(BasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], num_classes=num_classes, include_top=include_top)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], num_classes=num_classes, include_top=include_top)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet101</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], num_classes=num_classes, include_top=include_top)</span><br></pre></td></tr></table></figure><h2 id="四-模型训练"><a href="#四-模型训练" class="headerlink" title="四.模型训练"></a>四.模型训练</h2><h3 id="1-使用迁移训练的方法，下载预训练权重"><a href="#1-使用迁移训练的方法，下载预训练权重" class="headerlink" title="1.使用迁移训练的方法，下载预训练权重"></a>1.使用迁移训练的方法，下载预训练权重</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = resnet34()</span><br><span class="line"><span class="comment"># download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth</span></span><br><span class="line">model_weight_path = <span class="string">&quot;./resnet34-pre.pth&quot;</span> <span class="comment">#预训练的权重文件</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(model_weight_path), <span class="string">&quot;file &#123;&#125; does not exist.&quot;</span>.<span class="built_in">format</span>(model_weight_path)</span><br><span class="line">net.load_state_dict(torch.load(model_weight_path, map_location=<span class="string">&#x27;cpu&#x27;</span>)) <span class="comment">#加载权重并将权重加载到模型中</span></span><br><span class="line"></span><br><span class="line">net.to(device)</span><br><span class="line">in_channel = net.fc.in_features <span class="comment">#获取原始全连接层的输入特征维度（即最后一个卷积层输出的特征图数量）</span></span><br><span class="line">net.fc = nn.Linear(in_channel, <span class="number">5</span>).to(device) <span class="comment">#然后，创建一个新的全连接层nn.Linear(in_channel, 5)，其输入维度与原始全连接层相同，但输出维度为5，以匹配新的分类任务。最后，将这个新的全连接层赋值给net.fc，从而完成了模型的修改。</span></span><br></pre></td></tr></table></figure><h3 id="2-训练模型"><a href="#2-训练模型" class="headerlink" title="2.训练模型"></a>2.训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#通过列表推导式，从模型的参数中筛选出需要梯度的参数（即那些被训练的参数）。这通常排除了那些不参与训练的参数，如批量归一化层（BatchNorm）中的运行均值和方差。</span></span><br><span class="line">params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">optimizer = optim.Adam(params, lr=<span class="number">0.0001</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">3</span></span><br><span class="line">best_acc = <span class="number">0.0</span></span><br><span class="line">save_path = <span class="string">&#x27;resnet34-pre-again.pth&#x27;</span></span><br><span class="line">train_steps = <span class="built_in">len</span>(train_loader)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># train</span></span><br><span class="line">    net.train()</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    train_bar = tqdm(train_loader, file=sys.stdout) <span class="comment">#创建进度条</span></span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_bar):</span><br><span class="line">        images, labels = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        logits = net(images.to(device))</span><br><span class="line">        loss = loss_function(logits, labels.to(device))</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">        train_bar.desc = <span class="string">&quot;train epoch[&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                                                 epochs,</span><br><span class="line">                                                                 loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># validate</span></span><br><span class="line">    net.<span class="built_in">eval</span>() <span class="comment">#验证模式下的BN层将不起作用</span></span><br><span class="line">    acc = <span class="number">0.0</span>  <span class="comment"># accumulate accurate number / epoch</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        val_bar = tqdm(validate_loader, file=sys.stdout)</span><br><span class="line">        <span class="keyword">for</span> val_data <span class="keyword">in</span> val_bar:</span><br><span class="line">            val_images, val_labels = val_data</span><br><span class="line">            outputs = net(val_images.to(device))</span><br><span class="line">            <span class="comment"># loss = loss_function(outputs, test_labels)</span></span><br><span class="line">            predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            acc += torch.eq(predict_y, val_labels.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">            val_bar.desc = <span class="string">&quot;valid epoch[&#123;&#125;/&#123;&#125;]&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                                       epochs)</span><br><span class="line"></span><br><span class="line">    val_accurate = acc / val_num</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;[epoch %d] train_loss: %.3f  val_accuracy: %.3f&#x27;</span> %</span><br><span class="line">          (epoch + <span class="number">1</span>, running_loss / train_steps, val_accurate))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> val_accurate &gt; best_acc:</span><br><span class="line">        best_acc = val_accurate</span><br><span class="line">        torch.save(net.state_dict(), save_path)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="五-测试结果"><a href="#五-测试结果" class="headerlink" title="五.测试结果"></a>五.测试结果</h2><ul><li>如同前一章结果</li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;深度学习第五章-ResNet网络&quot;&gt;&lt;a href=&quot;#深度学习第五章-ResNet网络&quot; class=&quot;headerlink&quot; title=&quot;深度学习第五章-ResNet网络&quot;&gt;&lt;/a&gt;深度学习第五章-ResNet网络&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;本模型位于：E:&#92;python文件&#92;deep-learning-for-image-processing-master&#92;pytorch_classification&#92;Test5_resnet&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="硕士阶段学习笔记(入门阶段)" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/"/>
    
    <category term="模型" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="深度学习基础" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>深度学习第四章-GoogleNet</title>
    <link href="http://example.com/2024/09/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%9B%9B%E7%AB%A0-GoogleNet/"/>
    <id>http://example.com/2024/09/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%9B%9B%E7%AB%A0-GoogleNet/</id>
    <published>2024-09-02T09:32:32.000Z</published>
    <updated>2024-09-13T08:09:59.597Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度学习第四章-GoogleNet"><a href="#深度学习第四章-GoogleNet" class="headerlink" title="深度学习第四章-GoogleNet"></a>深度学习第四章-GoogleNet</h1><blockquote><p>本模型位于：E:\python文件\deep-learning-for-image-processing-master\pytorch_classification\Test4_googlenet</p></blockquote><span id="more"></span><h2 id="一-模型介绍"><a href="#一-模型介绍" class="headerlink" title="一.模型介绍"></a>一.模型介绍</h2><ul><li><p>引入了Inception结构（融合不同尺度的特征信息）</p><ul><li><img src="https://s21.ax1x.com/2024/09/02/pAV3x41.png" alt="pAV3x41.png"></li></ul></li><li><p>使用1x1的卷积核进行降维以及映射处理 （减少模型参数）</p><ul><li><img src="https://s21.ax1x.com/2024/09/02/pAV8Z4I.png" alt="pAV8Z4I.png"></li></ul></li><li><p>添加两个辅助分类器帮助训练</p></li><li>丢弃全连接层，使用平均池化层（大大减少模型参数）</li></ul><p>图示：</p><p><img src="https://s21.ax1x.com/2024/09/03/pAV0PJg.jpg" alt="pAV0PJg.jpg"></p><h2 id="二-数据集-花分类数据集"><a href="#二-数据集-花分类数据集" class="headerlink" title="二.数据集-花分类数据集"></a>二.数据集-花分类数据集</h2><ul><li>如同前一章的内容</li></ul><h2 id="三-网络模型搭建"><a href="#三-网络模型搭建" class="headerlink" title="三.网络模型搭建"></a>三.网络模型搭建</h2><h3 id="1-普通卷积层模版"><a href="#1-普通卷积层模版" class="headerlink" title="1.普通卷积层模版"></a>1.普通卷积层模版</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module): <span class="comment">#基本的卷积层模版</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicConv2d, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv = nn.Conv2d(in_channels, out_channels, **kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.conv(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="2-inception模块模版"><a href="#2-inception模块模版" class="headerlink" title="2.inception模块模版"></a>2.inception模块模版</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module): <span class="comment">#定义inception模版</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=<span class="number">1</span>) <span class="comment">#分支一</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.branch2 = nn.Sequential( <span class="comment">#分支二</span></span><br><span class="line">            BasicConv2d(in_channels, ch3x3red, kernel_size=<span class="number">1</span>),</span><br><span class="line">            BasicConv2d(ch3x3red, ch3x3, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)   <span class="comment"># 保证输出大小等于输入大小</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.branch3 = nn.Sequential( <span class="comment">#分支三</span></span><br><span class="line">            BasicConv2d(in_channels, ch5x5red, kernel_size=<span class="number">1</span>),</span><br><span class="line">            BasicConv2d(ch5x5red, ch5x5, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)   <span class="comment"># 保证输出大小等于输入大小</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.branch4 = nn.Sequential( <span class="comment">#分支四</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            BasicConv2d(in_channels, pool_proj, kernel_size=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment">#定义前向传播函数</span></span><br><span class="line">        branch1 = <span class="variable language_">self</span>.branch1(x)</span><br><span class="line">        branch2 = <span class="variable language_">self</span>.branch2(x)</span><br><span class="line">        branch3 = <span class="variable language_">self</span>.branch3(x)</span><br><span class="line">        branch4 = <span class="variable language_">self</span>.branch4(x)</span><br><span class="line"></span><br><span class="line">        outputs = [branch1, branch2, branch3, branch4]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>) <span class="comment">#进行合并</span></span><br></pre></td></tr></table></figure><h3 id="3-辅助分类器模版"><a href="#3-辅助分类器模版" class="headerlink" title="3.辅助分类器模版"></a>3.辅助分类器模版</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionAux</span>(nn.Module): <span class="comment">#定义辅助分类器模版</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionAux, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.averagePool = nn.AvgPool2d(kernel_size=<span class="number">5</span>, stride=<span class="number">3</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv = BasicConv2d(in_channels, <span class="number">128</span>, kernel_size=<span class="number">1</span>)  <span class="comment"># output[batch, 128, 4, 4]</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">1024</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14</span></span><br><span class="line">        x = <span class="variable language_">self</span>.averagePool(x)</span><br><span class="line">        <span class="comment"># aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4</span></span><br><span class="line">        x = <span class="variable language_">self</span>.conv(x)</span><br><span class="line">        <span class="comment"># N x 128 x 4 x 4</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = F.dropout(x, <span class="number">0.5</span>, training=<span class="variable language_">self</span>.training)</span><br><span class="line">        <span class="comment"># N x 2048</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x), inplace=<span class="literal">True</span>)</span><br><span class="line">        x = F.dropout(x, <span class="number">0.5</span>, training=<span class="variable language_">self</span>.training)</span><br><span class="line">        <span class="comment"># N x 1024</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="comment"># N x num_classes</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="4-GooleNet网络"><a href="#4-GooleNet网络" class="headerlink" title="4.GooleNet网络"></a>4.GooleNet网络</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GoogLeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, aux_logits=<span class="literal">True</span>, init_weights=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GoogLeNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.aux_logits = aux_logits</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = BasicConv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>)</span><br><span class="line">        <span class="variable language_">self</span>.maxpool1 = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>) <span class="comment">#向上取整</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = BasicConv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv3 = BasicConv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.maxpool2 = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.inception3a = Inception(<span class="number">192</span>, <span class="number">64</span>, <span class="number">96</span>, <span class="number">128</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">        <span class="variable language_">self</span>.inception3b = Inception(<span class="number">256</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">192</span>, <span class="number">32</span>, <span class="number">96</span>, <span class="number">64</span>)</span><br><span class="line">        <span class="variable language_">self</span>.maxpool3 = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.inception4a = Inception(<span class="number">480</span>, <span class="number">192</span>, <span class="number">96</span>, <span class="number">208</span>, <span class="number">16</span>, <span class="number">48</span>, <span class="number">64</span>)</span><br><span class="line">        <span class="variable language_">self</span>.inception4b = Inception(<span class="number">512</span>, <span class="number">160</span>, <span class="number">112</span>, <span class="number">224</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        <span class="variable language_">self</span>.inception4c = Inception(<span class="number">512</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        <span class="variable language_">self</span>.inception4d = Inception(<span class="number">512</span>, <span class="number">112</span>, <span class="number">144</span>, <span class="number">288</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        <span class="variable language_">self</span>.inception4e = Inception(<span class="number">528</span>, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">        <span class="variable language_">self</span>.maxpool4 = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.inception5a = Inception(<span class="number">832</span>, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">        <span class="variable language_">self</span>.inception5b = Inception(<span class="number">832</span>, <span class="number">384</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">48</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.aux_logits:</span><br><span class="line">            <span class="variable language_">self</span>.aux1 = InceptionAux(<span class="number">512</span>, num_classes)</span><br><span class="line">            <span class="variable language_">self</span>.aux2 = InceptionAux(<span class="number">528</span>, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)) <span class="comment">#自适应池化层</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(<span class="number">0.4</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">1024</span>, num_classes)</span><br><span class="line">        <span class="keyword">if</span> init_weights:</span><br><span class="line">            <span class="variable language_">self</span>._initialize_weights()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># N x 3 x 224 x 224</span></span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        <span class="comment"># N x 64 x 112 x 112</span></span><br><span class="line">        x = <span class="variable language_">self</span>.maxpool1(x)</span><br><span class="line">        <span class="comment"># N x 64 x 56 x 56</span></span><br><span class="line">        x = <span class="variable language_">self</span>.conv2(x)</span><br><span class="line">        <span class="comment"># N x 64 x 56 x 56</span></span><br><span class="line">        x = <span class="variable language_">self</span>.conv3(x)</span><br><span class="line">        <span class="comment"># N x 192 x 56 x 56</span></span><br><span class="line">        x = <span class="variable language_">self</span>.maxpool2(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># N x 192 x 28 x 28</span></span><br><span class="line">        x = <span class="variable language_">self</span>.inception3a(x)</span><br><span class="line">        <span class="comment"># N x 256 x 28 x 28</span></span><br><span class="line">        x = <span class="variable language_">self</span>.inception3b(x)</span><br><span class="line">        <span class="comment"># N x 480 x 28 x 28</span></span><br><span class="line">        x = <span class="variable language_">self</span>.maxpool3(x)</span><br><span class="line">        <span class="comment"># N x 480 x 14 x 14</span></span><br><span class="line">        x = <span class="variable language_">self</span>.inception4a(x)</span><br><span class="line">        <span class="comment"># N x 512 x 14 x 14</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.training <span class="keyword">and</span> <span class="variable language_">self</span>.aux_logits:    <span class="comment"># 若处于训练模式则添加辅助分类器1</span></span><br><span class="line">            aux1 = <span class="variable language_">self</span>.aux1(x)</span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.inception4b(x)</span><br><span class="line">        <span class="comment"># N x 512 x 14 x 14</span></span><br><span class="line">        x = <span class="variable language_">self</span>.inception4c(x)</span><br><span class="line">        <span class="comment"># N x 512 x 14 x 14</span></span><br><span class="line">        x = <span class="variable language_">self</span>.inception4d(x)</span><br><span class="line">        <span class="comment"># N x 528 x 14 x 14</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.training <span class="keyword">and</span> <span class="variable language_">self</span>.aux_logits:    <span class="comment"># 若处于训练模式则添加辅助分类器2</span></span><br><span class="line">            aux2 = <span class="variable language_">self</span>.aux2(x)</span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.inception4e(x)</span><br><span class="line">        <span class="comment"># N x 832 x 14 x 14</span></span><br><span class="line">        x = <span class="variable language_">self</span>.maxpool4(x)</span><br><span class="line">        <span class="comment"># N x 832 x 7 x 7</span></span><br><span class="line">        x = <span class="variable language_">self</span>.inception5a(x)</span><br><span class="line">        <span class="comment"># N x 832 x 7 x 7</span></span><br><span class="line">        x = <span class="variable language_">self</span>.inception5b(x)</span><br><span class="line">        <span class="comment"># N x 1024 x 7 x 7</span></span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.avgpool(x)</span><br><span class="line">        <span class="comment"># N x 1024 x 1 x 1</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># N x 1024</span></span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc(x)</span><br><span class="line">        <span class="comment"># N x 1000 (num_classes)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.training <span class="keyword">and</span> <span class="variable language_">self</span>.aux_logits:   <span class="comment"># eval model lose this layer</span></span><br><span class="line">            <span class="keyword">return</span> x, aux2, aux1</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>): <span class="comment">#初始化参数函数</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="variable language_">self</span>.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="四-训练模型"><a href="#四-训练模型" class="headerlink" title="四.训练模型"></a>四.训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">net = GoogLeNet(num_classes=<span class="number">5</span>, aux_logits=<span class="literal">True</span>, init_weights=<span class="literal">True</span>)</span><br><span class="line">net.to(device)</span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.0003</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">30</span></span><br><span class="line">best_acc = <span class="number">0.0</span></span><br><span class="line">save_path = <span class="string">&#x27;./googleNet.pth&#x27;</span></span><br><span class="line">train_steps = <span class="built_in">len</span>(train_loader)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># train</span></span><br><span class="line">    net.train()</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    train_bar = tqdm(train_loader, file=sys.stdout)</span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_bar):</span><br><span class="line">        images, labels = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        logits, aux_logits2, aux_logits1 = net(images.to(device))</span><br><span class="line">        loss0 = loss_function(logits, labels.to(device))</span><br><span class="line">        loss1 = loss_function(aux_logits1, labels.to(device))</span><br><span class="line">        loss2 = loss_function(aux_logits2, labels.to(device))</span><br><span class="line">        loss = loss0 + loss1 * <span class="number">0.3</span> + loss2 * <span class="number">0.3</span> <span class="comment">#总损失结合相应带权重的辅助分类器的损失</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">        train_bar.desc = <span class="string">&quot;train epoch[&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                                                 epochs,</span><br><span class="line">                                                                 loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># validate</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    acc = <span class="number">0.0</span>  <span class="comment"># accumulate accurate number / epoch</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        val_bar = tqdm(validate_loader, file=sys.stdout)</span><br><span class="line">        <span class="keyword">for</span> val_data <span class="keyword">in</span> val_bar:</span><br><span class="line">            val_images, val_labels = val_data</span><br><span class="line">            outputs = net(val_images.to(device))  <span class="comment"># eval model only have last output layer</span></span><br><span class="line">            predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            acc += torch.eq(predict_y, val_labels.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    val_accurate = acc / val_num</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;[epoch %d] train_loss: %.3f  val_accuracy: %.3f&#x27;</span> %</span><br><span class="line">          (epoch + <span class="number">1</span>, running_loss / train_steps, val_accurate))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> val_accurate &gt; best_acc:</span><br><span class="line">        best_acc = val_accurate</span><br><span class="line">        torch.save(net.state_dict(), save_path)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;深度学习第四章-GoogleNet&quot;&gt;&lt;a href=&quot;#深度学习第四章-GoogleNet&quot; class=&quot;headerlink&quot; title=&quot;深度学习第四章-GoogleNet&quot;&gt;&lt;/a&gt;深度学习第四章-GoogleNet&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;本模型位于：E:&#92;python文件&#92;deep-learning-for-image-processing-master&#92;pytorch_classification&#92;Test4_googlenet&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="硕士阶段学习笔记(入门阶段)" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/"/>
    
    <category term="模型" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="深度学习基础" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>深度学习基础第三章-VGG网络</title>
    <link href="http://example.com/2024/09/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%89%E7%AB%A0-VGG%E7%BD%91%E7%BB%9C/"/>
    <id>http://example.com/2024/09/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%89%E7%AB%A0-VGG%E7%BD%91%E7%BB%9C/</id>
    <published>2024-09-02T00:59:46.000Z</published>
    <updated>2024-09-13T08:09:10.110Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度学习基础第三章-VGG网络"><a href="#深度学习基础第三章-VGG网络" class="headerlink" title="深度学习基础第三章-VGG网络"></a>深度学习基础第三章-VGG网络</h1><blockquote><p>本模型存放于目录：</p><p>E:\python文件\deep-learning-for-image-processing-master\pytorch_classification\Test3_vggnet</p></blockquote><span id="more"></span><h2 id="一-模型介绍"><a href="#一-模型介绍" class="headerlink" title="一.模型介绍"></a>一.模型介绍</h2><p>特点：</p><ul><li>通过堆叠多个3x3的卷积核来替代大尺度卷积核（减少所需参数）</li><li>论文中提到，可以通过堆叠两个3x3的卷积核替代5x5的卷积核，堆叠三个3x3的卷积核替代7x7的卷积核 （拥有相同的感受野）</li></ul><p><img src="https://s21.ax1x.com/2024/09/02/pAVk8Ag.jpg" alt="pAVk8Ag.jpg"></p><h2 id="二-数据集-花分类数据集"><a href="#二-数据集-花分类数据集" class="headerlink" title="二.数据集-花分类数据集"></a>二.数据集-花分类数据集</h2><h3 id="1-定义预处理函数"><a href="#1-定义预处理函数" class="headerlink" title="1.定义预处理函数"></a>1.定义预处理函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data_transform = &#123; <span class="comment">#对训练集与测试集图片进行预处理</span></span><br><span class="line">    <span class="string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(<span class="number">224</span>), <span class="comment">#裁剪图片尺寸</span></span><br><span class="line">                                 transforms.RandomHorizontalFlip(), <span class="comment">#对图片进行随机翻转</span></span><br><span class="line">                                 transforms.ToTensor(), <span class="comment">#转换为张量形式</span></span><br><span class="line">                                 transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))]), <span class="comment">#标准化数据</span></span><br><span class="line">    <span class="string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">                               transforms.ToTensor(),</span><br><span class="line">                               transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])&#125;</span><br></pre></td></tr></table></figure><h3 id="2-从磁盘中读取数据集"><a href="#2-从磁盘中读取数据集" class="headerlink" title="2.从磁盘中读取数据集"></a>2.从磁盘中读取数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data_root = os.path.abspath(os.path.join(os.getcwd(), <span class="string">&quot;../..&quot;</span>))  <span class="comment"># get data root path</span></span><br><span class="line">image_path = os.path.join(data_root, <span class="string">&quot;data_set&quot;</span>, <span class="string">&quot;flower_data&quot;</span>)  <span class="comment"># flower data set path</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(image_path), <span class="string">&quot;&#123;&#125; path does not exist.&quot;</span>.<span class="built_in">format</span>(image_path)</span><br><span class="line">train_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="string">&quot;train&quot;</span>),</span><br><span class="line">                                     transform=data_transform[<span class="string">&quot;train&quot;</span>])</span><br><span class="line">train_num = <span class="built_in">len</span>(train_dataset)</span><br></pre></td></tr></table></figure><h3 id="3-保存各类比的字典索引"><a href="#3-保存各类比的字典索引" class="headerlink" title="3.保存各类比的字典索引"></a>3.保存各类比的字典索引</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># &#123;&#x27;daisy&#x27;:0, &#x27;dandelion&#x27;:1, &#x27;roses&#x27;:2, &#x27;sunflower&#x27;:3, &#x27;tulips&#x27;:4&#125;</span></span><br><span class="line">flower_list = train_dataset.class_to_idx</span><br><span class="line">cla_dict = <span class="built_in">dict</span>((val, key) <span class="keyword">for</span> key, val <span class="keyword">in</span> flower_list.items())</span><br><span class="line"><span class="comment"># write dict into json file</span></span><br><span class="line">json_str = json.dumps(cla_dict, indent=<span class="number">4</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;class_indices.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    json_file.write(json_str)</span><br></pre></td></tr></table></figure><h3 id="4-加载训练集与测试集"><a href="#4-加载训练集与测试集" class="headerlink" title="4.加载训练集与测试集"></a>4.加载训练集与测试集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset,</span><br><span class="line">                                           batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                           num_workers=<span class="number">0</span></span><br><span class="line">validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="string">&quot;val&quot;</span>),</span><br><span class="line">                                        transform=data_transform[<span class="string">&quot;val&quot;</span>])</span><br><span class="line">val_num = <span class="built_in">len</span>(validate_dataset)</span><br><span class="line">validate_loader = torch.utils.data.DataLoader(validate_dataset,</span><br><span class="line">                                              batch_size=batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                                              num_workers=nw)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;using &#123;&#125; images for training, &#123;&#125; images for validation.&quot;</span>.<span class="built_in">format</span>(train_num,</span><br><span class="line">                                                                       val_num))</span><br></pre></td></tr></table></figure><h2 id="三-网络模型搭建"><a href="#三-网络模型搭建" class="headerlink" title="三.网络模型搭建"></a>三.网络模型搭建</h2><h3 id="1-根据版本提供相应的网络结构"><a href="#1-根据版本提供相应的网络结构" class="headerlink" title="1.根据版本提供相应的网络结构"></a>1.根据版本提供相应的网络结构</h3><ul><li>由于vgg网络有很多版本，因此通过字典保存相应不同的结构</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#字典文件保存各网络模型的配置文件 （特征提取部分）</span></span><br><span class="line">cfgs = &#123; </span><br><span class="line">    <span class="string">&#x27;vgg11&#x27;</span>: [<span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg13&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg16&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;vgg19&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_features</span>(<span class="params">cfg: <span class="built_in">list</span></span>): <span class="comment">#根据字典的列表得到相应的网络模型结构</span></span><br><span class="line">    layers = []</span><br><span class="line">    in_channels = <span class="number">3</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> cfg: <span class="comment">#遍历列表</span></span><br><span class="line">        <span class="keyword">if</span> v == <span class="string">&quot;M&quot;</span>: <span class="comment">#此时为最大池化层</span></span><br><span class="line">            layers += [nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment">#其他为卷积层与激活层</span></span><br><span class="line">            conv2d = nn.Conv2d(in_channels, v, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            layers += [conv2d, nn.ReLU(<span class="literal">True</span>)]</span><br><span class="line">            in_channels = v</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers) <span class="comment">#最终返回网络模型</span></span><br></pre></td></tr></table></figure><h3 id="2-定义网络模型"><a href="#2-定义网络模型" class="headerlink" title="2.定义网络模型"></a>2.定义网络模型</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VGG</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, num_classes=<span class="number">1000</span>, init_weights=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(VGG, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.features = features <span class="comment">#定义特征提取层</span></span><br><span class="line">        <span class="variable language_">self</span>.classifier = nn.Sequential(  <span class="comment">#定义全连接层</span></span><br><span class="line">            nn.Linear(<span class="number">512</span>*<span class="number">7</span>*<span class="number">7</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, num_classes)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> init_weights: <span class="comment">#判断是否初始化参数</span></span><br><span class="line">            <span class="variable language_">self</span>._initialize_weights()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment">#前向传播过程</span></span><br><span class="line">        <span class="comment"># N x 3 x 224 x 224</span></span><br><span class="line">        x = <span class="variable language_">self</span>.features(x) <span class="comment">#特征提取层</span></span><br><span class="line">        <span class="comment"># N x 512 x 7 x 7</span></span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>) <span class="comment">#展平</span></span><br><span class="line">        <span class="comment"># N x 512*7*7</span></span><br><span class="line">        x = <span class="variable language_">self</span>.classifier(x) <span class="comment">#全连接分类层</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>): <span class="comment">#初始化参数函数</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="variable language_">self</span>.modules(): <span class="comment">#遍历所有层</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.xavier_uniform_(m.weight) <span class="comment">#使用xavier方法对卷积层参数初始化</span></span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>) <span class="comment">#若采用偏置，将其初始化为0</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.xavier_uniform_(m.weight)</span><br><span class="line">                <span class="comment"># nn.init.normal_(m.weight, 0, 0.01)</span></span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="3-实例化网络模型（使用vgg16）"><a href="#3-实例化网络模型（使用vgg16）" class="headerlink" title="3.实例化网络模型（使用vgg16）"></a>3.实例化网络模型（使用vgg16）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">model_name=<span class="string">&quot;vgg16&quot;</span>, **kwargs</span>): <span class="comment">#实例化模型</span></span><br><span class="line">    <span class="keyword">assert</span> model_name <span class="keyword">in</span> cfgs, <span class="string">&quot;Warning: model number &#123;&#125; not in cfgs dict!&quot;</span>.<span class="built_in">format</span>(model_name)</span><br><span class="line">    cfg = cfgs[model_name]</span><br><span class="line">    model = VGG(make_features(cfg), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h2 id="四·训练模型"><a href="#四·训练模型" class="headerlink" title="四·训练模型"></a>四·训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">model_name = <span class="string">&quot;vgg16&quot;</span> <span class="comment">#使用vgg16版本</span></span><br><span class="line">net = vgg(model_name=model_name, num_classes=<span class="number">5</span>, init_weights=<span class="literal">True</span>)</span><br><span class="line">net.to(device)</span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.0001</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">30</span></span><br><span class="line">best_acc = <span class="number">0.0</span></span><br><span class="line">save_path = <span class="string">&#x27;./&#123;&#125;Net.pth&#x27;</span>.<span class="built_in">format</span>(model_name) <span class="comment">#设置保存参数文件的路径</span></span><br><span class="line">train_steps = <span class="built_in">len</span>(train_loader)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># train</span></span><br><span class="line">    net.train()  <span class="comment">#将网络设置为训练模式，此时dropout层将发挥作用</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    train_bar = tqdm(train_loader, file=sys.stdout) <span class="comment">#利用了 tqdm 库来在训练过程中添加一个进度条，使得用户可以直观地看到数据加载和训练的进度</span></span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_bar):</span><br><span class="line">        images, labels = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = net(images.to(device))</span><br><span class="line">        loss = loss_function(outputs, labels.to(device))</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="comment"># 更新 train_bar（即之前通过 tqdm 包装的 train_loader 迭代器）的描述（description）字段。这个描述字段通常用于在进度条旁边显示额外的信息，比如当前的训练轮次（epoch）、总轮次、以及某个指标（如损失值）的当前值。</span></span><br><span class="line">        train_bar.desc = <span class="string">&quot;train epoch[&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                                                 epochs,</span><br><span class="line">                                                                 loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># validate</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    acc = <span class="number">0.0</span>  <span class="comment"># accumulate accurate number / epoch</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        val_bar = tqdm(validate_loader, file=sys.stdout)</span><br><span class="line">        <span class="keyword">for</span> val_data <span class="keyword">in</span> val_bar:</span><br><span class="line">            val_images, val_labels = val_data</span><br><span class="line">            outputs = net(val_images.to(device))</span><br><span class="line">            predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            acc += torch.eq(predict_y, val_labels.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    val_accurate = acc / val_num</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;[epoch %d] train_loss: %.3f  val_accuracy: %.3f&#x27;</span> %</span><br><span class="line">          (epoch + <span class="number">1</span>, running_loss / train_steps, val_accurate))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> val_accurate &gt; best_acc:</span><br><span class="line">        best_acc = val_accurate</span><br><span class="line">        torch.save(net.state_dict(), save_path)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="五-测试模型效果"><a href="#五-测试模型效果" class="headerlink" title="五.测试模型效果"></a>五.测试模型效果</h2><ul><li>代码与AlnexNet部分一致</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">data_transform = transforms.Compose(</span><br><span class="line">    [transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">     transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># load image</span></span><br><span class="line">img_path = <span class="string">&quot;../tulip.jpg&quot;</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(img_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(img_path)</span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">plt.imshow(img)</span><br><span class="line"><span class="comment"># [N, C, H, W]</span></span><br><span class="line">img = data_transform(img)</span><br><span class="line"><span class="comment"># expand batch dimension</span></span><br><span class="line">img = torch.unsqueeze(img, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># read class_indict</span></span><br><span class="line">json_path = <span class="string">&#x27;./class_indices.json&#x27;</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(json_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(json_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(json_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    class_indict = json.load(f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create model</span></span><br><span class="line">model = vgg(model_name=<span class="string">&quot;vgg16&quot;</span>, num_classes=<span class="number">5</span>).to(device)</span><br><span class="line"><span class="comment"># load model weights</span></span><br><span class="line">weights_path = <span class="string">&quot;./vgg16Net.pth&quot;</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(weights_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(weights_path)</span><br><span class="line">model.load_state_dict(torch.load(weights_path, map_location=device))</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># predict class</span></span><br><span class="line">    output = torch.squeeze(model(img.to(device))).cpu()</span><br><span class="line">    predict = torch.softmax(output, dim=<span class="number">0</span>)</span><br><span class="line">    predict_cla = torch.argmax(predict).numpy()</span><br><span class="line"></span><br><span class="line">print_res = <span class="string">&quot;class: &#123;&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(predict_cla)],</span><br><span class="line">                                             predict[predict_cla].numpy())</span><br><span class="line">plt.title(print_res)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(predict)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;class: &#123;:10&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(i)],</span><br><span class="line">                                              predict[i].numpy()))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;深度学习基础第三章-VGG网络&quot;&gt;&lt;a href=&quot;#深度学习基础第三章-VGG网络&quot; class=&quot;headerlink&quot; title=&quot;深度学习基础第三章-VGG网络&quot;&gt;&lt;/a&gt;深度学习基础第三章-VGG网络&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;本模型存放于目录：&lt;/p&gt;
&lt;p&gt;E:&#92;python文件&#92;deep-learning-for-image-processing-master&#92;pytorch_classification&#92;Test3_vggnet&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="硕士阶段学习笔记(入门阶段)" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/"/>
    
    <category term="模型" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="深度学习基础" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>深度学习第二章-AlexNet</title>
    <link href="http://example.com/2024/08/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0-AlexNet/"/>
    <id>http://example.com/2024/08/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0-AlexNet/</id>
    <published>2024-08-30T03:34:33.000Z</published>
    <updated>2024-09-19T08:59:33.275Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度学习第二章-AlexNet搭建"><a href="#深度学习第二章-AlexNet搭建" class="headerlink" title="深度学习第二章-AlexNet搭建"></a>深度学习第二章-AlexNet搭建</h1><blockquote><p>本模型存放于目录：</p><p>E:\python文件\deep-learning-for-image-processing-master\tensorflow_classification\Test2_alexnet</p></blockquote><span id="more"></span><h2 id="一-模型介绍"><a href="#一-模型介绍" class="headerlink" title="一.模型介绍"></a>一.模型介绍</h2><ul><li>首次利用GPU进行网络加速训练。</li><li>使用了ReLU激活函数，而不是传统的Sigmoid激活函数以及Tanh激活函数。</li><li>使用了LRN局部响应归一化。</li><li>在全连接层的前两层中使用了Dropout随机失活神经元操作，以减少过拟合。<ul><li><img src="https://s21.ax1x.com/2024/08/30/pAAq9vn.png" alt="pAAq9vn.png"></li></ul></li></ul><h2 id="二-数据集-花分类数据集"><a href="#二-数据集-花分类数据集" class="headerlink" title="二.数据集-花分类数据集"></a>二.数据集-花分类数据集</h2><h3 id="1-定义预处理函数"><a href="#1-定义预处理函数" class="headerlink" title="1.定义预处理函数"></a>1.定义预处理函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data_transform = &#123; <span class="comment">#处理训练集与测试集的方法</span></span><br><span class="line">        <span class="string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(<span class="number">224</span>), <span class="comment">#随机裁剪</span></span><br><span class="line">                                     transforms.RandomHorizontalFlip(), <span class="comment">#随机翻转</span></span><br><span class="line">                                     transforms.ToTensor(),</span><br><span class="line">                                     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))]),</span><br><span class="line">        <span class="string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># cannot 224, must (224, 224)</span></span><br><span class="line">                                   transforms.ToTensor(),</span><br><span class="line">                                   transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-从磁盘中读取数据集"><a href="#2-从磁盘中读取数据集" class="headerlink" title="2.从磁盘中读取数据集"></a>2.从磁盘中读取数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data_root = os.path.abspath(os.path.join(os.getcwd(), <span class="string">&quot;../..&quot;</span>))  <span class="comment"># 计算数据集的根目录，从当前目录向上回溯两级目录</span></span><br><span class="line">image_path = os.path.join(data_root, <span class="string">&quot;data_set&quot;</span>, <span class="string">&quot;flower_data&quot;</span>)  <span class="comment"># 从根目录下找到数据集所在的目录</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(image_path), <span class="string">&quot;&#123;&#125; path does not exist.&quot;</span>.<span class="built_in">format</span>(image_path) <span class="comment">#检查图片路径是否存在，若不存在，则抛出一个异常</span></span><br><span class="line">train_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="string">&quot;train&quot;</span>),</span><br><span class="line">                                     transform=data_transform[<span class="string">&quot;train&quot;</span>])  <span class="comment">#从指定的路径加载，创建训练数据集的对象，并使用预处理方法</span></span><br><span class="line">train_num = <span class="built_in">len</span>(train_dataset) <span class="comment">#计算训练集中的样本总数</span></span><br><span class="line">validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="string">&quot;val&quot;</span>), </span><br><span class="line">                                        transform=data_transform[<span class="string">&quot;val&quot;</span>]) <span class="comment">#从指定的路径加载，创建测试数据集的对象，并使用预处理方法</span></span><br><span class="line">val_num = <span class="built_in">len</span>(validate_dataset) <span class="comment">#计算测试集中的样本总数</span></span><br></pre></td></tr></table></figure><h3 id="3-保存各类别的字典索引"><a href="#3-保存各类别的字典索引" class="headerlink" title="3.保存各类别的字典索引"></a>3.保存各类别的字典索引</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># &#123;&#x27;daisy&#x27;:0, &#x27;dandelion&#x27;:1, &#x27;roses&#x27;:2, &#x27;sunflower&#x27;:3, &#x27;tulips&#x27;:4&#125;</span></span><br><span class="line">flower_list = train_dataset.class_to_idx  <span class="comment">#保存一个字典文件,将每个类别的名称映射到该类别的索引</span></span><br><span class="line">cla_dict = <span class="built_in">dict</span>((val, key) <span class="keyword">for</span> key, val <span class="keyword">in</span> flower_list.items()) <span class="comment">#创建一个逆映射，将每个类别的索引映射到该类别的名称</span></span><br><span class="line">json_str = json.dumps(cla_dict, indent=<span class="number">4</span>) <span class="comment">#将cla_dict字典转换成JSON格式的字符串，通过indent=4参数设置缩进来提高可读性</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;class_indices.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> json_file: <span class="comment">#保存到指定的JSON文件中</span></span><br><span class="line">    json_file.write(json_str)</span><br></pre></td></tr></table></figure><h3 id="4-加载训练集与测试集"><a href="#4-加载训练集与测试集" class="headerlink" title="4.加载训练集与测试集"></a>4.加载训练集与测试集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, <span class="comment">#加载数据集</span></span><br><span class="line">                                           batch_size=batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                           num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">validate_loader = torch.utils.data.DataLoader(validate_dataset, <span class="comment">#加载测试集</span></span><br><span class="line">                                              batch_size=<span class="number">4</span>, shuffle=<span class="literal">False</span>,</span><br><span class="line">                                              num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;using &#123;&#125; images for training, &#123;&#125; images for validation.&quot;</span>.<span class="built_in">format</span>(train_num,</span><br><span class="line">                                                                       val_num))</span><br></pre></td></tr></table></figure><h2 id="三-网络模型搭建"><a href="#三-网络模型搭建" class="headerlink" title="三.网络模型搭建"></a>三.网络模型搭建</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, init_weights=<span class="literal">False</span></span>): <span class="comment">#设置类别个数，以及是否初始化权重</span></span><br><span class="line">        <span class="built_in">super</span>(AlexNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.features = nn.Sequential( <span class="comment">#将各层打包为一个模块, input[3, 224, 224]</span></span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">48</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),  <span class="comment"># output[48, 55, 55]</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),                  <span class="comment"># output[48, 27, 27]</span></span><br><span class="line">            nn.Conv2d(<span class="number">48</span>, <span class="number">128</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),           <span class="comment"># output[128, 27, 27]</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),                  <span class="comment"># output[128, 13, 13]</span></span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),          <span class="comment"># output[192, 13, 13]</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),          <span class="comment"># output[192, 13, 13]</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">192</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),          <span class="comment"># output[128, 13, 13]</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),                  <span class="comment"># output[128, 6, 6]</span></span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.classifier = nn.Sequential(</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>), <span class="comment">#50%概率使全连接层失活</span></span><br><span class="line">            nn.Linear(<span class="number">128</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">2048</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">2048</span>, <span class="number">2048</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">2048</span>, num_classes),</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> init_weights:</span><br><span class="line">            <span class="variable language_">self</span>._initialize_weights()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.features(x)</span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>) <span class="comment">#展平</span></span><br><span class="line">        x = <span class="variable language_">self</span>.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>): <span class="comment">#参数的初始化</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="variable language_">self</span>.modules(): <span class="comment">#遍历模型的所有模块，判断模块类型并进行权重初始化</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="四-训练模型"><a href="#四-训练模型" class="headerlink" title="四.训练模型"></a>四.训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">net = AlexNet(num_classes=<span class="number">5</span>, init_weights=<span class="literal">True</span>)</span><br><span class="line">net.to(device)</span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.0002</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line">save_path = <span class="string">&#x27;./AlexNet.pth&#x27;</span> <span class="comment">#设置保存参数文件的路径</span></span><br><span class="line">best_acc = <span class="number">0.0</span> <span class="comment">#设置最佳准确率</span></span><br><span class="line">train_steps = <span class="built_in">len</span>(train_loader) <span class="comment">#保存训练集的长度</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs): </span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    net.train() <span class="comment">#将网络设置为训练模式，此时dropout层将发挥作用 </span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    train_bar = tqdm(train_loader, file=sys.stdout) <span class="comment">#利用了 tqdm 库来在训练过程中添加一个进度条，使得用户可以直观地看到数据加载和训练的进度</span></span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_bar):</span><br><span class="line">        images, labels = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = net(images.to(device))</span><br><span class="line">        loss = loss_function(outputs, labels.to(device))</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line"><span class="comment"># 更新 train_bar（即之前通过 tqdm 包装的 train_loader 迭代器）的描述（description）字段。这个描述字段通常用于在进度条旁边显示额外的信息，比如当前的训练轮次（epoch）、总轮次、以及某个指标（如损失值）的当前值。</span></span><br><span class="line">        train_bar.desc = <span class="string">&quot;train epoch[&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                                                 epochs,</span><br><span class="line">                                                                 loss)</span><br><span class="line">        net.<span class="built_in">eval</span>() <span class="comment">#设置为验证模式，此时dropout层效果失效</span></span><br><span class="line">        acc = <span class="number">0.0</span>  <span class="comment"># accumulate accurate number / epoch</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            val_bar = tqdm(validate_loader, file=sys.stdout) <span class="comment">#进度条显示</span></span><br><span class="line">            <span class="keyword">for</span> val_data <span class="keyword">in</span> val_bar:</span><br><span class="line">                val_images, val_labels = val_data</span><br><span class="line">                outputs = net(val_images.to(device))</span><br><span class="line">                predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">                acc += torch.eq(predict_y, val_labels.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">        val_accurate = acc / val_num</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;[epoch %d] train_loss: %.3f  val_accuracy: %.3f&#x27;</span> %  <span class="comment">#每一次迭代后，打印相关信息</span></span><br><span class="line">              (epoch + <span class="number">1</span>, running_loss / train_steps, val_accurate)) </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> val_accurate &gt; best_acc: <span class="comment">#保存历史最优准确率，并将该准确率下的参数情况传入到指定文件中</span></span><br><span class="line">            best_acc = val_accurate</span><br><span class="line">            torch.save(net.state_dict(), save_path)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure><p>训练结果如下：</p><p><img src="https://s21.ax1x.com/2024/08/31/pAEsrFK.png" alt="pAEsrFK.png"></p><h2 id="五-测试模型效果"><a href="#五-测试模型效果" class="headerlink" title="五.测试模型效果"></a>五.测试模型效果</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">data_transform = transforms.Compose(</span><br><span class="line">    [transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">     transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># load image</span></span><br><span class="line">img_path = <span class="string">&quot;../tulip.jpg&quot;</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(img_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(img_path)</span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line">plt.imshow(img) <span class="comment">#展示图片</span></span><br><span class="line"><span class="comment"># [N, C, H, W]</span></span><br><span class="line">img = data_transform(img) <span class="comment">#对图片进行预处理</span></span><br><span class="line"><span class="comment"># expand batch dimension</span></span><br><span class="line">img = torch.unsqueeze(img, dim=<span class="number">0</span>) <span class="comment">#添加一个维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># read class_indict</span></span><br><span class="line">json_path = <span class="string">&#x27;./class_indices.json&#x27;</span> </span><br><span class="line"><span class="keyword">assert</span> os.path.exists(json_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(json_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(json_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f: <span class="comment">#读取类别映射文件，并解码</span></span><br><span class="line">    class_indict = json.load(f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create model</span></span><br><span class="line">model = AlexNet(num_classes=<span class="number">5</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load model weights</span></span><br><span class="line">weights_path = <span class="string">&quot;./AlexNet.pth&quot;</span> </span><br><span class="line"><span class="keyword">assert</span> os.path.exists(weights_path), <span class="string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="built_in">format</span>(weights_path)</span><br><span class="line">model.load_state_dict(torch.load(weights_path)) <span class="comment">#读取之前保存的参数文件</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>() <span class="comment">#设置为验证模式</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># predict class</span></span><br><span class="line">    output = torch.squeeze(model(img.to(device))).cpu() <span class="comment">#输出，并移除张量中所有大小为1的维度</span></span><br><span class="line">    predict = torch.softmax(output, dim=<span class="number">0</span>) <span class="comment">#使用softmax将输出中的张量变成概率分布形式</span></span><br><span class="line">    predict_cla = torch.argmax(predict).numpy() <span class="comment">#找到其中最大的值</span></span><br><span class="line"></span><br><span class="line">print_res = <span class="string">&quot;class: &#123;&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(predict_cla)], <span class="comment">#返回预测的类别名称与预测概率</span></span><br><span class="line">                                             predict[predict_cla].numpy())</span><br><span class="line">plt.title(print_res) <span class="comment">#展示图片与预测结果</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(predict)): <span class="comment">#遍历显示所用的预测结果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;class: &#123;:10&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="built_in">format</span>(class_indict[<span class="built_in">str</span>(i)],</span><br><span class="line">                                              predict[i].numpy()))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果如图所示：</p><ul><li><p><img src="https://s21.ax1x.com/2024/08/31/pAEyAmR.png" alt="pAEyAmR.png"></p></li><li><p><img src="https://s21.ax1x.com/2024/08/31/pAEySYT.png" alt="pAEySYT.png"></p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;深度学习第二章-AlexNet搭建&quot;&gt;&lt;a href=&quot;#深度学习第二章-AlexNet搭建&quot; class=&quot;headerlink&quot; title=&quot;深度学习第二章-AlexNet搭建&quot;&gt;&lt;/a&gt;深度学习第二章-AlexNet搭建&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;本模型存放于目录：&lt;/p&gt;
&lt;p&gt;E:&#92;python文件&#92;deep-learning-for-image-processing-master&#92;tensorflow_classification&#92;Test2_alexnet&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="硕士阶段学习笔记(入门阶段)" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/"/>
    
    <category term="模型" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="深度学习基础" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>深度学习第一章-LeNet搭建</title>
    <link href="http://example.com/2024/08/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0-LeNET%E6%90%AD%E5%BB%BA/"/>
    <id>http://example.com/2024/08/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0-LeNET%E6%90%AD%E5%BB%BA/</id>
    <published>2024-08-29T11:51:21.000Z</published>
    <updated>2024-09-19T09:01:29.394Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度学习第一章-LeNet搭建"><a href="#深度学习第一章-LeNet搭建" class="headerlink" title="深度学习第一章-LeNet搭建"></a>深度学习第一章-LeNet搭建</h1><blockquote><p>基于PyTorch框架，本模型存放于目录：</p><p>E:\python文件\deep-learning-for-image-processing-master\pytorch_classification\Test1_official_demo</p><p>经卷积后的矩阵尺寸大小计算公式为：</p><p>N=(W-F+2P)/S+1</p><ul><li>输入图片大小W×W</li><li>Filter大小FXF</li><li>步长S</li><li>padding的像素数P</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br></pre></td></tr></table></figure></blockquote><span id="more"></span><h2 id="一-设置运行设备：GPU运行"><a href="#一-设置运行设备：GPU运行" class="headerlink" title="一.设置运行设备：GPU运行"></a>一.设置运行设备：GPU运行</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>) <span class="comment">#确认运行设备为GPU</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;定义了一个名字为Net的网络模型&#x27;&#x27;&#x27;</span></span><br><span class="line">net = Net().to(device) <span class="comment">#实例化一个网络模型,并将网络模型在GPU上运算</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;定义训练方法&#x27;&#x27;&#x27;</span></span><br><span class="line">data,target=data.to(device),target.to(device) <span class="comment">#将数据集添加到GPU上运算</span></span><br></pre></td></tr></table></figure><h2 id="二-下载数据集-CIFAR10"><a href="#二-下载数据集-CIFAR10" class="headerlink" title="二.下载数据集-CIFAR10"></a>二.下载数据集-CIFAR10</h2><p>It has the classes:’airplane’,’automobile’,’bird’,’cat’,’deer’,dog’,’frog’,’horse’, ship’,’truck’.</p><p>The images in CIFAR-10 are of size 3x32x32</p><h3 id="1-定义预处理函数"><a href="#1-定义预处理函数" class="headerlink" title="1.定义预处理函数"></a>1.定义预处理函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose( <span class="comment">#预处理函数</span></span><br><span class="line">        [transforms.ToTensor(), <span class="comment">#将图片数据转换为张量</span></span><br><span class="line">         transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))]) <span class="comment">#进行标准化,分别为均值与标准差，              #output[channel] = (input[channel] - mean[channel]) / std[channel]</span></span><br></pre></td></tr></table></figure><h3 id="2-分别下载训练集，测试集并加载"><a href="#2-分别下载训练集，测试集并加载" class="headerlink" title="2.分别下载训练集，测试集并加载"></a>2.分别下载训练集，测试集并加载</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 50000张训练图片</span></span><br><span class="line"><span class="comment"># 第一次使用时要将download设置为True才会自动去下载数据集</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>,  <span class="comment">#下载训练集集</span></span><br><span class="line">                                         download=<span class="literal">False</span>, transform=transform)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">36</span>,  <span class="comment">#加载训练集</span></span><br><span class="line">                                           shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 10000张验证图片</span></span><br><span class="line"><span class="comment"># 第一次使用时要将download设置为True才会自动去下载数据集</span></span><br><span class="line">val_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                       download=<span class="literal">False</span>, transform=transform)</span><br><span class="line">val_loader = torch.utils.data.DataLoader(val_set, batch_size=<span class="number">5000</span>,</span><br><span class="line">                                         shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="3-设置一个数据迭代器"><a href="#3-设置一个数据迭代器" class="headerlink" title="3.设置一个数据迭代器"></a>3.设置一个数据迭代器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val_data_iter = <span class="built_in">iter</span>(val_loader) <span class="comment">#将数据转换为迭代器</span></span><br><span class="line">val_image, val_label = <span class="built_in">next</span>(val_data_iter) <span class="comment">#设置迭代器下的输入与输出</span></span><br><span class="line">val_image, val_label = val_image.to(device), val_label.to(device) <span class="comment">#移到GPU</span></span><br></pre></td></tr></table></figure><h2 id="三-定义网络模型"><a href="#三-定义网络模型" class="headerlink" title="三.定义网络模型"></a>三.定义网络模型</h2><ul><li><strong>张量的序列：（batch,channel,height,width）</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>): <span class="comment">#网络模型搭建</span></span><br><span class="line">        <span class="built_in">super</span>(LeNet, <span class="variable language_">self</span>).__init__() <span class="comment">#必要的初识化函数</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.pool1 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.pool2 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>) </span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">32</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment">#前向传播设置</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))    <span class="comment"># input(3, 32, 32) output(16, 28, 28)</span></span><br><span class="line">        x = <span class="variable language_">self</span>.pool1(x)            <span class="comment"># output(16, 14, 14)</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))    <span class="comment"># output(32, 10, 10)</span></span><br><span class="line">        x = <span class="variable language_">self</span>.pool2(x)            <span class="comment"># output(32, 5, 5)</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">32</span>*<span class="number">5</span>*<span class="number">5</span>)       <span class="comment"># output(32*5*5),此处在进入全连接层时，需要将张量展平</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))      <span class="comment"># output(120)</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc2(x))      <span class="comment"># output(84)</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc3(x)              <span class="comment"># output(10)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="四-训练网络模型"><a href="#四-训练网络模型" class="headerlink" title="四.训练网络模型"></a>四.训练网络模型</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">net = LeNet().to(device) <span class="comment">#实例化神经网络模型</span></span><br><span class="line">loss_function = nn.CrossEntropyLoss() <span class="comment">#设置损失函数为交叉熵函数</span></span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.001</span>) <span class="comment">#设置参数更新方法</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line">    </span><br><span class="line">    running_loss = <span class="number">0.0</span> <span class="comment">#损失率累加</span></span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, start=<span class="number">0</span>):</span><br><span class="line">        inputs, labels = data <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = inputs.to(device), labels.to(device) <span class="comment">#移到GPU</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()  <span class="comment">#梯度清零以防止梯度累计</span></span><br><span class="line">        outputs = net(inputs) <span class="comment">#输入数据训练，得到预测值</span></span><br><span class="line">        loss = loss_function(outputs, labels) <span class="comment">#计算损失值</span></span><br><span class="line">        loss.backward()  <span class="comment">#根据损失值进行反向传播，计算梯度</span></span><br><span class="line">        optimizer.step() <span class="comment">#更新参数</span></span><br><span class="line">        running_loss += loss.item() <span class="comment">#进行损失值累加</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">499</span>:    <span class="comment"># print every 500 mini-batches</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():  <span class="comment">#在验证过程中不要计算损失梯度</span></span><br><span class="line">                outputs = net(val_image)  <span class="comment"># [batch, 10]</span></span><br><span class="line">                predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">                accuracy = torch.eq(predict_y, val_label).<span class="built_in">sum</span>().item() / val_label.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] train_loss: %.3f  test_accuracy: %.3f&#x27;</span> %</span><br><span class="line">                      (epoch + <span class="number">1</span>, step + <span class="number">1</span>, running_loss / <span class="number">500</span>, accuracy))</span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure><p>结果如下：</p><p><img src="https://s21.ax1x.com/2024/08/31/pAEswe1.png" alt="pAEswe1.png"></p><h2 id="五-保存训练完成之后的模型参数文件"><a href="#五-保存训练完成之后的模型参数文件" class="headerlink" title="五.保存训练完成之后的模型参数文件"></a>五.保存训练完成之后的模型参数文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">save_path = <span class="string">&#x27;./Lenet.pth&#x27;</span></span><br><span class="line">torch.save(net.state_dict(), save_path) </span><br></pre></td></tr></table></figure><h2 id="六-使用任意图片测试分类效果"><a href="#六-使用任意图片测试分类效果" class="headerlink" title="六.使用任意图片测试分类效果"></a>六.使用任意图片测试分类效果</h2><ul><li>此处在文件夹里面添加了一个文件名为“1.jpg”的飞机图片</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)), <span class="comment">#将图像分辨率改变</span></span><br><span class="line">     transforms.ToTensor(), <span class="comment">#将图像数据变为张量形式</span></span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))]) <span class="comment">#标准化图像</span></span><br><span class="line"></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br><span class="line"></span><br><span class="line">net = LeNet()</span><br><span class="line">net.load_state_dict(torch.load(<span class="string">&#x27;Lenet.pth&#x27;</span>)) <span class="comment">#载入参数文件</span></span><br><span class="line"></span><br><span class="line">im = Image.<span class="built_in">open</span>(<span class="string">&#x27;1.jpg&#x27;</span>)</span><br><span class="line">im = transform(im)  <span class="comment"># [C, H, W] #调整图像</span></span><br><span class="line">im = torch.unsqueeze(im, dim=<span class="number">0</span>)  <span class="comment"># [N, C, H, W] #增添一个维度</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = net(im)</span><br><span class="line">    predict = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>].numpy()</span><br><span class="line"><span class="built_in">print</span>(classes[<span class="built_in">int</span>(predict)])</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;深度学习第一章-LeNet搭建&quot;&gt;&lt;a href=&quot;#深度学习第一章-LeNet搭建&quot; class=&quot;headerlink&quot; title=&quot;深度学习第一章-LeNet搭建&quot;&gt;&lt;/a&gt;深度学习第一章-LeNet搭建&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;基于PyTorch框架，本模型存放于目录：&lt;/p&gt;
&lt;p&gt;E:&#92;python文件&#92;deep-learning-for-image-processing-master&#92;pytorch_classification&#92;Test1_official_demo&lt;/p&gt;
&lt;p&gt;经卷积后的矩阵尺寸大小计算公式为：&lt;/p&gt;
&lt;p&gt;N=(W-F+2P)/S+1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入图片大小W×W&lt;/li&gt;
&lt;li&gt;Filter大小FXF&lt;/li&gt;
&lt;li&gt;步长S&lt;/li&gt;
&lt;li&gt;padding的像素数P&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torchvision&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch.nn &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch.nn.functional &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; F&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch.optim &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; optim&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torchvision.transforms &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; transforms&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; PIL &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; Image&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="硕士阶段学习笔记(入门阶段)" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/"/>
    
    <category term="模型" scheme="http://example.com/categories/%E7%A1%95%E5%A3%AB%E9%98%B6%E6%AE%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A5%E9%97%A8%E9%98%B6%E6%AE%B5/%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="深度学习基础" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>MRI超分辨率-初见</title>
    <link href="http://example.com/2024/08/23/MRI%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87-%E5%88%9D%E8%A7%81/"/>
    <id>http://example.com/2024/08/23/MRI%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87-%E5%88%9D%E8%A7%81/</id>
    <published>2024-08-23T03:04:29.000Z</published>
    <updated>2024-08-27T14:09:05.583Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MRI超分辨率-初见"><a href="#MRI超分辨率-初见" class="headerlink" title="MRI超分辨率-初见"></a>MRI超分辨率-初见</h1><blockquote><p>参考文献：<a href="https://kns.cnki.net/kcms2/article/abstract?v=MBTPQIn9ZKHwJKNtmL5eu7ZFUIb7_nCdbjKrKYFXDX6PgaFW9ljQXL28jn2X8Tj-Fz9N66Pw2G-7sZKyd45rdpBDmk8uzaCVlHLs8CrT9djwL1vEujSi471jNVkdUlEig7F0PF8coO7qxlD8DotHOeCi2IyxGNPoEJVYW6xeQwx-YP19kSVBS-Q5b6eyw2jueAmJScamN0AezEUaa59P5Xn_eLIAQ_aMAfpnTy3Ubv6W8vCSIDYLpFbFr3BReGkWAXsly07QS-ieh65-u_RMjowiogGdel8q&amp;uniplatform=NZKPT&amp;language=CHS">单幅3D磁共振图像超分辨率算法研究</a></p></blockquote><span id="more"></span><h2 id="一-研究背景"><a href="#一-研究背景" class="headerlink" title="一.研究背景"></a>一.研究背景</h2><h3 id="1-MRI图像存在的问题"><a href="#1-MRI图像存在的问题" class="headerlink" title="1.MRI图像存在的问题"></a>1.MRI图像存在的问题</h3><ul><li><p>MRI图像的质量受到信噪比（SNR，Signal to Noise Ratio）、分辨率和扫描时间等多方面因素影响</p></li><li><p>层厚的概念</p><ul><li><blockquote><p>层厚的定义：表示成像层面在三维空间中的厚度。在MRI扫描中，由于技术的限制，无法直接获取无限薄的层面，因此层面的选取在实际操作中都是有一定厚度的。</p></blockquote></li></ul></li><li><p>层厚对图片质量的影响</p><ul><li>分辨率：层厚直接影响到MRI图像的分辨率。层厚越小，图像的分辨率越高，能够显示更细微的解剖结构。但过小的层厚会导致扫描时间增加，且可能因信号强度不足而影响图像质量。</li><li>信噪比：层厚还影响图像的信噪比。层厚较小时，每个体素内质子数量减少，产生的信号强度降低，可能导致信噪比下降。而适当增加层厚，可以提高信号强度，改善图像质量。</li><li>扫描时间：层厚越大，扫描时间越短</li></ul></li></ul><h3 id="2-优化MRI图像的方法—-超分辨率"><a href="#2-优化MRI图像的方法—-超分辨率" class="headerlink" title="2.优化MRI图像的方法—-超分辨率"></a>2.优化MRI图像的方法—-超分辨率</h3><ul><li>定义：超分辨率（SR，Super-resolution）算法是一种从软件层面提高 MRI 图像分辨率的有效方案。从软件方面提升 MRI 图像分辨率的方式具有良好的可移植性与扩展性。</li><li>MRI 图像超分辨率问题是典型的病态逆问题，其主要目的是从一幅或多幅低分辨率MRI图像中重建出对应的高分辨率 MRI 图像</li></ul><h3 id="3-超分辨率的不同研究类别"><a href="#3-超分辨率的不同研究类别" class="headerlink" title="3.超分辨率的不同研究类别"></a>3.超分辨率的不同研究类别</h3><ul><li><p>静态对象与动态对象</p></li><li><p>单幅低分辨率图像和多幅低分辨率图像</p><ul><li><p>多幅的方法需要将多幅低分辨率的 MRI 图像进行配准与重采样</p></li><li><p>配准概念</p><blockquote><ul><li><p>配准（Image Registration）也被称为图像匹配或图像相关，是图像处理中的一个关键步骤。它旨在将两幅或多幅图像中的相应部分进行空间上的对齐，使它们在同一坐标系下具有相同的空间位置。这个过程要求相邻图像之间有一部分在逻辑上是相同的，即相邻的图像有一部分反映了同一目标区域，这是实现图像配准的基本条件。</p><p>配准通常包括两个主要步骤：空间变换和灰度变换。</p><ul><li><strong>空间变换</strong>：将图像像素的坐标从一个坐标系映射到另一个新的坐标系中，通常使用多项式函数或其他变换模型来描述这种映射关系。对于具有全局性形变的图像配准问题，非线性变换（如将直线映射为曲线）是一个常用的选择。</li><li><strong>灰度变换</strong>：在空间变换完成后，对变换后的图像值进行重新赋值，以确保图像在视觉上保持一致性和连续性。这一步骤与重采样紧密相关。</li></ul></li></ul></blockquote></li><li><p>重采样概念</p><blockquote><ul><li><p>重采样（Resampling）是指根据一类象元（像素）的信息内插出另一类象元信息的过程。在遥感、图像处理及GIS等领域中，重采样常用于从高分辨率图像中提取低分辨率图像，或者对图像进行几何校正、尺寸调整等操作。</p><p>重采样的方法多种多样，常见的包括最邻近法、双线性内插法和三次卷积内插法等。</p><ul><li><strong>最邻近法</strong>：将距离某像元位置最近的像元值作为该像元的新值。这种方法简单高效，但可能会产生半个像元大小的位移，计算不够精确。</li><li><strong>双线性内插法</strong>：通过取采样点到周围4个邻域像元的距离加权来计算其新值。这种方法通常比最邻近法产生的结果更加光滑，但可能会改变原始栅格值，丢失一些局部细微的特征。</li><li><strong>三次卷积内插法</strong>：通过增加参与内插计算的邻近像元的数目来提高重采样的精度。这种方法能够增强图像的细节表现，但计算量较大，且同样会改变原始栅格值。</li></ul></li></ul></blockquote></li></ul></li></ul><ul><li><p>频域与空间域：空间域中处理更优</p></li><li><p>各向同性与各向异性</p><ul><li>各向同性 3D MRI 图像超分辨率方法主要目标是提高 3D MRI 图像所有层面的分辨率，使其整体的分辨率更高。</li><li>各向异性：由于成像技术的限制，图像在不同方向上的分辨率往往是不均等的。这种在不同方向上分辨率存在差异的现象被称为“各向异性”。<ul><li>对于3D MRI图像来说，这种各向异性主要体现在层面选择方向（通常指的是沿着成像序列中切片堆叠的方向，即深度或厚度方向）与层面内方向（即每个切片内部的二维空间，包括宽度和高度方向）的分辨率差异上</li><li>在3D MRI图像中，沿着层面选择方向（深度或厚度）的像素（或体素）尺寸通常比层面内方向的像素尺寸要大，因此在这个方向上的图像细节相对较少，分辨率较低。</li><li>相比之下，层面内的切片包含了更多的细节信息，即高频信息（图像中变化较快的部分，如边缘、纹理等），这些信息的分辨率较高。</li><li>对于各向异性的3D MRI图像，超分辨率方法的主要目标是提高层面选择方向的分辨率，以减小或消除与层面内方向分辨率之间的差异。</li><li>通过这种方法，可以使得3D MRI图像在所有方向上的分辨率更加均匀，从而提高图像的整体质量和可用性。</li></ul></li></ul><h3 id="4-超分辨率的研究算法分类"><a href="#4-超分辨率的研究算法分类" class="headerlink" title="4.超分辨率的研究算法分类"></a>4.超分辨率的研究算法分类</h3><ul><li>基于插值的超分辨率算法<ul><li>方法：预定义一个变换函数，并利用已知的 MRI 图像像素或体素信息来对未知的像素或体素信息进行拟合。</li><li>该类算法虽然简单高效，但也存在着一定的缺陷，如插值后的图像具有比较明显的块效应、振铃效应与锯齿效应。</li></ul></li><li>基于重构的超分辨算法<ul><li>方法：从图像的降质退化模型出发，对高分辨率 MRI 图像退化到低分辨率 MRI 图像的过程进行建模（运动变换、模糊及噪声等），而后求解成像系统的逆过程。这种算法会提取低分辨率图像中的关键信息，并通过先验知识来约束高分辨率图像的的重建过程。</li><li>该类算法在重建系数较小时能达到令人满意的结果，但当重建系数较大时，很难获得合适的人工定义的先验知识与正则参数，因此难以保证超分辨率图像的质量。</li></ul></li><li>基于学习的超分辨率算法（✪）<ul><li>从大量的训练数据中学习高分辨率图像和低分辨率图像之间某种映射关系，并根据学习到的映射关系提高目标低分辨率图像的分辨率</li><li>分类<ul><li>基于浅层学习的超分辨率算法<ul><li>将整个超分辨率过程分为样本库的建立（特征提取）、特征映射（学习与搜索）、高频信息重建三个阶段，且每个阶段独立优化</li></ul></li><li>基于深度学习的超分辨率算法（✪）<ul><li>直线结构    <ul><li>指整个图像超分辨率网络结构是一个不包含任何跳接或分支的线性结构，即模型是由多个卷积层依次堆叠组成。该类结构具有简单、效率高等特点</li><li>使用线性结构的算法具有结构简单、重建速度快等优点，但同时存在收敛速度慢，训练过程中容易产生梯度消失与爆炸的问题</li></ul></li><li>跳接结构<ul><li>是指在线性结构的基础上加入卷积层之间的跨层跳接。跳接结构通过信息跨层传递的方式来缓解梯度消失与爆炸问题，同时为设计更深的网络结构提供可能。</li><li>残差跳接是将不同卷积层的特征进行对应通道的相加，即融合后的特征图保持通道数量不变，但特征图内容发生了变化。</li><li>通道连接跳接则是将不同卷积层的特征以通道方向进行拼接，融合后的特征图不变，但通道数量增加。</li></ul></li><li>递归结构<ul><li>将卷积层的输出继续作为该层卷积的输入，由于多次卷积操作的参数是一样的，该种递归卷积的方式可以有效地减少网络的参数数量。</li></ul></li><li>生成对抗结构<ul><li>采用的是一种博弈论的思想，该类模型由生成器与鉴别器两部分组成。生成器生成高分辨率图像，鉴别器区分真实的高分辨率图像与生成器生成的高分辨率图像。</li></ul></li><li>注意力结构<ul><li>注意力结构则是利用人类视觉中的注意力机制，对网络所提取的特征进行重新校准</li><li>注意力结构可以有效的调整特征通道或特征本身不同区域的权重，专注于模型中对最终结果贡献大的特征或特征区域</li></ul></li></ul></li><li>图示<ul><li><img src="https://s21.ax1x.com/2024/08/27/pAk4BcT.png" alt="pAk4BcT.png"></li></ul></li></ul></li></ul></li></ul></li></ul><h2 id="二-基础知识（基于CNN的算法）"><a href="#二-基础知识（基于CNN的算法）" class="headerlink" title="二.基础知识（基于CNN的算法）"></a>二.基础知识（基于CNN的算法）</h2><h3 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1.数据预处理"></a>1.数据预处理</h3><ul><li><p><img src="https://s21.ax1x.com/2024/08/27/pAk4DjU.png" alt="pAk4DjU.png"></p></li><li><h4 id="数据归一化"><a href="#数据归一化" class="headerlink" title="数据归一化"></a>数据归一化</h4><ul><li>需将 MRI 图像的体素值归一化到相同的取值范围（如[0，1]），使优化时大部分位置的梯度方向近似于最优搜索方向，从而提高训练效率。否则会导致模型收敛慢。</li></ul></li><li><h4 id="模拟退化"><a href="#模拟退化" class="headerlink" title="模拟退化"></a>模拟退化</h4><ul><li>首先将高分辨率 MRI 图像经过傅里叶变换至 k 空间。<ul><li>k 空间中央部分包含的数据具有高信号振幅和低分辨率，决定 MRI 图像的对比度</li><li>边缘的部分具有低信号振幅和高分辨率，决定 MRI 图像的解剖细节。</li></ul></li><li>k 空间的模拟退化中，本文仅保留中央部分的信息。根据超分辨率重建系数对 k 空间数据的边缘部分进行截断处理，并用零填充的方式对截断的部分进行填充。随后对填充过的 k 空间数据进行傅里叶逆变换，将其转化至图像空间。</li><li>最后，对 MRI 图像进行空间下采样至目标大小，生成最终的低分辨率 MRI 图像</li><li>傅里叶变换：将图像从空间域（spatial domain）转换到频率域（frequency domain）。这种转换使我们能够分析图像的频率成分，即图像中不同模式的强度或频率。</li><li><img src="https://s21.ax1x.com/2024/08/27/pAk4f9x.png" alt="pAk4f9x.png"></li></ul></li><li><h4 id="块切分"><a href="#块切分" class="headerlink" title="块切分"></a>块切分</h4><ul><li>在构建训练集时将整幅 3D MRI 图像切割成固定大小的 3D MRI 图像块。本文采用固定步长的滑动窗口方式来对图像进 行切割，保证相邻图像块既有相似重叠区域又有不同区域，从而提升训练数据的 数量与训练数据的丰富度。</li></ul></li></ul><h3 id="2-模型框架"><a href="#2-模型框架" class="headerlink" title="2.模型框架"></a>2.模型框架</h3><ul><li><p><img src="https://s21.ax1x.com/2024/08/27/pAk4h36.png" alt="pAk4h36.png"></p></li><li><p>预采样模型框架</p><ul><li>该类框架需将低分辨率图像在输入网络前用传统的插值方法（线性插值法或双三次插值法）上采样到目标高分辨率图像的大小，而后再用 CNN 模型重建出高质量的纹理细节， 进一步提高图像的质量。</li></ul></li><li><p>后上采样模型框架</p><ul><li>该类模型框架以原始的低分辨率图像作为输入，将上采样任务放在模型的尾端。该类模型直接从原始低分辨率图像中提取特征并学习数据的原始分布，用可学习的上采样方式（如：反卷积与亚像素卷积）对图像进行上采样与重建。</li></ul></li><li><p>渐进式上采样模型框架</p><ul><li>该框架将大尺度超分辨率重建问题分解为多个小尺度的超分辨率重建问题。该框架利用级联的方式进行对图像进行逐步的上采样，并在此过程中融入多监督机制来缓解学习难度大的问题。</li></ul></li><li>迭代上下采样模型框架<ul><li>该类模型框架通过反复的将低分辨率空间图像上采样至高分辨率空间，再将高分辨率空间图像下采样至低分辨率空间，在此过程中利用投影误差来反复纠正超分辨率结果，达到提升超分辨性能的目的。</li></ul></li></ul><h2 id="三-评价标准"><a href="#三-评价标准" class="headerlink" title="三.评价标准"></a>三.评价标准</h2><ul><li>峰值信噪比（PSNR）<ul><li><script type="math/tex; mode=display">M S E = \frac { 1 } { N } \sum _ { I = 1 } ^ { N } ( I _ { y } ( i ) - \widehat { I } _ { y } ( i ) ) ^ { 2 }</script></li><li><script type="math/tex; mode=display">P S N R = 1 0 \cdot \log _ { 1 0 } ( \frac { L ^ { 2 } } { M S E } )</script></li></ul></li><li><p>结构相似度指数（SSIM）</p><ul><li><img src="https://s21.ax1x.com/2024/08/27/pAk45jO.png" alt="pAk45jO.png"></li></ul></li><li><p>主观评价</p></li></ul><h2 id="四-超分辨率的相关总结"><a href="#四-超分辨率的相关总结" class="headerlink" title="四.超分辨率的相关总结"></a>四.超分辨率的相关总结</h2><h3 id="1-超分辨率算法分类"><a href="#1-超分辨率算法分类" class="headerlink" title="1.超分辨率算法分类"></a>1.超分辨率算法分类</h3><h4 id="①-基于插值的算法"><a href="#①-基于插值的算法" class="headerlink" title="① 基于插值的算法"></a>① 基于插值的算法</h4><h4 id="②-基于重建的算法"><a href="#②-基于重建的算法" class="headerlink" title="② 基于重建的算法"></a>② 基于重建的算法</h4><h4 id="③-基于学习的算法-（常用）"><a href="#③-基于学习的算法-（常用）" class="headerlink" title="③ 基于学习的算法 （常用）"></a>③ 基于学习的算法 （常用）</h4><ul><li>基于深度学习的算法：基于深度学习的ＳＲ重建算法通常是构建一个端对端的网络模型，将ＬＲ图像输入到该特定网络模型中，通过特征映射和尺度放大等方式优化网络的损失函数，进而得到ＨＲ图 像</li></ul><h3 id="2-尺度放大方式"><a href="#2-尺度放大方式" class="headerlink" title="2.尺度放大方式"></a>2.尺度放大方式</h3><h4 id="①-上采样方法"><a href="#①-上采样方法" class="headerlink" title="① 上采样方法"></a>① 上采样方法</h4><h5 id="基于插值的上采样"><a href="#基于插值的上采样" class="headerlink" title="基于插值的上采样"></a>基于插值的上采样</h5><h5 id="基于反卷积的上采样-（常用）"><a href="#基于反卷积的上采样-（常用）" class="headerlink" title="基于反卷积的上采样 （常用）"></a>基于反卷积的上采样 （常用）</h5><h5 id="基于亚像素卷积的上采样"><a href="#基于亚像素卷积的上采样" class="headerlink" title="基于亚像素卷积的上采样"></a>基于亚像素卷积的上采样</h5><h4 id="②-上采样实施方式"><a href="#②-上采样实施方式" class="headerlink" title="② 上采样实施方式"></a>② 上采样实施方式</h4><h5 id="预先上采样"><a href="#预先上采样" class="headerlink" title="预先上采样"></a>预先上采样</h5><h5 id="单次上采样"><a href="#单次上采样" class="headerlink" title="单次上采样"></a>单次上采样</h5><h5 id="渐进上采样"><a href="#渐进上采样" class="headerlink" title="渐进上采样"></a>渐进上采样</h5><h5 id="迭代上采样-（常用）"><a href="#迭代上采样-（常用）" class="headerlink" title="迭代上采样 （常用）"></a>迭代上采样 （常用）</h5><h3 id="3-模型结构组成"><a href="#3-模型结构组成" class="headerlink" title="3.模型结构组成"></a>3.模型结构组成</h3><h4 id="①-基于CNN的模型（一般不使用池化层）"><a href="#①-基于CNN的模型（一般不使用池化层）" class="headerlink" title="① 基于CNN的模型（一般不使用池化层）"></a>① 基于CNN的模型（一般不使用池化层）</h4><h5 id="直线连接模型"><a href="#直线连接模型" class="headerlink" title="直线连接模型"></a>直线连接模型</h5><ul><li>最大问题就是随着网络深度的加深，参数逐渐增加，网络训练的难度越来越大，导致网络难以收敛，</li></ul><h5 id="残差连接模型"><a href="#残差连接模型" class="headerlink" title="残差连接模型"></a>残差连接模型</h5><ul><li>由于原始ＬＲ图像和输出的ＨＲ图像在很大程度上是相似的， 也就是说ＬＲ图像携带的低频信息与ＨＲ图像的低频信息基本一致。残差连接的应用使得原始的稠密矩阵学习转化为稀疏矩阵学习，因而使得计算量大幅度降低</li></ul><h5 id="密集连接模型"><a href="#密集连接模型" class="headerlink" title="密集连接模型"></a>密集连接模型</h5><ul><li>在保证网络中层与层之间最大程度的信息传输的前提下，直接将所有层连接起来，使网络中每一层输入为之前卷积层输出的总和，极大地增强了信息流动的能力，有效抑制了梯度爆炸和消失的问题。</li></ul><h5 id="注意力模型"><a href="#注意力模型" class="headerlink" title="注意力模型"></a>注意力模型</h5><ul><li>通过学习不同通道的重要性得到一个权重值，这相当于对信道间特征的相互关系进行建模， 自适应调整每个信道特征，从而在有效强化有用特 征通道的同时抑制无用特征通道，使计算资源得到更充分的利用。</li></ul><h4 id="②-基于CNN-RNN的模型"><a href="#②-基于CNN-RNN的模型" class="headerlink" title="② 基于CNN-RNN的模型"></a>② 基于CNN-RNN的模型</h4><ul><li>递归神经网络就是充分利用参数共享机制，使其在不增加参数的情况下加深网络的深度，降低网络的复杂度，加快训练速度</li></ul><h4 id="③-基于GAN的模型"><a href="#③-基于GAN的模型" class="headerlink" title="③ 基于GAN的模型"></a>③ 基于GAN的模型</h4><ul><li><img src="https://s21.ax1x.com/2024/08/27/pAk7O6x.png" alt="pAk7O6x.png"></li></ul><h3 id="4-损失函数分类"><a href="#4-损失函数分类" class="headerlink" title="4.损失函数分类"></a>4.损失函数分类</h3><h4 id="①基于像素的损失函数"><a href="#①基于像素的损失函数" class="headerlink" title="①基于像素的损失函数"></a>①基于像素的损失函数</h4><ul><li>均方误差（MSE）</li><li>平均绝对值误差（MAE）</li><li>本质上都是反映对应像素之间的误差关系，忽略了像素与邻域像素间存在的内在联系，因 而重建图像质量存在边缘模糊和振铃现象。</li></ul><h4 id="②基于感知的损失函数"><a href="#②基于感知的损失函数" class="headerlink" title="②基于感知的损失函数"></a>②基于感知的损失函数</h4><ul><li>内容损失函数</li><li>对抗损失函数</li><li>上下文损失函数</li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;MRI超分辨率-初见&quot;&gt;&lt;a href=&quot;#MRI超分辨率-初见&quot; class=&quot;headerlink&quot; title=&quot;MRI超分辨率-初见&quot;&gt;&lt;/a&gt;MRI超分辨率-初见&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;参考文献：&lt;a href=&quot;https://kns.cnki.net/kcms2/article/abstract?v=MBTPQIn9ZKHwJKNtmL5eu7ZFUIb7_nCdbjKrKYFXDX6PgaFW9ljQXL28jn2X8Tj-Fz9N66Pw2G-7sZKyd45rdpBDmk8uzaCVlHLs8CrT9djwL1vEujSi471jNVkdUlEig7F0PF8coO7qxlD8DotHOeCi2IyxGNPoEJVYW6xeQwx-YP19kSVBS-Q5b6eyw2jueAmJScamN0AezEUaa59P5Xn_eLIAQ_aMAfpnTy3Ubv6W8vCSIDYLpFbFr3BReGkWAXsly07QS-ieh65-u_RMjowiogGdel8q&amp;amp;uniplatform=NZKPT&amp;amp;language=CHS&quot;&gt;单幅3D磁共振图像超分辨率算法研究&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="学术论文总结" scheme="http://example.com/categories/%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="MRI超分辨率" scheme="http://example.com/tags/MRI%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>医学图像降噪-初见</title>
    <link href="http://example.com/2024/08/19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E9%99%8D%E5%99%AA-%E5%88%9D%E8%A7%81/"/>
    <id>http://example.com/2024/08/19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E9%99%8D%E5%99%AA-%E5%88%9D%E8%A7%81/</id>
    <published>2024-08-19T13:11:33.000Z</published>
    <updated>2024-08-27T13:31:14.142Z</updated>
    
    <content type="html"><![CDATA[<h1 id="医学图像降噪-初见（未完待续）"><a href="#医学图像降噪-初见（未完待续）" class="headerlink" title="医学图像降噪-初见（未完待续）"></a>医学图像降噪-初见（未完待续）</h1><blockquote><p>医学图像降噪概况-参考论文：<a href="https://www.sciencedirect.com/science/article/pii/S1746809420301920">A review on medical image denoising algorithms</a></p></blockquote><span id="more"></span><h2 id="一-主要的问题"><a href="#一-主要的问题" class="headerlink" title="一.主要的问题"></a>一.主要的问题</h2><ul><li>医学图像将被噪音影响，噪声包括随机噪声，白噪声（在一定频率范围内的功率谱密度均匀分布的噪声），伪影</li><li>伪影（Artifacts）：与患者有关或者与机器有关，指的是原本被扫描或拍摄的物体上并不存在，但在最终生成的图像上却出现的各种形态的影像。</li></ul><h2 id="二-医学图像分类"><a href="#二-医学图像分类" class="headerlink" title="二.医学图像分类"></a>二.医学图像分类</h2><h3 id="1-Ultrasound-US-images（超声图像）"><a href="#1-Ultrasound-US-images（超声图像）" class="headerlink" title="1.Ultrasound (US) images（超声图像）"></a>1.Ultrasound (US) images（超声图像）</h3><ul><li><p>概述：利用超声波在组织内的传播、反射和衍射等现象，通过声波与组织之间的相互作用，获取人体内部结构的图像（对气体及骨性结构成像效果差）</p></li><li><p>斑点噪声（Speckle）</p><blockquote><p>由于超声波在人体组织中的散射效应而产生的随机分布的亮点或暗点。这些斑点噪声的形成主要是由于人体软组织对探头发射出的超声波会形成大量大小不等、方向各异的回波界面，这些回波在局部空间相互叠加或抵消，从而形成回声强度不一的斑点，即超声散斑。</p></blockquote></li></ul><h3 id="2-Magnetic-Resonance-MR-images（核磁共振图像）"><a href="#2-Magnetic-Resonance-MR-images（核磁共振图像）" class="headerlink" title="2.Magnetic Resonance (MR) images（核磁共振图像）"></a>2.Magnetic Resonance (MR) images（核磁共振图像）</h3><ul><li><p>利用原子核在磁场内共振所产生的信号经重建成像而得到的。</p><blockquote><p>当人体置于特定的磁场中，并接受一定频率的射频脉冲激励时，人体组织中的氢原子核会发生磁共振现象。射频脉冲终止后，氢原子核在驰豫过程中会感应出MR信号，这些信号被接收并经过空间编码和图像重建等处理过程，最终生成MR图像。</p></blockquote></li></ul><h3 id="3-Computed-Tomography-CT-计算机断层扫描"><a href="#3-Computed-Tomography-CT-计算机断层扫描" class="headerlink" title="3.Computed Tomography (CT,计算机断层扫描)"></a>3.Computed Tomography (CT,计算机断层扫描)</h3><ul><li>使用X射线束从一些不同的角度观察对象，然后将其横截面图像重建为3D数据</li><li>由X射线量子的统计涨落引起的量子噪声是CT投影（正弦图）中的主要噪声源</li></ul><h3 id="4-Positron-Emission-Tomog-raphy-images-PET-正电子发射断层扫描"><a href="#4-Positron-Emission-Tomog-raphy-images-PET-正电子发射断层扫描" class="headerlink" title="4.Positron Emission Tomog-raphy  images (PET,正电子发射断层扫描)"></a>4.Positron Emission Tomog-raphy  images (PET,正电子发射断层扫描)</h3><ul><li>PET通过检测放射性物质发出的辐射来产生身体的图像。这些物质被注射到体内，通常用放射性原子标记，如衰变时间短的碳11。</li></ul><h2 id="三-医学图像降噪的基本需求"><a href="#三-医学图像降噪的基本需求" class="headerlink" title="三.医学图像降噪的基本需求"></a>三.医学图像降噪的基本需求</h2><ul><li>边缘保护和细节</li><li>保持结构相似性（不出现伪影）</li><li>降低复杂度</li><li>对数据集的数量要求不多</li></ul><h2 id="四-相关技术"><a href="#四-相关技术" class="headerlink" title="四.相关技术"></a>四.相关技术</h2><h3 id="1-超声图像（US）"><a href="#1-超声图像（US）" class="headerlink" title="1.超声图像（US）"></a>1.超声图像（US）</h3><ul><li>斑点噪声（由散射现象引起的）会降低图片的质量</li><li>待续</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>医学图像去噪问题<ul><li>（i）保留边缘和其他更精细的细节</li><li>（ii）保持结构相似性</li><li>（iii）不出现伪影和</li><li>（iv）低操作复杂度。</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;医学图像降噪-初见（未完待续）&quot;&gt;&lt;a href=&quot;#医学图像降噪-初见（未完待续）&quot; class=&quot;headerlink&quot; title=&quot;医学图像降噪-初见（未完待续）&quot;&gt;&lt;/a&gt;医学图像降噪-初见（未完待续）&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;医学图像降噪概况-参考论文：&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1746809420301920&quot;&gt;A review on medical image denoising algorithms&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="学术论文总结" scheme="http://example.com/categories/%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="医学图像降噪" scheme="http://example.com/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E9%99%8D%E5%99%AA/"/>
    
  </entry>
  
  <entry>
    <title>操作系统第五章-输入/输出(I/O)管理</title>
    <link href="http://example.com/2024/08/12/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AC%AC%E4%BA%94%E7%AB%A0-%E8%BE%93%E5%85%A5-%E8%BE%93%E5%87%BA-I-O-%E7%AE%A1%E7%90%86/"/>
    <id>http://example.com/2024/08/12/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AC%AC%E4%BA%94%E7%AB%A0-%E8%BE%93%E5%85%A5-%E8%BE%93%E5%87%BA-I-O-%E7%AE%A1%E7%90%86/</id>
    <published>2024-08-11T17:34:20.000Z</published>
    <updated>2024-08-12T03:18:37.355Z</updated>
    
    <content type="html"><![CDATA[<h2 id="操作系统第五章-输入-输出（I-O）管理"><a href="#操作系统第五章-输入-输出（I-O）管理" class="headerlink" title="操作系统第五章 输入/输出（I/O）管理"></a>操作系统第五章 输入/输出（I/O）管理</h2><blockquote><p>计算机学科基础：操作系统第五章输入/输出(I/O)管理的学习笔记</p></blockquote><span id="more"></span><h3 id="一-I-O管理概述（✠）"><a href="#一-I-O管理概述（✠）" class="headerlink" title="一.I/O管理概述（✠）"></a>一.I/O管理概述（✠）</h3><h4 id="1-I-O设备"><a href="#1-I-O设备" class="headerlink" title="1.I/O设备"></a>1.I/O设备</h4><ul><li><p>I/O设备的定义</p><ul><li>I/O设备就是可以将数据输入到计算机，或者可以接收计算机输出数据的外部设备，属于计算机中的硬件部件</li><li>UNIX系统将外部设备抽象为一种特殊的文件，用户可以使用与文件操作相同的方式对外部设备进行操作。</li><li>Write:操作：向外部设备写出数据；Read操作：从外部设备读入数据</li></ul></li><li><p>I/O设备的分类</p><ul><li>按信息交换的单位分类<ul><li>块设备：信息交换以数据块为单位。它属于有结构设备，<strong>如磁盘等</strong>。<br>磁盘设备的基本特征是传输速率较高、可寻址，即对它可随机地读/写任意一块，<strong>如共享设备</strong>。</li><li>字符设备：信息交换以字符为单位。它属于无结构类型，<strong>如交互式终端机、打印机等</strong>。<br>传输速率较慢，不可寻址，在输入/输出时常采用<strong>中断驱动方式</strong></li></ul></li><li>按使用特性分类<ul><li>人机交互类外部设备：鼠标、键盘、打印机等一一用于人机交互，数据传输速度慢</li><li>存储设备：移动硬盘、光盘等一一用于数据存储，数据传输速度快</li><li>网络通信设备：调制解调器等一一用于网络通信，数据传输速度介于上述二者之间</li></ul></li><li>按传输速率分类<ul><li>低速设备。传输速率仅为每秒几字节到数百字节的一类设备，如键盘、鼠标等。</li><li>中速设备。传输速率为每秒数千字节至数万字节的一类设备，如激光打印机等。</li><li>高速设备。传输速率在数百千字节至千兆字节的一类设备，如磁盘机、光盘机等。</li></ul></li></ul></li><li>I/O设备的组成<ul><li>机械部件<ul><li>I/O设备的机械部件主要用来执行具体I/O操作<br>如我们看得见摸得着的鼠标/键盘的按钮；显示器的LED屏；移动硬盘的磁臂、磁盘盘面。</li></ul></li><li>电子部件<ul><li>I/O设备的电子部件通常是一块插入主板扩充槽的印刷电路板</li><li>I/O接口（I/O控制器，设备控制器）<ul><li>I/O接口（设备控制器）位于CPU与设备之间（作为中介），<br>它既要与CPU通信，又要与设备通信，还要具有按CPU发来的命令去控制设备工作的功能</li><li>I/O设备的功能<ul><li>接受和识别CPU发出的命令（<strong>要有控制寄存器</strong>）<ul><li>如CPU发来的read/write命令，I/O控制器中会有相应的控制寄存器来存放命令和参数</li></ul></li><li>向CPU报告设备的状态（<strong>要有状态寄存器</strong>）<ul><li>I/O控制器中会有相应的状态寄存器用于记录1/0设备的当前状态。如：1表示空闲，0表示忙碌</li></ul></li><li>数据交换（<strong>要有数据寄存器，暂存输入输出的数据</strong>）<ul><li>I/O控制器中会设置相应的数据寄存器。输出时，数据寄存器用于暂存CPU发来的数据，之后再由控制器传送设备。</li><li>输入时，数据寄存器用于暂存设备发来的数据，之后CPU从数据寄存器中取走数据</li></ul></li><li>地址识别（<strong>由I/O逻辑实现</strong>)<ul><li>类似于内存的地址，为了区分设备控制器中的各个寄存器，也需要给各个寄存器设置一个特定的“地址”。</li><li>I/O控制器通过CPU提供的“地址”来判断CPU要读/写的是哪个寄存器</li></ul></li></ul></li><li>I/O接口的组成<ul><li>CPU与控制器之间的接口（<strong>实现控制器与CPU之间的通信</strong>）<ul><li>用于实现CPU与控制器之间的通信。</li><li>CPU通过控制线发出命令；通过地址线指明要操作的设备；<br>通过数据线来取出（输入）数据，或放入（输出）数据</li><li>数据线常与两类寄存器相连：数据寄存器（存放从设备送来的输入数据或从CPU送来的输出数据）和控制/状态寄存器(存放从CPU送来的控制信息或设备的状态信息)。</li></ul></li><li>I/O逻辑（<strong>负责识别CPU发出的命令，并向设备发出命令，实现对设备的控制</strong>）<ul><li>用于实现对设备的控制。它通过一组控制线与CPU交互，对从CPU收到的I/O命令进行译码。</li><li>CPU启动设备时，将启动命令发送给控制器，同时通过地址线把地址发送给控制器，由控制器的I/O逻辑对地址进行译码，并相应地对所选设备进行控制。</li></ul></li><li>控制器与设备之间的接口（<strong>实现控制器与设备之间的通信</strong>）<ul><li>一个设备控制器可以连接一个或多个设备，因此控制器中有一个或多个设备接口。每个接口中都存在数据、控制和状态三种类型的信号。</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/25/pPtdZlT.png" alt="pPtdZlT.png"></li></ul></li></ul></li></ul></li><li>I/O端口<ul><li>I/O端口是指设备控制器中可被CPU直接访问的寄存器</li><li>寄存器的分类<ul><li>数据寄存器：实现CPU和外设之间的数据缓冲。</li><li>状态寄存器：获取执行结果和设备的状态信息，以让CPU知道是否准备好。</li><li>控制寄存器：由CPU写入，以便启动命令或更改设备模式。</li></ul></li><li>寄存器的两种编址方式<ul><li>独立编址。为每个端口分配一个I/O端口号，所有I/O端口形成I/O端口空间，普通用户程序不能对其进行访问，只有操作系统使用特殊的I/O指令才能访问端口</li><li>统一编址。又称<strong>内存映射I/O</strong>，每个端口被分配唯一的内存地址，且不会有内存被分配这一地址，通常分配给端口的地址靠近地址空间的顶端。可以采用对内存进行操作的指令来对控制器进行操作</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/25/pPtdx41.png" alt="pPtdx41.png"></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><h4 id="2-I-O控制方式"><a href="#2-I-O控制方式" class="headerlink" title="2.I/O控制方式"></a>2.I/O控制方式</h4><ul><li>程序直接控制方式（轮询）<ul><li>完成一次读/写操作的流程<ul><li><img src="https://s1.ax1x.com/2023/08/25/pPt0FzV.png" alt="pPt0FzV.png"></li></ul></li><li>流程图<ul><li><img src="https://s1.ax1x.com/2023/08/25/pPt0AMT.png" alt="pPt0AMT.png"></li></ul></li><li>CPU干预的频率<ul><li>很频繁，I/O操作开始之前、完成之后需要CPU介入，并且在等待I/O完成的过程中CPU需要不断地轮询检查。</li></ul></li><li>数据传送的单位：每次读/写一个字</li><li>数据的流向<ul><li>读操作（数据输入）：I/O设备→CPU→内存</li><li>写操作（数据输出）：内存→CPU→I/O设备</li><li>每个字的读/写都需要CPU的帮助</li></ul></li><li>主要缺点和主要优点<ul><li>优点：实现简单。在读/写指令之后，加上实现循环检查的一系列指令即可(因此才称为“程序直接控制方式”)</li><li><strong>缺点：CPU和I/O设备只能串行工作，CPU需要一直轮询检查，长期处于“忙等”状态，CPU利用率低</strong>。</li></ul></li></ul></li><li>中断驱动方式<ul><li>完成一次读写操作的流程<ul><li>引入中断机制。由于I/O设备速度很慢，因此在CPU发出读/写命令后，可将等待I/O的进程阻塞，先切换到别的进程执行。</li><li>当I/O完成后，控制器会向CPU发出一个中断信号，CPU检测到中断信号后，会保存当前进程的运行环境信息，转去执行中断处理程序处理该中断。</li><li>处理中断的过程中，CPU从I/O控制器读一个字的数据传送到CPU寄存器，再写入主存。接着，CPU恢复等待I/O的进程（或其他进程）的运行环境，然后继续执行。</li><li>注意<ul><li>①CPU会在每个指令周期的末尾检查中断</li><li>②中断处理过程中需要保存、恢复进程的运行环境，这个过程是需要一定时间开销的。可见，如果中断发生的频率太高，也会降低系统性能。</li></ul></li></ul></li><li>流程图<ul><li><img src="https://s1.ax1x.com/2023/08/25/pPt0Lk9.png" alt="pPt0Lk9.png"></li></ul></li><li>CPU干预的频率<ul><li>每次I/O操作开始之前、完成之后需要CPU介入。</li><li>等待I/O完成的过程中CPU可以切换到别的进程执行。</li></ul></li><li>数据传送的单位：每次读/写一个字</li><li>数据的流向<ul><li>读操作（数据输入）：I/O设备→CPU→内存</li><li>写操作（数据输出）：内存→CPU→I/O设备</li></ul></li><li>主要缺点和主要优点<ul><li>优点<ul><li>与“程序直接控制方式”相比，在“中断驱动方式”中，I/O控制器会通过中断信号主动报告I/O已完成，CPU不再需要不停地轮询。</li><li>CPU和I/O设备可并行工作，CPU利用率得到明显提升。</li></ul></li><li>缺点：每个字在I/O设备与内存之间的传输，都需要经过CPU。而频繁的中断处理会消耗较多的CPU时间。</li></ul></li></ul></li><li>DMA方式（直接存储器存取，外设—内存）<ul><li>特点<ul><li><strong>数据的传送单位是“块”</strong>。不再是一个字、一个字的传送，<strong>适用于磁盘设备</strong></li><li><strong>数据的流向是从设备直接放入内存，或者从内存直接到设备。不再需要CPU作为中介</strong></li><li>仅在传送一个或多个数据块的开始和结束时，才需要CPU干预。<ul><li>CPU指明此次要进行的操作(如：读操作)，并说明要读入多少数据、数据要存放在内存的什么位置，数据在外部设备上的地址(如：在磁盘上的地址)</li><li>控制器会根据CPU提出的要求完成数据的读/写工作，整块数据的传输完成后，才向CPU发出中断信号</li></ul></li></ul></li><li>DMA控制器<ul><li><img src="https://s1.ax1x.com/2023/08/25/pPtBYcV.png" alt="pPtBYcV.png"></li></ul></li><li>CPU干预的频率：<strong>仅在传送一个或多个数据块的开始和结束时，才需要CPU干预</strong>。</li><li>数据传送的单位：每次读/写一个或多个块（注意：<strong>每次读写的只能是连续的多个块，且这些块读入内存后在内存中也必须是连续的</strong>）</li><li>数据的流向  (不再需要经过CPU)<ul><li>读操作（数据输入）：I/O设备→内存</li><li>写操作（数据输出）：内存→I/O设备</li></ul></li><li>主要缺点和主要优点<ul><li>优点：数据传输以“块”为单位，CPU介入频率进一步降低。<br>数据的传输不再需要先经过CPU再写入内存，数据传输效率进一步增加。CPU和I/O设备的并行性得到提升。</li><li>缺点：CPU每发出一条I/O指令，只能读/写一个或多个连续的数据块。<br>如果要读/写多个离散存储的数据块，或者要将数据分别写到不同的内存区域时，CPU要分别发出多条I/O指令，进行多次中断处理才能完成。</li></ul></li></ul></li><li>通道控制方式（<strong>硬件</strong>）<ul><li>通道是一种硬件，与CPU相比，通道可以执行的指令很单一，并且通道程序是放在主机内存中的，也就是说通道与CPU共享内存</li><li>完成一次读写操作的流程<ul><li><img src="https://s1.ax1x.com/2023/08/25/pPtDiuT.png" alt="pPtDiuT.png"></li></ul></li><li>CPU干预的频率<strong>：极低，通道会根据CPU的指示执行相应的通道程序，只有完成一组数据块的读/写后才需要发出中断信号，请求CPU干预。</strong></li><li>数据传送的单位：每次读/写一组数据块</li><li>数据的流向（在通道的控制下进行）<ul><li>读操作（数据输入）：I/O设备→内存</li><li>写操作（数据输出）：内存→I/O设备</li></ul></li><li>主要缺点和主要优点<ul><li>缺点：实现复杂，需要专门的通道硬件支持</li><li>优点：CPU、通道、I/O设备可并行作，资源利用率很高</li></ul></li></ul></li></ul><h4 id="3-I-O软件层次结构"><a href="#3-I-O软件层次结构" class="headerlink" title="3.I/O软件层次结构"></a>3.I/O软件层次结构</h4><ul><li>概览<ul><li>分为四层，越上层越接近用户，下层为上层提供服务，屏蔽实现的具体细节，越下层越接近硬件</li><li>中间的三层处于内核部分，称为I/O（核心子系统）系统</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/25/pPtD1bD.png" alt="pPtD1bD.png"></li></ul></li></ul></li><li>I/O操作的流程：用户程序→系统调用处理程序→设备驱动程序→中断处理程序。</li><li>用户层I/O软件<ul><li>用户层软件实现了与用户交互的接口，用户可直接使用该层提供的、与I/O操作相关的库函数对设备进行操作</li><li><strong>用户层软件将用户请求翻译成格式化的I/O请求，并通过“系统调用”请求操作系统内核的服务</strong></li></ul></li><li>设备独立性软件<ul><li><strong>设备独立性软件，又称设备无关性软件</strong>。与设备的硬件特性无关的功能几乎都在这一层实现。<br><strong>处理上层的系统调用参数</strong></li><li>功能<ul><li>①向上层提供统一的调用接口(如read/write系统调用) </li><li>②设备的保护</li><li>③差错控制</li><li><strong>④设备的分配与回收</strong></li><li><strong>⑤数据缓冲区管理</strong></li><li>⑥<strong>建立逻辑设备名到物理设备名的映射关系；</strong><ul><li>根据设备类型选择调用相应的驱动程序用户或用户层软件发出I/O操作相关系统调用的系统调用时，需要指明此次要操作的I/O设备的逻辑设备名，不同类型的I/O设备需要有不同的驱动程序处理</li><li>设备独立性软件需要通过逻辑设备表来确定逻辑设备对应的物理设备，并找到该设备对应的设备驱动程序</li></ul></li><li>管理逻辑设备表的（LUT）方式<ul><li>第一种方式，整个系统只设置一张LUT，这就意味着所有用户不能使用相同的逻辑设备名，因此这种方式只适用于单用户操作系统。</li><li>第二种方式，为每个用户设置一张ULT，各个用户使用的逻辑设备名可以重复，适用于多用户操作系统。<br>系统会在用户登录时为其建立一个用户管理进程，而LUT就存放在用户管理进程的PCB中。</li></ul></li></ul></li></ul></li><li>设备驱动程序（<strong>驱动程序与操作系统无关</strong>）<ul><li>不同的I/O设备有不同的硬件特性，具体细节只有设备的厂家才知道，因此厂家需要根据设备的硬件特性设计并提供相应的驱动程序，驱动程序一般以一个独立的进程存在</li><li><strong>负责对硬件设备的具体控制，将上层发出的一系列命令（如read/write)转化成特定设备“能听得懂”的一系列操作。</strong><br><strong>包括设置设备寄存器，检查设备状态等</strong></li><li><strong>计算数据所在磁盘的柱面号、磁头号、扇区号</strong></li></ul></li><li>中断处理程序<ul><li><strong>当I/O任务完成时，I/O控制器会发送一个中断信号，系统会根据中断信号类型找到相应的中断处理程序并执行。</strong></li></ul></li></ul><h4 id="4-应用程序I-O接口"><a href="#4-应用程序I-O接口" class="headerlink" title="4.应用程序I/O接口"></a>4.应用程序I/O接口</h4><ul><li>输入/输出应用程序接口<ul><li>包括：字符设备接口、块设备接口、网络设备接口(网络套接字)<ul><li><img src="https://s1.ax1x.com/2023/08/25/pPtgkb6.png" alt="pPtgkb6.png"></li></ul></li><li>阻塞I/O与非阻塞I/O<ul><li>阻塞I/O：应用程序发出I/O系统调用，进程需转为阻塞态等待。<br>如字符设备接口一一从键盘读一个字符get</li><li>非阻塞I/O：应用程序发出I/O系统调用，系统调用可迅速返回，进程无需阻塞等待。<br>如块设备接口一一往磁盘写数据write</li></ul></li></ul></li><li>设备驱动程序接口（<strong>驱动程序只与厂商有关，与操作系统无关</strong>）<ul><li>不同的操作系统，对设备驱动程序接口的标准各不相同。</li><li>设备厂商必须根据操作系统的接口要求，开发相应的设备驱动程序，设备才能被使用</li></ul></li></ul><h4 id="5-提高磁盘I-O速度的方法"><a href="#5-提高磁盘I-O速度的方法" class="headerlink" title="5.提高磁盘I/O速度的方法"></a>5.提高磁盘I/O速度的方法</h4><ul><li>提前读。在读磁盘当前块时，把下一磁盘块也读入内存缓冲区。</li><li>延迟写。仅在缓冲区首部设置延迟写标志，然后释放此缓冲区并将其链入空闲缓冲区链表的尾部，<br>当其他进程申请到此缓冲区时，才真正把缓冲区信息写入磁盘块。</li><li>虚拟盘。是指用内存空间去仿真磁盘，又叫RAM盘。虚拟盘是一种易失性存储器。虚拟盘常用于存放临时文件。</li></ul><h3 id="二-设备独立性软件（✠）"><a href="#二-设备独立性软件（✠）" class="headerlink" title="二.设备独立性软件（✠）"></a>二.设备独立性软件（✠）</h3><h4 id="1-假脱机（SPOOLing）技术"><a href="#1-假脱机（SPOOLing）技术" class="headerlink" title="1.假脱机（SPOOLing）技术"></a>1.假脱机（SPOOLing）技术</h4><ul><li>脱机技术<ul><li>在外围控制机的控制下，慢速输入设备的数据先被输入到更快速的磁带上<br>之后主机可以从快速的磁带上读入数据，从而缓解了速度矛盾</li><li><strong>引入脱机技术后，缓解了CPU与慢速I/O设备的速度矛盾。另一方面，即使CPU在忙碌，也可以提前将数据输入到磁带；</strong><br><strong>即使慢速的输出设备正在忙碌，也可以提前将数据输出到磁带</strong>。</li></ul></li><li>假脱机技术（<strong>用软件的方式模拟脱机技术</strong>）<ul><li>输入井和输出井<ul><li><strong>在磁盘上开辟出两个存储区域一一“输入井”和“输出井”</strong></li><li>“输入井”模拟脱机输入时的磁带，用于收容I/O设备输入的数据</li><li>“输出井”模拟脱机输出时的磁带，用于收容用户进程输出的数据</li></ul></li><li>输入进程和输出进程<ul><li><strong>要实现SPOOLing技术，必须要有多道程序技术的支持。系统会建立“输入进程”和“输出进程”</strong>。</li><li>输入进程模拟脱机输入时的外围控制机</li><li>输出进程模拟脱机输出时的外围控制机</li></ul></li><li>输入缓冲区和输出缓冲区<ul><li><strong>两个缓冲区都是在内存中的</strong></li><li>在输入进程的控制下，“输入缓冲区”用于暂存从输入设备输入的数据，之后再转存到输入井中</li><li>在输出进程的控制下，“输出缓冲区”用于暂存从输出井送来的数据，之后再传送到输出设备上</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/25/pPtWvLR.png" alt="pPtWvLR.png"></li></ul></li></ul></li><li>共享打印机<ul><li>独占式设备一一只允许各个进程串行使用的设备。一段时间内只能满足一个进程的请求。</li><li>共享设备一一允许多个进程“同时”使用的设备（宏观上同时使用，微观上可能是交替使用）可以同时满足多个进程的使用请求。</li><li><strong>打印机是种“独占式设备”，SPOOLing技术可以把一台物理设备虚拟成逻辑上的多台设备，可将打印机变为共享设备</strong><ul><li><strong>虽然系统中只有一个台打印机，但每个进程提出打印请求时，系统都会为在输出井中为其分配一个存储区（外存中）；</strong><br><strong>相当于分配了一个逻辑设备，使每个用户进程都觉得自己在独占一台打印机，从而实现对打印机的共享。</strong></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/25/pPtf3lQ.png" alt="pPtf3lQ.png"></li></ul></li></ul></li></ul></li></ul><h4 id="2-设备的分配与回收"><a href="#2-设备的分配与回收" class="headerlink" title="2.设备的分配与回收"></a>2.设备的分配与回收</h4><ul><li><strong>设备独立性是指用户在编程序时使用的设备与实际设备无关。一个程序应独立于分配给它的某类设备的具体设备，</strong><br><strong>即在用户程序中只指明I/O使用的设备类型即可。</strong></li><li>设备分配时应考虑的因素<ul><li>设备的固有属性<ul><li>独占设备：一个时段只能分配给一个进程（如打印机）</li><li>共享设备：可同时分配给多个进程使用（如磁盘），各进程往往是宏观上同时共享使用设备，而微观上交替使用。</li><li><strong>虚拟设备：采用SPOOLing技术将独占设备改造成虚拟的共享设备，可同时分配给多个进程使用</strong><br><strong>(如采用SPOOLing技术实现的共享打印机)</strong></li></ul></li><li>设备分配算法<ul><li>先来先服务、优先级高者优先、短任务优先等算法</li></ul></li><li>设备分配中的安全性<ul><li>安全分配方式：为进程分配一个设备后就将进程阻塞，本次I/O完成后才将进程唤醒。(考虑进程请求打印机打印输出的例子)一个时段内每个进程只能使用一个设备<ul><li>优点：破坏了“请求和保持”条件，不会死锁</li><li>缺点：对于一个进程来说，CPU和/O设备只能串行工作</li></ul></li><li>不安全分配方式：进程发出I/O请求后，系统为其分配I/O设备，进程可继续执行，之后还可以发出新的I/O请求。只有某I/O请求得不到满足时才将进程阻塞。一个进程可以同时使用多个设备<ul><li>优点：进程的计算任务和I/O任务可以并行处理，使进程迅速推进</li><li>缺点：有可能发生死锁（死锁避免、死锁的检测和解除)</li></ul></li></ul></li></ul></li><li>静态分配与动态分配<ul><li>静态分配：进程运行前为其分配全部所需资源，运行结束后归还资源，破坏了“请求和保持”条件，不会发生死锁</li><li>动态分配：进程运行过程中动态申请设备资源</li></ul></li><li>设备分配管理中的数据结构<ul><li>一个通道可控制多个设备控制器，每个设备控制器可控制多个设备。</li><li>设备控制表(DCT)：系统为每个设备配置一张DCT，用于记录设备情况<ul><li><strong>设备类型（逻辑设备名）：如打印机/扫描仪/键盘</strong></li><li><strong>设备标识符：即物理设备名，系统中的每个设备的物理设备名唯一</strong></li><li>设备状态：忙碌/空闲/故障</li><li><strong>指向控制器表的指针：每个设备由一个控制器控制，该指针可找到相应控制器的信息</strong></li><li>重复执行次数或时间：当重复执行多次I/O操作后仍不成功，才认为此次I/O失败</li><li>设备队列的队首指针：指向正在等待该设备的进程队列(由进程PCB组成队列)<br>系统会根据阻塞原因不同，将进程PCB挂到不同的阻塞队列中</li></ul></li><li>控制器控制表(COCT)：每个设备控制器都会对应一张COCT。操作系统根据COCT的信息对控制器进行操作和管理。<ul><li><strong>控制器标识符：各个控制器的唯一ID</strong></li><li>控制器状态：忙碌/空闲/故障</li><li><strong>指向通道表的指针：每个控制器由一个通道控制，该指针可找到相应通道的信息</strong></li><li>控制器队列的队首指针</li><li>控制器队列的队尾指针：指向正在等待该控制器的进程队列(由进程PCB组成队列)</li></ul></li><li>通道控制表(CHCT)：每个通道都会对应一张CHCT。操作系统根据CHCT的信息对通道进行操作和管理<ul><li>通道标识符：各个通道的唯一ID</li><li>通道状态：忙碌/空闲/故障</li><li>与通道连接的控制器表首址：可通过该指针找到该通道管理的所有控制器相关信息(COCT)</li><li>通道队列的队首指针</li><li>通道队列的队尾指针：指向正在等待该通道的进程队列(由进程PCB组成队列)</li></ul></li><li><strong>系统设备表(SDT)：记录了系统中全部设备的情况，每个设备对应一个表目。</strong></li></ul></li><li>设备分配的步骤<ul><li>步骤<ul><li>①根据进程请求的物理设备名查找SDT(注：物理设备名是进程请求分配设备时提供的参数)</li><li>②根据SDT找到DCT，若设备忙碌则将进程PCB挂到设备等待队列中，不忙碌则将设备分配给进程。</li><li>③根据DCT找到COCT，若控制器忙碌则将进程PCB挂到控制器等待队列中，不忙碌则将控制器分配给进程。</li><li>④根据COCT找到CHCT，若通道忙碌则将进程PCB挂到通道等待队列中，不忙碌则将通道分配给进程。</li></ul></li><li>注：只有设备、控制器、通道三者都分配成功时，这次设备分配才算成功，之后便可启动I/O设备进行数据传送</li><li>缺点<ul><li>①用户编程时必须使用“物理设备名”，底层细节对用户不透明，不方便编程</li><li>②若换了一个物理设备，则程序无法运行</li><li>③若进程请求的物理设备正在忙碌，则即使系统中还有同类型的设备，进程也必须阻塞等待</li></ul></li><li><strong>改进方法：建立逻辑设备名与物理设备名的映射机制，用户编程时只需提供逻辑设备名</strong></li></ul></li><li><p>设备分配的改进</p><ul><li><p>增加设备的独立性并考虑多通路情况</p><ul><li><p>增加设备的独立性</p><ul><li>进程使用逻辑设备名请求I/O。这样，系统首先从SDT中找出第一个该类设备的DCT。</li><li>若该设备忙，则又查找第二个该类设备的DCT。仅当所有该类设备都忙时，才把进程挂到该类设备的等待队列上</li><li>只要有一个该类设备可用，系统便进一步计算分配该设备的安全性。</li></ul></li><li><p>考虑多通路情况</p><ul><li>为防止I/O系统的“瓶颈”现象，通常采用多通路的I/O系统结构。此时对控制器和通道的分配同样要经过几次反复，<br>即若设备（控制器）所连接的第一个控制器（通道）忙时，则应查看其所连接的第二个控制器（通道）</li><li>仅当所有控制器（通道）都忙时，此次的控制器（通道）分配才算失败，才把进程挂到控制器（通道）的等待队列上<br>而只要有一个控制器（通道）可用，系统便可将它分配给进程。</li><li>设备分配过程中，先后分别访问的数据结构为SDT→DCT→COCT→CHCT。要成功分配一个设备，必须要：<br>①设备可用；②控制器可用；③通道可用。所以，“设备分配，要过三关”。</li></ul></li></ul></li><li><p>逻辑设备表(LUT)：<strong>建立了逻辑设备名与物理设备名之间的映射关系</strong></p><ul><li>某用户进程第一次使用设备时使用逻辑设备名向操作系统发出请求，操作系统根据用户进程指定的设备类型（逻辑设备名）查找系统设备表，找到一个空闲设备分配给进程，并在LUT中增加相应表项。</li><li>如果之后用户进程再次通过相同的逻辑设备名请求使用设备，则操作系统通过LUT表即可知道用户进程实际要使用的是哪个物理设备了，并且也能知道该设备的驱动程序入口地址。</li><li>此时不仅可以通过物理设备名查找物理设备，还可以通过逻辑设备名访问物理设备</li><li>此时更改物理设备后不用修改访问改设备的应用程序</li></ul></li><li>设备分配的步骤<ul><li>①根据进程请求的逻辑设备名查找SDT(注：用户编程时提供的逻辑设备名其实就是“设备类型”)</li><li><strong>②查找SDT，找到用户进程指定类型的、并且空闲的设备，将其分配给该进程。操作系统在逻辑设备表(LUT)中新增一个表项。</strong></li><li>③根据DCT找到COCT，若控制器忙碌则将进程PCB挂到控制器等待队列中，不忙碌则将控制器分配给进程。</li><li>④根据COCT找到CHCT，若通道忙碌则将进程PCB挂到通道等待队列中，不忙碌则将通道分配给进程。</li></ul></li><li>逻辑设备表的设置问题<ul><li>整个系统只有一张LUT：各用户所用的逻辑设备名不允许重复，适用于单用户操作系统</li><li>每个用户一张LUT：不同用户的逻辑设备名可重复，适用于多用户操作系统</li></ul></li></ul></li></ul><h4 id="3-高速缓存与缓冲区"><a href="#3-高速缓存与缓冲区" class="headerlink" title="3.高速缓存与缓冲区"></a>3.高速缓存与缓冲区</h4><ul><li><p>高速缓存</p><ul><li>操作系统中使用磁盘高速缓存技术来提高磁盘的I/O速度<br>利用内存中的存储空间来暂存从磁盘中读出的一系列盘块中</li><li><strong>磁盘高速缓存逻辑上属于磁盘，物理上则是驻留在内存中的盘块。</strong></li><li>高速缓存在内存中分为两种形式<ul><li>一种是在内存中开辟一个单独的空间作为磁盘高速缓存，大小固定</li><li>另一种是把未利用的内存空间作为一个缓冲池，供请求分页系统和磁盘I/O时共享。</li></ul></li></ul></li><li><p>缓冲区的定义</p><ul><li>缓冲区是一个存储区域，可以由专门的硬件寄存器组成，也可利用内存作为缓冲区。</li><li>使用硬件作为缓冲区的成本较高，容量也较小，一般仅用在对速度要求非常高的场合；<br>如存储器管理中所用的联想寄存器（快表），由于对页表的访问频率极高，因此使用速度很快的联想寄存器来存放页表项的副本</li><li>一般情况下，更多的是利用内存作为缓冲区，“设备独立性软件”的缓冲区管理就是要组织管理好这些缓冲区</li></ul></li><li>缓冲区的作用<ul><li>缓和CPU与I/O设备之间速度不匹配的矛盾<ul><li><img src="https://s1.ax1x.com/2023/08/25/pPtI6gO.png" alt="pPtI6gO.png"></li></ul></li><li>减少对CPU的中断频率，放宽对CPU中断响应时间的限制<ul><li>如果是字符型设备则每输出完一个字符就要向CPU发送一次中断信号</li></ul></li><li>解决数据粒度不匹配的问题<ul><li>输出进程每次可以生成一块数据但I/O设备每次只能输出一个字符</li></ul></li><li>提高CPU与I/O设备之间的并行性</li></ul></li><li>单缓冲<ul><li>操作系统会在主存中为其分配一个缓冲区，用户进程的内存空间中，会分出一片工作区来接受输入/输出数据（一般也默认工作区大小与缓冲区相同）</li><li>当缓冲区数据非空时，不能往缓冲区冲入数据，只能从缓冲区把数据传出；</li><li>当缓冲区为空时，可以往缓冲区冲入数据，但必须把缓冲区充满以后，才能从缓冲区把数据传出。</li><li><strong>分析问题的初始状态：工作区满；缓冲区空处理一块数据的平均耗时时间：Max(C,T)+M</strong><ul><li><img src="https://s1.ax1x.com/2023/08/25/pPtHKzV.png" alt="pPtHKzV.png"></li></ul></li></ul></li><li>双缓冲<ul><li><strong>分析问题的初始状态：工作区空，一个缓冲区满，另一个缓冲区空；处理一块数据平均耗时Max(T,C+M)</strong></li></ul></li><li>单缓冲与双缓冲的区别<ul><li>两台机器之间通信时，可以配置缓冲区用于数据的发送和接受。</li><li>若两个相互通信的机器只设置单缓冲区，在任一时刻只能实现数据的单向传输。</li><li>若两个相互通信的机器设置双缓冲区，则同一时刻可以实现双向的数据传输。</li></ul></li><li>例题<ul><li>例1<ul><li>选B<img src="https://s1.ax1x.com/2023/08/25/pPN9BOs.png" alt="pPN9BOs.png"></li><li><img src="https://s1.ax1x.com/2023/08/25/pPN9ckV.png" alt="pPN9ckV.png"></li></ul></li><li>例2<ul><li>选C<img src="https://s1.ax1x.com/2023/08/25/pPN9fl4.png" alt="pPN9fl4.png"></li></ul></li></ul></li></ul><h3 id="三-磁盘与固态硬盘（✪）"><a href="#三-磁盘与固态硬盘（✪）" class="headerlink" title="三.磁盘与固态硬盘（✪）"></a>三.磁盘与固态硬盘（✪）</h3><h4 id="1-磁盘的结构"><a href="#1-磁盘的结构" class="headerlink" title="1.磁盘的结构"></a>1.磁盘的结构</h4><ul><li>磁盘、磁道、扇区的概念<ul><li>磁盘由表面涂有磁性物质的圆形盘片组成</li><li>每个盘片被划分为一个个磁道，每个磁道又划分为一个个扇区</li><li>每个扇区就是一个磁盘块”。各个扇区存放的数据量相同(如1KB)</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/26/pPN3pHx.png" alt="pPN3pHx.png"></li></ul></li></ul></li><li>如何在磁盘中读/写数据<ul><li>磁头移动到目标位置，盘片旋转，对应扇区划过磁道才能完成读/写（机械操作）</li><li>存储一个文件时，当一个磁道存储不下时，选择同一个柱面的不同盘面进行存储，此时可以避免磁臂移动，减少了处理时间</li></ul></li><li>盘面、柱面的概念<ul><li>磁盘有多个盘片“摞”起来，每个盘片有两个盘面</li><li>所有盘面中相对位置相同的磁道组成柱面</li></ul></li><li>磁盘的物理地址<ul><li><strong>可用(柱面号，盘面号，扇区号)来定位任意一个“磁盘块”。</strong></li><li>①根据“柱面号”移动磁臂，让磁头指向指定柱面</li><li>②激活指定盘面对应的磁头；</li><li>③磁盘旋转的过程中，指定的扇区会从磁头下面划过，这样就完成了对指定扇区的读/写。</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/26/pPNutDf.png" alt="pPNutDf.png"></li></ul></li></ul></li><li>磁盘的分类<ul><li>根据磁头是否可移动<ul><li>固定头磁盘（每个磁道有一个磁头，则一个盘面有多个磁头）</li><li>移动头磁盘（每个盘面只有一个磁头）</li></ul></li><li>根据盘片是否可更换：固定盘磁盘可换盘磁盘</li></ul></li><li>簇/块：操作系统限制的存储空间分配基本单位<br>簇：windows系统的说法；块：Linux、Uniⅸ系统的说法<ul><li>本题需为簇的整数倍，选2048B<img src="https://s1.ax1x.com/2023/08/26/pPNDsk4.png" alt="pPNDsk4.png"></li></ul></li></ul><h4 id="2-磁盘的管理"><a href="#2-磁盘的管理" class="headerlink" title="2.磁盘的管理"></a>2.磁盘的管理</h4><ul><li>磁盘初始化<ul><li>Step1：进行低级格式化（物理格式化）：将磁盘的各个磁道划分为扇区。一个扇区通常可分为头、数据区域（如512B大小）、尾三个部分组成。<ul><li>管理扇区所需要的各种数据结构一般存放在头、尾两个部分，包括扇区校验码，如奇偶校验、CRC循环冗余校验码等，校验码用于校验扇区中的数据是否发生错误</li></ul></li><li>Step2：将磁盘分区，每个分区由若干柱面组成（即分为我们熟悉的C盘、D盘、E盘）</li><li>Step3：进行逻辑格式化（高级格式化），创建文件系统。包括创建文件系统的根目录、初始化存储空间管理所用的数据结构（如位示图、空闲分区表）、产生引导扇区</li></ul></li><li>引导块<ul><li>计算机启动时需要运行初始化程序（自举程序）来完成初始化</li><li>ROM中存放很小的自举装入程序，ROM(只读存储器）中的数据在出厂时就写入了，并且以后不能再修改</li><li>完整的自举程序存放在初始块（引导块/启动分区）上，启动分区位于磁盘的固定位置，拥有启动分区的磁盘为系统磁盘或启动磁盘</li><li>开机时计算机先运行“自举装入程序”，通过执行该程序就可找到引导块，并将完整的“自举程序”读入内存，完成初始化</li></ul></li><li>坏块管理<ul><li>简单的磁盘：逻辑格式化时对整个磁盘进行坏块检查，将坏块标记出来；<br>如在FAT表上标明。（在这种方式中，坏块对操作系统不透明）</li><li>复杂的磁盘：磁盘控制器维护一个坏块链，在磁盘出厂前进行低级格式化（物理格式化）时就将坏块链进行初始化<br>并会保留一些“备用扇区”，用于替换坏块。这种方案称为扇区备用。且这种处理方式中，坏块对操作系统透明。</li></ul></li></ul><h4 id="3-磁盘调度算法（♚）"><a href="#3-磁盘调度算法（♚）" class="headerlink" title="3.磁盘调度算法（♚）"></a>3.磁盘调度算法（♚）</h4><ul><li>一次读/写操作需要的时间<ul><li>寻道（找）时间${\mathrm{T}_{\mathrm{S}}}$: 在读/写数据前, <strong>将磁头移动到指定磁道所花的时间</strong>。<ul><li>启动磁头臂是需要时间的。假设耗时为${s}$;</li><li><strong>移动磁头也是需要时间的。假设磁头匀速移动, 每跨越一 个磁道耗时为${m}$, 总共需要跨越${n}$条磁道。则:</strong><br><strong>寻道时间${T_{s}=s+m * n}$</strong></li><li>现在的硬盘移动一个磁道大约需要0.2ms,磁臂启动时间约为2ms</li><li>寻道时间在所有时间中，寻道时间需要移动磁臂，所占用的时间最长</li></ul></li><li>延迟时间${\mathrm{T}_{\mathrm{R}}}$: 通过旋转磁盘, <strong>使磁头定位到目标扇区所需要的时间</strong>。<ul><li><strong>设磁盘转速为${r}$(单位: 转/秒, 或转/分), 则平均所需的延迟时间${T_{R}=(1 / 2) *(1 / r)=1 / 2 r}$</strong></li><li>1/r就是转一圈需要的时间。找到目标扇区平均需要转半圈，因此再乘以1/2</li><li>硬盘的典型转速为5400转/分，或7200转/分</li></ul></li><li>传输时间${\mathrm{T}_{\mathrm{t}}}$: <strong>从磁盘读出或向磁盘写入数据所经历的时间</strong><ul><li><strong>设磁盘转速为${r}$，此次读/写的字节数为${b}$，每个磁道上的字节数为${\mathrm{N}}$。则传输时间${T_{t}=(1 / r) *(b / N)=b /(r N)}$</strong></li><li>每个磁道要可存N字节的数据，因此b字节的数据需要b/N个磁道才能存储。而读/写一个磁道所需的时间刚好又是转一圈所需要的时间1/r</li></ul></li><li>注意事项<ul><li><strong>寻道时间主要受磁盘调度算法的影响</strong></li><li><strong>延迟时间受到磁盘空闲空间分配程序以及文件的物理结构的影响</strong></li><li><strong>扇区数据的处理时间对旋转延迟有影响但是影响不大。</strong></li></ul></li></ul></li><li>磁盘调度算法（最优化传输时间）<ul><li>先来先服务(FCFS)<ul><li>根据进程请求访问磁盘的先后顺序进行调度。</li><li>优点：公平；如果请求访问的磁道比较集中的话，算法性能还算过的去</li><li>缺点：如果有大量进程竞争使用磁盘，请求访问的破道很分散，则FCFS在性能上很差，寻道时间长。</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/26/pPNQCE8.png" alt="pPNQCE8.png"></li></ul></li></ul></li><li>最短寻找时间优先(SSTF)<ul><li>SSTF算法会优先处理的磁道是与当前磁头最近的磁道。可以保证每次的寻道时间最短，但是并不能保证总的寻道时间最短。(其实就是贪心算法的思想，只是选择眼前最优，但是总体未必最优)</li><li>优点：性能较好，平均寻道时间短</li><li>缺点：<strong>可能产生“饥饿”现象</strong>，磁头有可能在一个小区域内来回来去地移动</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/26/pPNQNb6.png" alt="pPNQNb6.png"></li></ul></li></ul></li><li>扫描算法(SCAN)<ul><li>解决饥饿问题：只有磁头移动到最外侧磁道的时候才能往内移动，移动到最内侧磁道的时候才能往外移动。这就是扫描算法(SCAN)的思想。由于磁头移动的方式很像电梯，因此也叫<strong>电梯算法</strong>。（<strong>移动时，只有到了最边上的磁道才能改变磁头移动方向</strong>）</li><li><strong>优点：性能较好，平均寻道时间较短，不会产生饥饿现象</strong></li><li>缺点<ul><li><strong>①只有到达最边上的磁道时才能改变磁头移动方向；</strong><br><strong>事实上，处理了184号磁道的访问请求之后就不需要再往右移动磁头了。</strong></li><li><strong>②SCAN算法对于各个位置磁道的响应频率不平均</strong></li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/26/pPNlkdK.png" alt="pPNlkdK.png"></li></ul></li></ul></li><li>LOOK调度算法<ul><li><strong>还是要向一个边上移动，但是不用移动到底</strong>，如果在磁头移动方向上已经<strong>没有别的请求，就可以立即改变磁头移动方向</strong>。(边移动边观察，因此叫LOOK)，解决扫描算法非要移动到最边上的问题</li><li>优点：比起SCAN算法来，不需要每次都移动到最外侧或最内侧才改变磁头方向，使寻道时间进一步缩短</li><li>缺点：未解决响应不均衡的问题</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/26/pPNl2l9.png" alt="pPNl2l9.png"></li></ul></li></ul></li><li>C-SCAN算法<ul><li>规定只有磁头朝某个特定方向移动时才处理磁道访问请求，而返回时<strong>直接快速移动至起始端</strong>而不处理任何请求。<br>解决对于各个位置磁道的响应频率不平均</li><li>优点：比起SCAN来，对于各个位置磁道的响应频率很平均。</li><li>缺点：只有到达最边上的磁道时才能改变磁头移动方向，另外，比起SCAN算法来，平均寻道时间更长。</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/26/pPNl5TK.png" alt="pPNl5TK.png"></li></ul></li></ul></li><li>C-LOOK算法<ul><li>如果磁头移动的方向上已经没有磁道访问请求了，就可以立即让磁头返回，磁头只需要返回到<strong>最靠近边缘的并且需要访问的磁道上即可</strong></li><li>优点：比起C-SCAN算法来，不需要每次都移动到最外侧或最内侧才改变磁头方向，使寻道时间进一步缩短</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/26/pPNlbSH.png" alt="pPNlbSH.png"></li></ul></li></ul></li></ul></li><li>减少延迟时间的方法<ul><li>关于延迟时间的产生<ul><li><strong>假设要连续读取几个相邻扇区：磁头读取一块的内容后，需要一小段时间处理，而盘片又在不停地旋转</strong><br>因此，如果扇区相邻着排列，则读完1号扇区后无法连续不断地读入2号扇区，必须等盘片继续旋转，2号扇区再次划过磁头，才能完成扇区读入</li><li>结论：<strong>磁头读入一个扇区数据后需要一小段时间处理，如果逻辑上相邻的扇区在物理上也相邻，则读入几个连续的逻辑扇区，可能需要很长的延迟时间</strong></li></ul></li><li>磁盘地址结构的设计<ul><li>为什么？磁盘的物理地址是(柱面号，盘面号，扇区号)，而不是(盘面号，柱面号，扇区号)</li><li>答：读取地址连续的磁盘块时，采用（柱面号盘面号，扇区号）的地址结构可以<strong>减少磁头移动消耗的时间</strong></li></ul></li><li>两种方法（使<strong>读取连续的逻辑扇区</strong>所需要的延迟时间更小）<ul><li>交替编号：让编号相邻的扇区在物理上不相邻</li><li>错位命名：让相邻盘面的扇区编号“错位”</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/26/pPNJSDs.png" alt="pPNJSDs.png"></li></ul></li></ul></li></ul></li><li><p>例题</p><ul><li><p>例1</p><ul><li><img src="https://s1.ax1x.com/2023/08/26/pPNDMOf.png" alt="pPNDMOf.png"></li></ul></li><li><p>例2</p><ul><li>此时转1圈需要60/3000秒，每个扇区的时间再除以10，选C<br><img src="https://s1.ax1x.com/2023/08/26/pPNDGkQ.png" alt="pPNDGkQ.png"></li></ul></li></ul></li></ul><h4 id="4-固态磁盘（SSD）"><a href="#4-固态磁盘（SSD）" class="headerlink" title="4.固态磁盘（SSD）"></a>4.固态磁盘（SSD）</h4><ul><li>原理：基于闪存技术Flash Memory，属于电可擦除ROM，即EEPROM</li><li>组成<ul><li>闪存翻译层：负责翻译逻辑块号，找到对应页(Page)</li><li>存储介质：包含多个闪存芯片(Flash Chip)，每个芯片包含多个块(block)，每个块包含多个页(page)</li></ul></li><li>读写性能特征<ul><li><strong>以页(page)为单位读/写：相当于磁盘的”扇区”</strong></li><li><strong>以块(bock)为单位”擦除”：擦干净的块，其中的每页都可以写一次，读无限次</strong></li><li>支持随机访问，系统给定一个逻辑地址，闪存翻译层可通过电路迅速定位到对应的物理地址</li><li><strong>读快、写慢。要写的页如果有数据，则不能写入，需要将块内其他页全部复制到一个新的（擦除过的）块中，再写入新的页</strong></li></ul></li><li>与机械硬盘相比的特点<ul><li>SSD读写速度快，<strong>随机访问性能高</strong>，用电路控制访问位置；<br>机械硬盘通过移动磁臂旋转磁盘控制访问位置，有寻道时间和旋转延迟</li><li>SSD安静无噪音、耐摔抗震、能耗低、造价更贵</li><li><strong>SSD的一个”块”被擦除次数过多（重复写同一个块）可能会坏掉，而机械硬盘的扇区不会因为写的次数太多而坏掉</strong></li></ul></li><li>磨损均衡技术<ul><li>思想：将“擦除”平均分布在各个块上，以提升使用寿命</li><li>动态磨损均衡：写入数据时，优先选择累计擦除次数少的新闪存块</li><li>静态磨损均衡：SSD监测并自动进行数据分配、迁移，让老旧的闪存块承担以读为主的储存任务，让较新的闪存块承担更多的写任务（<strong>更优秀</strong>）</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/26/pPNM9fJ.png" alt="pPNM9fJ.png"></li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;操作系统第五章-输入-输出（I-O）管理&quot;&gt;&lt;a href=&quot;#操作系统第五章-输入-输出（I-O）管理&quot; class=&quot;headerlink&quot; title=&quot;操作系统第五章 输入/输出（I/O）管理&quot;&gt;&lt;/a&gt;操作系统第五章 输入/输出（I/O）管理&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;计算机学科基础：操作系统第五章输入/输出(I/O)管理的学习笔记&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="计算机基础学习笔记" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="操作系统" scheme="http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>操作系统第四章-文件管理</title>
    <link href="http://example.com/2024/08/12/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/"/>
    <id>http://example.com/2024/08/12/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/</id>
    <published>2024-08-11T17:33:29.000Z</published>
    <updated>2024-08-12T03:28:31.006Z</updated>
    
    <content type="html"><![CDATA[<h2 id="操作系统第四章-文件管理"><a href="#操作系统第四章-文件管理" class="headerlink" title="操作系统第四章 文件管理"></a>操作系统第四章 文件管理</h2><blockquote><p>计算机学科基础：操作系统第四章文件管理的学习笔记</p></blockquote><span id="more"></span><h3 id="一-文件系统基础（✪）"><a href="#一-文件系统基础（✪）" class="headerlink" title="一.文件系统基础（✪）"></a>一.文件系统基础（✪）</h3><h4 id="1-文件控制块（FCB）和索引结点"><a href="#1-文件控制块（FCB）和索引结点" class="headerlink" title="1.文件控制块（FCB）和索引结点"></a>1.文件控制块（FCB）和索引结点</h4><ul><li><p>文件的概念：以硬盘为载体的存储在计算机上的信息集合</p></li><li><p>FCB（目录项）</p><ul><li><p>定义：一个文件对应一个FCB，一个FCB就是一个目录项；文件系统在创建文件时，建立一个文件目录项；多个FCB组成文件目录</p></li><li><p>FCB的组成</p><ul><li>文件的基本信息（文件名、物理地址、逻辑结构、物理结构等）<ul><li>FCB实现了文件名和文件之间的映射。使用户（用户程序）可以实现“按名存取“</li></ul></li><li>存取控制信息（是否可读/可写、禁止访问的用户名单等）</li><li>使用信息(如文件的建立时间、修改时间等)</li></ul></li></ul></li><li><p>索引结点（FCB的瘦身策略，iNode）</p><ul><li><strong>除了文件名之外的所有信息都放到索引结点中，每个文件对应一个索引结点，简称i结点</strong><br><strong>目录项中只包含文件名、索引结点指针，因此每个目录项的长度大幅减小</strong></li><li><strong>由于目录项长度减小，因此每个磁盘块可以存放更多个目录项，因此检索文件时磁盘I/O的次数就少了很多</strong></li><li><strong>当找到文件名对应的目录项时，才需要将索引结点调入内存，索引结点中记录了文件的各种信息，包括文件在外存中的存放位置，根据“存放位置”即可找到文件。</strong></li><li><strong>存放在外存中的索引结点称为“磁盘索引结点”，当索引结点放入内存后称为“内存/活动索引结点”</strong><br>相比之下内存索引结点中需要增加一些信息，比如：文件是否被修改、此时有几个进程正在访问该文件</li><li>文件的打开过程描述<ul><li>①检索目录，要求打开的文件应该是已经创建的文件，它应登记在文件目录中，否则会出错。<br>在检索到指定文件后，就将其磁盘Node复制到活动iNode表中。</li><li>②把参数mode所给出的打开方式与活动iNode中在创建文件时所记录的文件访问权限相比较，如果合法，则此次打开操作成功。</li><li>③当打开合法时，为文件分配用户打开文件表表项和系统打开文件表表项，并为后者设置初值，<br>通过指针建立表项与活动Node之间的联系，再把文件描述符fd返回给调用者。</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPYZtTs.png" alt="pPYZtTs.png"></li></ul></li></ul></li></ul><h4 id="2-目录的结构（一种特殊的文件）"><a href="#2-目录的结构（一种特殊的文件）" class="headerlink" title="2.目录的结构（一种特殊的文件）"></a>2.目录的结构（一种特殊的文件）</h4><ul><li>单级目录结构：一个系统只有一张目录表，<strong>不允许文件重名</strong></li><li>两级目录结构<ul><li>分为主文件目录和用户文件目录<ul><li>主文件目录记录用户名及相应用户文件目录的存放位置</li><li>用户文件目录由该用户的文件FCB组成</li></ul></li><li><strong>不同用户的文件可以重名，但不能对文件进行分类</strong></li></ul></li><li>树形目录结构<ul><li><strong>不同目录下的文件可以重名，可以对文件进行分类，不方便文件共享</strong></li><li>系统根据”文件路径”找到目标文件</li><li>从根目录出发的路径是“绝对路径”，<br>从”当前目录出发的路径是相对路径”，可以减少磁盘I/O次数</li></ul></li><li>无环图目录结构<ul><li>在树形目录结构的基础上，增加一些指向同一节点的有向边，使整个目录成为一个有向无环图<br><strong>可以更方便地实现多个用户间的文件共享</strong>。</li><li>为共享结点设置一个共享计数器，计数器为0时才真正删除该结点</li><li>共享文件不同于复制文件。在共享文件中，由于各用户指向的是同一个文件，因此只要其中一个用户修改了文件数据，那么所有用户都可以看到文件数据的变化。</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPJ7FHA.png" alt="pPJ7FHA.png"></li></ul></li></ul></li></ul><h4 id="3-文件的逻辑结构"><a href="#3-文件的逻辑结构" class="headerlink" title="3.文件的逻辑结构"></a>3.文件的逻辑结构</h4><ul><li>逻辑结构：在用户看来文件内部的数据应该是如何组织起来的。</li><li>无结构文件（流式文件）<ul><li>文件内部的数据就是一系列二进制流或字符流组成。又称“流式文件”。如：Windows操作系统中的txt文件</li></ul></li><li>有结构文件（记录式文件）<ul><li>定义<ul><li>由一组相似的记录组成，又称“记录式文件”。每条记录又若干个数据项组成。如：数据库表文件。</li><li><strong>一般来说，每条记录有一个数据项可作为关键字</strong>。</li><li>根据各条记录的长度（占用的存储空间）是否相等，又可分为定长记录和可变长记录两种。</li></ul></li><li>分类（逻辑上如何组织）<ul><li>顺序文件<ul><li>定义：文件中的记录一个接一个地顺序排列（逻辑上），记录可以是定长的或可变长的。<strong>各个记录在物理上可以顺序存储或链式存储。</strong></li><li>两种结构<ul><li>串结构：记录之间的顺序与关键字无关</li><li>顺序结构：记录之间的顺序按关键字顺序排列</li></ul></li><li><strong>可变长记录的顺序文件无法实现随机存取，定长记录的顺序存储方式可以</strong></li><li><strong>定长记录、顺序结构的顺序文件可以快速检索（根据关键字快速找到记录）</strong></li><li>最大缺点：不方便增加/删除记录</li></ul></li><li>索引文件<ul><li>建立一张索引表，每个记录对应一个表项。各记录不用保持顺序，方便增加/删除记录</li><li>索引表本身就是定长记录的顺序文件，一个索引表项就是一条定长记录，<strong>因此索引文件可支持随机存取</strong></li><li><strong>若索引表按关键字顺序排列，则可支持快速检索</strong></li><li><strong>解决了顺序文件不方便增/删记录的问题，同时让不定长记录的文件实现了随机存取。但索引表可能占用很多空间</strong></li></ul></li><li>索引顺序文件<ul><li>将记录分组，每组对应一个索引表项</li><li>检索记录时先顺序查索引表，找到分组，再顺序查找分组</li><li>当记录过多时，可建立多级索引表</li><li><strong>当只有一级索引号时，对于n个记录最好的分组记录为：$分为\sqrt{n}组,每组\sqrt{n}个记录$，此时平均查询次数为$\sqrt{n}/2*2$</strong><ul><li>若要为N个记录的文件建立K级索引，则最优的分组是每组$N^{\frac{1}{K+1}}$个记录（一般）采用一级索引即可k=1</li><li>检索一个记录的平均查找次数是$N^{\frac{1}{K+1}}/2*(K+1)$</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPJzaJH.png" alt="pPJzaJH.png"></li></ul></li></ul></li></ul></li></ul></li><li>例题<ul><li>本题可以看做是只有一张索引表，此时的查找记录最少时为$\sqrt{10000}/2*2=100$，此时为D<br><img src="https://s1.ax1x.com/2023/08/23/pPYUNWT.png" alt="pPYUNWT.png"></li></ul></li></ul><h4 id="4-文件的物理结构（♚）"><a href="#4-文件的物理结构（♚）" class="headerlink" title="4.文件的物理结构（♚）"></a>4.文件的物理结构（♚）</h4><ul><li><p>磁盘块</p><ul><li>在内存管理中，进程的逻辑地址空间被分为一个一个页面，在外存管理中，文件的逻辑地址空间也被分为了一个一个的文件“块”<br>很多操作系统中，磁盘块的大小与内存块、页面的大小相同</li><li>内存与磁盘之间的数据交换（即读/写操作、磁盘I/O)都是以块”为单位进行的。即每次读入一块，或每次写出一块</li><li>文件的逻辑地址也可以表示为(逻辑块号，块内地址)的形式。</li><li>操作系统为文件分配存储空间都是以块为单位的，用户通过逻辑地址来操作自己的文件，操作系统要负责实现从逻辑地址到物理地址的映射</li></ul></li><li><p>文件的分配方式（<strong>对非空闲磁盘块的管理，实现逻辑地址到物理地址的映射</strong>）</p><ul><li>连续分配<ul><li><strong>连续分配方式要求每个文件在磁盘上占有一组连续的块。</strong></li><li><strong>文件目录中记录存放的起始块号和长度（总共占用几个块）</strong><ul><li>给出要访问的逻辑块号，操作系统找到该文件对应的目录项(FCB)，物理块号=起始块号+逻辑块号</li><li>只需转换块号就行，块内地址保持不变，此时可实现到物理块号的映射</li></ul></li><li><strong>连续分配支持顺序访问和直接访问（即随机访问）</strong><ul><li>读取某个磁盘块时，需要移动磁头。访问的两个磁盘块相隔越远，移动磁头所需时间就越长。</li><li>连续分配的文件在顺序读/写时速度最快</li></ul></li><li><strong>缺点：不方便文件拓展：存储空间利用率低，会产生磁盘碎片</strong><br>可以用紧凑来处理碎片，但是需要耗费很大的时间代价。</li></ul></li><li>链接分配（离散分配）<ul><li>隐式链接<ul><li><strong>除了文件的最后一个磁盘块之外，每个磁盘块中都会保存指向下一个盘块的指针，这些指针对用户是透明的</strong></li><li><strong>目录中记录了文件存放的起始块号和结束块号</strong>。也可以增加一个字段来表示文件的长度<ul><li>操作系统找到该文件对应的目录项(FCB)，从目录项中找到起始块号(即0号块)，将0号逻辑块读入内存，<br>由此知道1号逻辑块存放的物理块号，以此类推，读入i号逻辑块总共需要$i+1$次磁盘I/O</li></ul></li><li>优点：<strong>很方便文件拓展，不会有碎片问题，外存利用率高</strong>。<ul><li>若此时要拓展文件，则可以随便找一个空闲磁盘块，挂到文件的磁盘块链尾，并修改文件的FCB</li></ul></li><li>缺点：<strong>只支持顺序访问，不支持随机访问，查找效率低，指向下一个盘块的指针也需要耗费少量的存储空间。</strong></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPJbo1f.png" alt="pPJbo1f.png"></li></ul></li></ul></li><li>显式链接<ul><li><strong>目录中只需记录文件的起始块号，把用于链接文件各物理块的指针显式地存放在一张表中，即文件分配表（FAT）</strong><ul><li>FAT的各个表项在物理上连续存储，且每一个表项长度相同，因此“物理块号”字段可以是隐含的</li><li><strong>逻辑块号转换成物理块号的过程不需要读磁盘操作</strong><ul><li>用户给出要访问的逻辑块号ⅰ，操作系统找到该文件对应的目录项 (FCB)，从目录项中找到起始块号，<br>之后查询内存中的文件分配表FAT，往后找到ⅰ号逻辑块对应的物理块号。</li></ul></li></ul></li><li><strong>一个磁盘仅设置一张FAT，开机时，将FAT读入内存，并常驻内存</strong>。</li><li>FAT不仅记录了文件中各个块的先后链接顺序，同时还标记了空闲的磁盘块，操作系统可以通过FAT对文件空闲存储空间实现管理</li><li>优点<ul><li><strong>采用链式分配（显式链接）方式的文件，支持顺序访问，也支持随机访问</strong></li><li><strong>由于块号转换的过程不需要访问磁盘，因此相比于隐式链接来说，访问速度快很多</strong>。</li><li><strong>显式链接也不会产生外部碎片，也可以很方便地对文件进行拓展，外存利用率高</strong></li></ul></li><li>缺点：文件分配表的需要占用一定的存储空间</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPJqxVH.png" alt="pPJqxVH.png"></li></ul></li></ul></li></ul></li><li>索引分配（离散分配）<ul><li>索引表<ul><li><strong>系统会为每个文件建立一张索引表，一张索引表的每个索引表项记录了文件的各个逻辑块对应的物理块，索引表中的“逻辑块号”可以是隐含的。</strong></li><li><strong>索引表存放的磁盘块称为索引块。文件数据存放的磁盘块称为数据块；目录需要记录文件名和其对应的索引块</strong></li><li><strong>索引表的功能类似于内存管理中的页表：建立逻辑页面到物理页之间的映射关系</strong><ul><li>用户给出要访问的逻辑块号ⅰ，操作系统找到该文件对应的目录项(FCB)，从目录项中可知索引表存放位置，<strong>将索引表从外存读入内存，并查找索引表即可知道ⅰ号逻辑块在外存中的存放位置</strong></li></ul></li><li>优点：<strong>索引分配方式可以支持随机访问，文件拓展也很容易实现</strong>，需要给文件分配一个空闲块，并增加一个索引表项</li><li>缺点：但是索引表需要占用一定的存储空间</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPJOFYR.png" alt="pPJOFYR.png"></li></ul></li></ul></li><li>解决由于文件太大导致，一个磁盘块装不下此文件索引表的问题<ul><li>链接方案：如果索引表太大，一个索引块装不下，那么可以将多个索引块链接起来存放，需要很多的I/O操作，太低效</li><li>多层索引<ul><li>原理类似于多级页表，使第一层索引块指向第二层的索引块。还可根据文件大小的要求再建立第三层、第四层索引块。</li><li>若采用多层索引，则各层索引表大小不能超过一个磁盘块</li><li><strong>采用K层索引结构，且顶级索引表未调入内存，则访问一个数据块只需要K+1次读磁盘操作</strong></li><li><strong>文件的最大长度：$最多存放索引项个数^{k}*磁盘块大小$</strong></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPJx3VS.png" alt="pPJx3VS.png"></li></ul></li><li>缺点：即使是小文件，访问一个数据块依然需要K+1次读磁盘。</li></ul></li><li>混合索引<ul><li>多种索引分配方式的结合。例如，一个文件的顶级索引表中，既包含直接地址索引（直接指向数据块），又包含一级间接索引（指向单层索引表）、还包含两级间接索引（指向两层索引表）</li><li>对于小文件，只需较少的读磁盘次数就可以访问目标数据块（一般计算机中小文件更多）</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPJxDVU.png" alt="pPJxDVU.png"></li></ul></li></ul></li></ul></li></ul></li><li><p>三种外存分配方式的区别</p><ul><li>连续分配：需访问磁盘1次</li><li>链接分配：需访问磁盘n次</li><li>索引分配：m级需访问磁盘m+1次</li><li><img src="https://s1.ax1x.com/2023/08/23/pPJxoIe.png" alt="pPJxoIe.png"></li></ul></li><li><p>例题</p><ul><li>例1：此时最大长度直接套公式计算：1KB/4=$2^8$个项，此时最大长度为${2^{8<em>2}}$</em>1KB=64MB<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPYaeB9.png" alt="pPYaeB9.png"></li></ul></li><li>例2：若此题的磁盘块没有读入内存，还需要分别加1(读入索引结点)，此时选D<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPYahCV.png" alt="pPYahCV.png"></li></ul></li></ul></li></ul><h4 id="5-文件的操作"><a href="#5-文件的操作" class="headerlink" title="5.文件的操作"></a>5.文件的操作</h4><ul><li><p>文件描述符（即索引号）</p><ul><li><strong>打开文件时，将目录项中的信息复制到内存中的打开文件表中，并将打开文件表的索引号返回给用户进程</strong><br><strong>“索引号”也称“文件描述符”</strong></li><li><strong>打开文件时并不会把文件数据直接读入内存，读数据时才需要读入内存，写数据时需要写出外存。</strong></li><li><strong>读/写文件时用“文件描述符”即可指明文件，不再需要用到文件名</strong></li></ul></li><li><p>创建文件(create系统调用)</p><ul><li>分配外存空间<ul><li>利用空闲链表法、位示图、成组链接法等管理策略，在外存中找到空闲空间</li></ul></li><li>在目录中创建目录项<ul><li>目录项中包含了文件名、文件在外存中的存放位置等信息。</li></ul></li></ul></li><li>删除文件(delete系统调用)<ul><li>回收外存空间<ul><li>回收磁盘块时，根据空闲表法、空闲链表法、位图法等管理策略的不同，需要做不同的处理</li></ul></li><li>从目录表中删除文件对应的目录项</li></ul></li><li>打开文件(open系统调用)<ul><li>过程<ul><li>从目录中找到文件名对应的的目录项，并检查该用户是否有指定的操作权限。</li><li><strong>将目录项中的信息复制到内存中的打开文件表中，并将打开文件表的索引号返回给用户</strong><ul><li>打开文件时并不会把文件数据直接读入内存。“索引号”也称“文件描述符”</li></ul></li><li><strong>打开文件之后，对文件的操作不再需要每次都查询目录，可以根据内存中的打开文件表进行操作</strong></li><li>每个进程有自己的打开文件表，系统中也有一张总的打开文件表<ul><li>进程打开文件表中特有的属性：读写指针、访问权限(只读？读写？)</li><li>系统打开文件表中特有的属性：打开计数器（有多少个进程打开了该文件）</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPYkxeA.png" alt="pPYkxeA.png"></li></ul></li></ul></li><li>关于打开文件表（注意用户打开文件表的读写指针和访问权限以及系统打开文件表的打开计数器）<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPYAPW8.png" alt="pPYAPW8.png"></li></ul></li></ul></li><li>关闭文件(close系统调用)<ul><li>将用户进程的打开文件表相应表项删除</li><li>回收分配给该文件的内存空间等资源</li><li>系统打开文件表的打开计数器count减1，若count=0，则删除对应表项。</li></ul></li><li>读文件(read系统调用)<ul><li><strong>根据读指针、读入数据量、内存位置等信息将文件数据从外存读入内存</strong></li><li>将文件数据读入内存，才能让CPU处理，双击后，应用程序通过read系统调用，将文件数据从外存读入内存，并显示在屏幕上（必须要打开文件之后）</li><li><strong>读/写文件“文件描述符”即可指明文件不再需要用到“文件名</strong></li></ul></li><li>写文件(write系统调用)<ul><li>根据写指针、写出数据量、内存位置将文件数据从内存写出外存</li><li>点击“保存”后，应用程序通过write系统调用，将文件数据从内存写回外存</li></ul></li></ul><h4 id="6-文件的保护"><a href="#6-文件的保护" class="headerlink" title="6.文件的保护"></a>6.文件的保护</h4><ul><li><p>口令保护</p><ul><li>为文件设置一个“口令”，用户想要访问文件时需要提供口令，由系统验证口令是否正确</li><li>实现开销小，但“口令”一般存放在FCB或索引结点中（也就是存放在系统中）因此不太安全</li></ul></li><li><p>加密保护</p><ul><li>用一个“密码”对文件加密，用户想要访问文件时，需要提供相同的“密码”才能正确的解密</li><li>安全性高，但加密/解密需要耗费一定的时间 (如异或加密)</li></ul></li><li>访问控制<ul><li>在每个文件的FCB（或索引结点）中增加一个访问控制表(ACL)，记录各个用户（或各组用户）对文件的访问权限<br>对文件的访问类型可以分为：读/写/执行/删除等</li><li>精简的访问列表：以“组”为单位，标记各“组”用户可以对文件执行哪些操作，系统需要管理分组的信息<ul><li>如：分为系统管理员、文件主、文件主的伙伴、其他用户几个分组。</li><li>当某用户想要访问文件时，系统会检查该用户所属的分组是否有相应的访问权限。</li><li>若想要让某个用户能够读取文件，只需要把该用户放入文件主的伙伴这个分组即可</li></ul></li><li>实现灵活，可以实现复杂的文件保护功能</li></ul></li></ul><h4 id="7-文件共享"><a href="#7-文件共享" class="headerlink" title="7.文件共享"></a>7.文件共享</h4><ul><li><p>文件共享与文件复制的区别</p><ul><li>操作系统为用户提供文件共享功能，可以让多个用户共享地使用同一个文件</li><li>多个用户共享同一个文件，意味着系统中只有“一份”文件数据。并且只要某个用户修改了该文件的数据，其他用户也可以看到文件数据的变化。</li><li>如果是多个用户都“复制”了同一个文件，那么系统中会有“好几份”文件数据。其中一个用户修改了自己的那份文件数据，对其他用户的文件数据并没有影响。</li></ul></li><li><p><strong>基于索引结点的共享方式（硬链接）</strong></p><ul><li>各个用户的目录项指向同一个索引结点，索引结点中需要有链接计数count，用于表示链接到本索引结点上的用户目录项数<br>若cout=2，说明此时有两个用户目录项链接到该索引结点上，或者说是有两个用户在共享此文件</li><li>若某个用户决定“删除”该文件，则只是要把用户目录中与该文件对应的目录项删除，且索引结点的count值减1.<ul><li>若cout&gt;0,说明还有别的用户要使用该文件，暂时不能把文件数据删除，否则会导致指针悬空。</li><li>当count=0时系统负责删除文件。</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPYe7vT.png" alt="pPYe7vT.png"></li></ul></li></ul></li><li><strong>基于符号链的共享方式（软链接）</strong><ul><li><strong>在一个Link型的文件中记录共享文件的存放路径(Windows快捷方式)</strong></li><li>操作系统根据路径一层层查找目录，最终找到共享文件即使软链接指向的共享文件已被删除，Link型文件依然存在，<br>只是通过Link型文件中的路径去查找共享文件会失败（找不到对应目录项）</li><li><strong>由于用软链接的方式访问共享文件时要查询多级目录，会有多次磁盘I/O，因此用软链接访问共享文件的速度要比硬链接更慢</strong></li></ul></li><li>例题<ul><li>例1：此时新创建硬链接的文件的索引节点指向F1的索引结点，删除F1后其技术值变为1，软链接的计数值不变，即选B<br><img src="https://s1.ax1x.com/2023/08/23/pPYwWlT.png" alt="pPYwWlT.png"></li></ul></li></ul></li></ul><h3 id="二-文件系统（✪）"><a href="#二-文件系统（✪）" class="headerlink" title="二.文件系统（✪）"></a>二.文件系统（✪）</h3><h4 id="1-文件系统布局"><a href="#1-文件系统布局" class="headerlink" title="1.文件系统布局"></a>1.文件系统布局</h4><ul><li>文件系统的概述<ul><li><strong>操作系统中负责管理和存储文件信息的软件机构称为文件管理系统，简称文件系统</strong></li><li>文件系统由三部分组成：与文件管理有关的软件、被管理文件及实施文件管理所需的数据结构。</li><li>文件系统的功能<ul><li>对于用户而言，文件系统最主要的功能是实现对文件的基本操作，<br><strong>让用户可以按名存储和查找文件</strong>，组织成合适的结构，并应当具有基本的文件共享和文件保护功能。</li><li>从系统角度看，文件系统负责对文件的存储空间进行组织、分配；负责文件的存储并对存入文件进行保护、检索。</li></ul></li></ul></li><li>文件系统在磁盘中的结构<ul><li>磁盘的物理格式化：即低级格式化，划分扇区，检测坏扇区，并用备用扇区替换坏扇区</li><li>磁盘的逻辑格式化：磁盘分区(分卷Volume)后，对各分区进行逻辑格式化，完成文件系统初始化</li><li>文件系统在磁盘中的组成<ul><li>主引导记录(MBR)<ul><li>位于磁盘的0号扇区，用来引导计算机，MBR后面是分区表，该表给出每个分区的起始和结束地址。</li><li>表中的一个分区被标记为活动分区，当计算机启动时，BIOS读入并执行MBR，MBR做的第一件事是确定活动分区，读<br>入它的第一块，即引导块。</li></ul></li><li>引导块<ul><li><strong>MBR执行引导块中的程序后，该程序负责启动该分区中的操作系统</strong>。为统一起见，每个分区都从一个引导块开始。Windows系统称之为分区引导扇区。</li></ul></li><li>超级块<ul><li><strong>包含文件系统的所有关键信息，在计算机启动时，或者在该文件系统首次使用时，超级块会被读入内存</strong>。</li><li>超级块中的典型信息包括分区的块的数量、块的大小、空闲块的数量和指针、空闲的FCB数量和FCB指针等。</li></ul></li><li>其它组成<ul><li>文件系统中空闲块的信息，可以使用位示图或指针链接的形式给出。</li><li>后面也许跟的是一组i结点，每个文件对应一个结点，ⅰ结点说明了文件的方方面面。</li><li>接着可能是根目录，它存放文件系统目录树的根部。最后，磁盘的其他部分存放了其他所有的目录和文件。</li></ul></li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/24/pPYhUy9.png" alt="pPYhUy9.png"></li></ul></li></ul></li><li>文件系统在内存中的结构<ul><li>近期访问过的目录文件会缓存在内存中，不用每次都从磁盘读入，这样可以加快目录检索速度</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/24/pPY58v4.png" alt="pPY58v4.png"></li></ul></li></ul></li><li>文件系统的层次结构（了解）<ul><li><img src="https://s21.ax1x.com/2024/08/12/pApQybF.png" alt="pApQybF.png"></li></ul></li></ul><h4 id="2-外存空闲空间管理（♚）"><a href="#2-外存空闲空间管理（♚）" class="headerlink" title="2.外存空闲空间管理（♚）"></a>2.外存空闲空间管理（♚）</h4><ul><li><p>存储空间的划分与初始化</p><ul><li>包含文件系统的物理磁盘分区被称为卷（逻辑卷、逻辑盘）C盘、D盘……</li><li>存储空间的初始化：将各个文件卷划分为目录区、文件区<ul><li>目录区主要存放文件目录信息(FCB)、用于磁盘存储空间管理的信息</li><li>文件区用于存放文件数据</li></ul></li><li>有的系统支持超大型文件，可支持由多个物理磁盘组成一个文件卷</li></ul></li><li><p>存储空间管理（对空闲块的组织、分配与回收）</p><ul><li><p>空闲表法（连续分配方式）</p><ul><li>与内存的动态分配相似，为每个文件分配一块连续的存储空间。系统为外存上的所有空闲区建立一张空闲表<br>每个空闲区对应一个空闲表项，包括表项序号，空闲区的第一个盘块号，空闲盘块数，再以起始盘块号递增的次序排列</li><li>分配磁盘块：与内存管理中的动态分区分配很类似，为一个文件分配连续的存储空间，同样可采用首次适应、最佳适应、最坏适应等算法来决定要为文件分配哪个区间。</li><li>回收磁盘块：与内存管理中的动态分区分配很类似，回收时需要注意表项的合并问题。</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPYiJeI.png" alt="pPYiJeI.png"></li></ul></li></ul></li><li><p>空闲链表法</p><ul><li>空闲盘块链（适用于离散分配）<ul><li>以盘块为单位组成一条空闲链，空闲盘块中存储着下一个空闲盘块的指针<br>操作系统保存着链头、链尾指针。</li><li>分配磁盘块：若某文件申请K个盘块，则从链头开始依次摘下K个盘块分配，并修改空闲链的链头指针。</li><li>回收磁盘块：回收的盘块依次挂到链尾，并修改空闲链的链尾指针。</li><li>为文件分配多个盘块时可能要重复多次操作</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPYi6wq.png" alt="pPYi6wq.png"></li></ul></li></ul></li><li>空闲盘区链（离散分配、连续分配都适用）<ul><li>以盘区为单位组成一条空闲链；连续的空闲盘块组成一个空闲盘区；空闲盘区中的第一个盘块内记录了盘区的长度、下一个盘区的指针；操作系统保存着链头、链尾指针。</li><li>分配磁盘块<ul><li>若某文件申请K个盘块，则可以采用首次适应、最佳适应等算法，从链头开始检索按照算法规则找到一个大小符合要求的空闲盘区，分配给文件</li><li>若没有合适的连续空闲块，也可以将不同盘区的盘块同时分配给一个文件，注意分配后可能要修改相应的链指针、盘区大小等数据</li></ul></li><li>回收磁盘块<ul><li>若回收区和某个空闲盘区相邻，则需要将回收区合并到空闲盘区中。</li><li>若回收区没有和任何空闲区相邻，将回收区作为单独的一个空闲盘区挂到链尾。</li></ul></li><li>为个文件分配多个盘块时效率更高</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/23/pPYdrPx.png" alt="pPYdrPx.png"></li></ul></li></ul></li></ul></li><li><p>位示图法（离散分配、连续分配都适用）</p><ul><li><p>位示图</p><ul><li><p>每个二进制位对应一个盘块。“0”代表盘块空闲，“1”代表盘块已分配。</p></li><li><p>位示图一般用连续的“字”来表示，字的字长是16位，字中的每一位对应一个盘块。<br>因此可以用（字号，位号）对应一个盘块号。</p></li><li><p>计算方法</p><ul><li><p><strong>位示图法中行和列都从1开始编号</strong></p><ul><li>(字号，位号)=(i，j) 的二进制位对应的盘块号：$b=n(i-1)+j$</li><li>b号盘块对应的字号：$i=(b-1)/n +1$，位号：$j=(b-1)\%n+1$</li></ul></li><li><p><strong>位示图法中行和列都从0开始编号</strong></p><ul><li><p>(字号，位号)=(i，j) 的二进制位对应的盘块号：$b=ni+j$</p></li><li><p>b号盘块对应的字号：$i=b/n$，位号：$j=b\%n$</p></li></ul></li></ul></li><li><p>图片</p><ul><li><img src="https://s1.ax1x.com/2023/08/23/pPYF0N6.png" alt="pPYF0N6.png"></li></ul></li></ul></li><li><p>磁盘的分配</p><ul><li>若文件需要K个块，①顺序扫描位示图，找到K个相邻或不相邻的“0”</li><li>②根据字号、位号算出对应的盘块号，将相应盘块分配给文件，并将相应位设置为“1”</li></ul></li><li><p>磁盘的回收</p><ul><li>①根据回收的盘块号计算出对应的字号、位号；②将相应二进制位设为“0”</li></ul></li></ul></li><li><p>成组链接法（了解）</p><ul><li><strong>空闲表法、空闲链表法不适用于大型文件系统，因为空闲表或空闲链表可能过大</strong>。<br>UNIX系统中采用了成组链接法对磁盘空闲块进行管理。</li><li>文件卷的目录区中专门用一个磁盘块作为“超级块”，当系统启动时需要将超级块读入内存。<br>并且要保证内存与外存的“超级块”数据一致，<strong>适合大型文件系统</strong></li></ul></li></ul></li><li><p>例题</p><ul><li>盘块号${=}$起始块号${+\lfloor}$盘块号${/(1024 \times 8)\rfloor=32+\lfloor 409612 /(1024 \times 8)\rfloor=32+50=82}$, 这里问的是块内字节号而不是位号, 因此还需除以${8(1 \mathrm{B}=8}$位${)}$, 块内字节号${=\lfloor(}$盘块号${\%(1024 \times 8)) / 8\rfloor=1}$。<br><img src="https://s1.ax1x.com/2023/08/24/pPYHxKS.png" alt="pPYHxKS.png"></li></ul></li></ul><h4 id="3-虚拟文件系统-VFS-与文件系统挂载"><a href="#3-虚拟文件系统-VFS-与文件系统挂载" class="headerlink" title="3.虚拟文件系统(VFS)与文件系统挂载"></a>3.虚拟文件系统(VFS)与文件系统挂载</h4><ul><li>虚拟文件系统<ul><li>虚拟文件系统的特点<ul><li><strong>向上层用户进程提供统一标准的系统调用接口，屏蔽底层具体文件系统的实现差异</strong></li><li>VFS要求下层的文件系统必须实现某些规定的函数功能，如：open/read/write<br>一个新的文件系统想要在某操作系统上被使用，就必须满足该操作系统VFS的要求</li><li>每打开一个文件，VFS就在主存中新建一个vnode，用统一的数据结构表示文件，无论该文件存储在哪个文件系统<br><strong>vnode只存在于主存中，而inode既会被调入主存，也会在外存中存储</strong></li><li>打开文件后，即创建vnode，并将文件信息复制到vnode中，vnode的功能指针指向具体文件系统的函数功能</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/24/pPY5TMQ.png" alt="pPY5TMQ.png"></li></ul></li></ul></li><li>文件系统挂载（文件系统装载）<ul><li>挂载的过程<ul><li>在VFS中注册新挂载的文件系统。内存中的挂载表(mount table)包含每个文件系统的相关信息，包括文件系统类型、容量大小等</li><li>新挂载的文件系统，要向VFS提供一个<strong>函数地址列表</strong></li><li>将新文件系统加到挂载点(mount point)，也就是将新文件系统挂载在某个父目录下</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/24/pPYIVJK.png" alt="pPYIVJK.png"></li></ul></li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;操作系统第四章-文件管理&quot;&gt;&lt;a href=&quot;#操作系统第四章-文件管理&quot; class=&quot;headerlink&quot; title=&quot;操作系统第四章 文件管理&quot;&gt;&lt;/a&gt;操作系统第四章 文件管理&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;计算机学科基础：操作系统第四章文件管理的学习笔记&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="计算机基础学习笔记" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="操作系统" scheme="http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>操作系统第三章-内存管理</title>
    <link href="http://example.com/2024/08/12/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    <id>http://example.com/2024/08/12/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</id>
    <published>2024-08-11T17:33:10.000Z</published>
    <updated>2024-08-12T03:08:30.379Z</updated>
    
    <content type="html"><![CDATA[<h2 id="操作系统第三章-内存管理"><a href="#操作系统第三章-内存管理" class="headerlink" title="操作系统第三章 内存管理"></a>操作系统第三章 内存管理</h2><blockquote><p>计算机学科基础：操作系统第三章内存管理的学习笔记</p></blockquote><span id="more"></span><h3 id="一-内存管理的概念（✪）"><a href="#一-内存管理的概念（✪）" class="headerlink" title="一.内存管理的概念（✪）"></a>一.内存管理的概念（✪）</h3><h4 id="1-内存管理的基本原理与要求"><a href="#1-内存管理的基本原理与要求" class="headerlink" title="1.内存管理的基本原理与要求"></a>1.内存管理的基本原理与要求</h4><ul><li><p>存储管理的目的：方便用户和提高内存利用率<br>对内存的访问以字或字节为单位</p></li><li><p>逻辑地址与物理地址</p><ul><li>逻辑地址<ul><li>编译后，每个目标模块都从0号单元开始编址，这称为该目标模块的相对地址（或逻辑地址）。</li><li>当链接程序将各个模块链接成一个完整的可执行目标程序时，<br>链接程序顺序依次按各个模块的相对地址构成统一的从0号单元开始编址的逻辑地址空间（或虚拟地址空间）</li><li>进程在运行时，看到和使用的地址都是逻辑地址。用户程序和程序员只需知道逻辑地址，<br>而内存管理的具体机制则是完全透明的。</li><li>不同进程可以有相同的逻辑地址，因为这些相同的逻辑地址可以映射到主存的不同位置。</li></ul></li><li>物理地址<ul><li>物理地址空间是指内存中物理单元的集合，它是地址转换的最终地址，进程在运行时执行指令和访问数据，最后都要通过物理地址从主存中存取。</li><li><strong>当装入程序将可执行代码装入内存时，必须通过地址转换将逻辑地址转换成物理地址，这个过程称为地址重定位。</strong></li><li>操作系统通过内存管理部件(MMU)将进程使用的逻辑地址转换为物理地址。进程使用虚拟内存空间中的地址，操作系统在相关硬件的协助下，将它“转换”成真正的物理地址。</li><li>逻辑地址通过页表映射到物理内存，页表由操作系统维护并被处理器引用。</li></ul></li></ul></li><li><p>程序的链接和装入</p><ul><li>创建进程首先要将程序和数据装入内存。将用户源程序变为可在内存中执行的程序，有以下三个步骤：</li><li>编译：由源代码文件生成目标模块(高级语言“<strong>翻译”为机器语言</strong>)</li><li>链接：由链接程序将编译后形成的一组目标模块及它们所需的库函数链接在一起，形成一个完整的装入模块，<strong>形成逻辑地址</strong><ul><li>静态链接：在程序运行之前，先将各目标模块及它们所需的库函数连接成一个完整的可执行文件（装入模块），<br>之后不再拆开。</li><li>装入时动态链接：将各目标模块装入内存时，边装入边链接的链接方式<br>其优点是便于修改和更新，便于实现对目标模块的共享</li><li>运行时动态链接：对某些目标模块的链接，是在程序执行中需要该目标模块时才进行的。凡在执行过程中未被用到的目标模块，都不会被调入内存和被链接到装入模块上。<br>其优点是能加快程序的装入过程，还可节省大量的内存空间</li></ul></li><li>装入：将装入模块装入内存，装入后<strong>形成物理地址（实现将逻辑地址转换为物理地址）</strong><ul><li>绝对装入<ul><li>在编译时，如果知道程序将放到内存中的哪个位置，编译程序将产生绝对地址的目标代码。<br>此时编译、链接后得到的装入模块的指令直接就使用了绝对地址，装入程序按照装入模块中的地址，将程序和数据装入内存。</li><li>绝对装入只适用于单道程序环境。</li><li>程序中使用的绝对地址，可在编译或汇编时给出，也可由程序员直接赋予。<br>通常情况下都是编译或汇编时再转换为绝对地址</li></ul></li><li>静态重定位（可重定位装入，装入时转换为物理地址）<ul><li>编译、链接后的装入模块的地址都是从0开始的，指令中使用的地址、数据存放的地址都是相对于起始地址而言的逻辑地址。</li><li><strong>可根据内存的当前情况，将装入模块装入到内存的适当位置。装入时对地址进行“重定位”，将逻辑地址变换为物理地址</strong>（地址变换是在装入时一次完成的）</li><li>适用于早期多道批处理系统</li><li>静态重定位的特点<ul><li>在一个作业装入内存时，必须分配其要求的全部内存空间，如果没有足够的内存，就不能装入该作业。</li><li><strong>作业一旦进入内存后，在运行期间就不能再移动，也不能再申请内存空间。</strong></li></ul></li></ul></li><li>动态重定位（动态运行时装入，执行时转换为物理地址）<ul><li><strong>程序在内存中若发生移动</strong>，则需要采用动态的装入方式。<strong>装入程序把装入模块装入内存后，并不立即把装入模块中的相对地址转换为绝对地址，而是把这种地址转换推迟到程序真正要<u>执行时</u>才进行</strong>。</li><li><strong>装入内存后的所有地址均为相对地址。这种方式需要一个重定位寄存器的支持，在运行时将逻辑地址转换为物理地址</strong><ul><li>重定位寄存器：存放装入模块存放的起始位置</li></ul></li><li>动态重定位的优点：<ul><li>可以将程序分配到不连续的存储区；</li><li>在程序运行之前可以只装入部分代码即可投入运行，然后在程序运行期间，根据需要动态申请分配内存<br>可以向用户提供一个比存储空间大得多的地址空间</li><li>便于程序段的共享。</li></ul></li></ul></li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/18/pP3N4qP.png" alt="pP3N4qP.png"></li></ul></li></ul></li><li>进程的内存映像<ul><li>组成<ul><li>只读代码段：即程序的二进制代码或<strong>由const关键字修饰的常变量</strong>，代码段是只读的，可以被多个进程共享。</li><li>读/写数据段：即程序运行时加工处理的对象，包括定义<strong>在函数外的全局变量、由static关键字修饰的静态变量</strong></li><li>堆：用来存放动态分配的变量。通过调用malloc函数动态地向高地址分配空间</li><li>共享库的存储映射区：是被调用的库函数</li><li>栈：用来实现函数调用。从用户空间的最大地址往低地址方向增长，<strong>在函数大括号内定义的局部变量、函数调用时传入的参数</strong></li><li>进程控制块(PCB):存放在系统区。操作系统通过PCB来控制和管理进程，在操作系统内核区。</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/19/pP3RRzt.png" alt="pP3RRzt.png"></li></ul></li></ul></li><li><p>内存管理的主要功能</p><ul><li><p>内存空间的分配与回收</p><ul><li>由操作系统完成主存储器空间的分配和管理，使程序员摆脱存储分配的麻烦，提高编程效率</li><li>包括连续分配的管理方式和非连续分配的管理方式<ul><li>连续分配管理方式包括：单一连续分配、固定分区分配、动态分区分配（可变分区分配）</li><li>非连续分配管理方式包括：基本分页存储管理、基本分段存储管理、段页式存储管理</li></ul></li></ul></li><li><p>地址转换</p><ul><li>在多道程序环境下，程序中的逻辑地址与内存中的物理地址不可能一致，因此存储管理必须提供地址变换功能，<br>把逻辑地址转换成相应的物理地址（地址重定位）</li></ul></li><li><p>内存空间的扩充</p><ul><li>利用虚拟存储技术或自动覆盖技术、交换技术，从逻辑上扩充内存</li><li><strong>覆盖技术适用于单一连续分配和固定分区分配</strong></li></ul></li><li><p>内存共享</p><ul><li>指允许多个进程访问内存的同一部分。例如，多个合作进程可能需要访问同一块数据，因此必须支持对内存共享区域进行受控访问。</li></ul></li><li><p>内存保护（界地址保护）</p><ul><li>保证各道作业在各自的存储空间内运行，互不干扰，不会越界访问，需要操作系统与硬件合作</li><li>方法一：在CPU中设置一对上、下限寄存器，存放进程的上、下限地址。进程的指令要访问某个地址时，<br>CPU检查是否越界。</li><li>方法二：采用重定位寄存器（又称基址寄存器）和界地址寄存器（又称限长寄存器）进行越界检查。<br>重定位寄存器中存放的是进程的起始物理地址。界地址寄存器中存放的是进程的最大逻辑地址。</li></ul></li></ul></li></ul><h4 id="2-连续分配管理方式"><a href="#2-连续分配管理方式" class="headerlink" title="2.连续分配管理方式"></a>2.连续分配管理方式</h4><ul><li>连续分配：指为用户进程分配的必须是一个<strong>连续的内存空间</strong>。</li><li>内部碎片与外部碎片<ul><li>内部碎片，程序小于固定分区的大小时，有些部分没有用上，会存在空间浪费</li><li>外部碎片，是指内存中的某些空闲分区由于太小而难以利用，内存的利用率下降</li><li>如果内存中空闲空间的总和本来可以满足某进程的要求，但由于进程需要的是一整块连续的内存空间，<br>因此这些“碎片”不能满足进程的需求。可以通过紧凑(拼凑)技术来解决外部碎片。</li></ul></li><li>三种连续分配方式<ul><li>单一连续分配<ul><li>只支持单道程序，内存被分为系统区和用户区。系统区通常位于内存的低地址部分，用于存放操作系统相关数据；<br>用户区用于存放用户进程相关数据。内存中只能有一道用户程序，用户程序独占整个用户区空间。</li><li>优点：实现简单；<strong>无外部碎片</strong>；<strong>可以采用覆盖技术扩充内存；无须采取内存保护</strong></li><li>缺点：只能用于单用户、单任务的操作系统中；<strong>有内部碎片</strong>；存储器利用率极低。</li></ul></li><li>固定分区分配<ul><li>最简单的一种<strong>多道程序存储管理方式</strong>，它将用户内存空间划分为若干固定大小的区域，每个分区只装入一道作业。<br>当有空闲分区时，便可再从外存的后备作业队列中选择适当大小的作业装入该分区，如此循环。</li><li>固定分区划分分区的两种方法<ul><li>分区大小相等：程序太小会造成浪费，程序太大又无法装入，缺乏灵活性。<br>适合用于用一台计算机控制多个相同对象的场合</li><li>分区大小不等：划分为多个较小的分区、适量的中等分区和少量大分区。</li></ul></li><li>分区使用表<ul><li>概述（一个数据结构，实现各个分区的分配与回收）<ul><li>每个表项对应一个分区，通常按分区大小排列。每个表项包括对应分区的大小、起始地址、状态（是否已分配）</li><li>分配内存时，便检索该表，以找到一个能满足要求且尚未分配的分区分配给装入程序，并将对应表项的状态置为“已分配”，若找不到这样的分区，则拒绝分配。</li><li>回收内存时，只需将对应表项的状态置为“未分配”即可。</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/19/pP3WakQ.png" alt="pP3WakQ.png"></li></ul></li></ul></li><li>优点：实现简单，<strong>无外部碎片</strong>。</li><li>缺点：当用户程序太大时，可能所有的分区都不能满足需求，此时不得不<strong>采用覆盖技术来解决</strong>，但这又会降低性能 ；<br>会<strong>产生内部碎片</strong>，内存利用率低</li></ul></li><li>动态分区分配（可变分区分配）<ul><li>支持多道程序，<strong>不会预先划分内存分区，而是在进程装入内存时，根据进程的大小动态地建立分区</strong>，并使分区的大小正好适合进程的需要。因此系统分区的大小和数目是可变的。</li><li>记录空闲内存分区的两种数据结构<ul><li>空闲分区表：每个空闲分区对应一个表项。表项中包含分区号分区大小、分区起始地址等信息</li><li>空闲分区链：每个分区的起始部分和末尾部分分别设置前向指针和后向指针。起始部分处还可记录分区大小等信息</li><li>分配内存和回收内存时，修改相应的表项，<strong>回收时出现相邻的空闲分区要合并（采用拼接技术）</strong></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/19/pP3WXhd.png" alt="pP3WXhd.png"></li></ul></li></ul></li><li>特点：<strong>会产生外部碎片（可用紧凑来解决），不会产生内部碎片</strong></li><li>动态分区分配算法：在动态分区分配中，当内存中很多个空闲分区都能满足需求时，应该选择哪个分区进行分配<ul><li>首次适应算法  (First Fit)<ul><li>算法思想：<strong>每次都从低地址开始查找，找到第一个能满足大小的空闲分区</strong>。</li><li>如何实现：<strong>空闲分区以地址递增的次序排列</strong>。每次分配内存时顺序查找空闲分区链（或空闲分区表），<br>找到大小能满足要求的第一个空闲分区。、</li><li>优点：<strong>算法开销小，性能最优</strong>（回收分区后不需要重新进行空闲分区的排序）</li><li>缺点：会使内存的低地址部分产生很多小的空闲分区，每次查找时都要经过这些分区，增加了开销</li></ul></li><li>最佳适应算法  (Best Fit)<ul><li>算法思想：保证当“大进程”到来时能有连续的大片空间，可以<strong>尽可能多地留下大片的空闲区，即优先使用更小的空闲区。</strong></li><li>如何实现：<strong>空闲分区按容量递增次序链接</strong>。每次分配内存时顺序查找空闲分区链（或空闲分区表），<br>找到大小能满足要求的第一个空闲分区；</li><li>缺点<ul><li>每次都选最小的分区进行分配，会留下越来越多的、很小的、难以利用的内存块，<strong>产生很多的外部碎片</strong></li><li><strong>在发生次序变动时需要对链表进行重新排列，开销大</strong></li></ul></li></ul></li><li>最坏/大适应算法  (Worst Fit)<ul><li>算法思想：为了解决最佳适应算法的问题一一即留下太多难以利用的小碎片，可以在每次分配时优先使用最大的连续空闲区，<strong>这样分配后剩余的空闲区就不会太小，更方便使用。</strong></li><li>如何实现：<strong>空闲分区按容量递减次序链接</strong>。每次分配内存时顺序查找空闲分区链（或空闲分区表），<br>找到大小能满足要求的第一个空闲分区；在发生次序变动时需要对链表进行重新排列</li><li>缺点：<ul><li>每次都选最大的分区进行分配，虽然可以让分配后留下的空闲区更大，更可用，但是这种方式会导致较大的连续空闲区被迅速用完。<strong>如果之后有“大进程”到达，就没有内存分区可用了</strong>。</li><li><strong>在发生次序变动时需要对链表进行重新排列，开销大</strong></li></ul></li></ul></li><li>邻近适应算法  (Next Fit)<ul><li>算法思想：首次适应算法每次都从链头开始查找的。这可能会导致低地址部分出现很多小的空闲分区，而每次分配查找时，都要经过这些分区，因此也增加了查找的开销。<br>如果每次都从上次查找结束的位置开始检索，就能解决上述问题。</li><li>如何实现：<strong>空闲分区以地址递增的顺序排列</strong>（可排成一个循环链表）。<strong>每次分配内存时从上次查找结束的位置开始查找空闲分区链</strong>（或空闲分区表），找到大小能满足要求的第一个空闲分区。</li><li>优点：开销小（回收分区后不需要重新进行空闲分区的排序）</li><li>缺点：邻近适应算法的规则可能会导致无论低地址、高地址部分的空闲分区都有相同的概率被使用，<br>也就导致了高地址部分的大分区更可能被使用，划分为小分区，最后<strong>导致无大分区可用</strong>（最大适应算法的缺点）</li></ul></li><li>四种算法的区别<ul><li><img src="https://s1.ax1x.com/2023/08/19/pP3hIeK.png" alt="pP3hIeK.png"></li></ul></li><li>例题<ul><li><img src="https://s1.ax1x.com/2023/08/20/pP8djns.png" alt="pP8djns.png"></li><li><img src="https://s1.ax1x.com/2023/08/20/pP8dvBn.png" alt="pP8dvBn.png"></li></ul></li></ul></li></ul></li></ul></li></ul><h4 id="4-基本分页存储管理（♚）"><a href="#4-基本分页存储管理（♚）" class="headerlink" title="4.基本分页存储管理（♚）"></a>4.基本分页存储管理（♚）</h4><ul><li>分页存储的基本概念<ul><li>内存空间与进程空间的划分<ul><li>内存空间中的概念<ul><li>页框、页帧：将内存划分为一个个大小相等的分区，即内存中的块（也称为内存块、物理块、物理页面）</li><li>页框号：每个页框的编号，从0开始（又称为页帧号、内存块号、物理块号、物理页号）</li></ul></li><li>进程中的概念<ul><li>页、页面：将进程的逻辑地址空间也分为与页框大小相等的一个个部分</li><li>页号、页面号：每个页面的编号，<strong>从0开始</strong></li><li>页面大小由多种因素决定，一旦决定就是等长的<ul><li>页面小，用于管理页面的页表就大，但是页内碎片小</li><li>页面大，用于管理页面的页表就小，但是页内碎片大</li></ul></li></ul></li><li>操作系统以页框为单位为各个进程分配内存空间。进程的每个页面分别放入一个页框中。<br><strong>进程的页面与内存的页框存在着一一对应的关系</strong>，但各个页面不必连续存放，可以放在不相邻的各个页框中</li><li>注：<strong>分页管理不会产生外部碎片</strong>；进程只会在为最后一个页面申请一个主存块空间时，才<strong>可能会产生内部碎片</strong>，<br>但这种碎片相对于进程来说也是很小的，每个进程平均只产生半个块大小的内部碎片（也称页内碎片）</li></ul></li><li>页表的概念（页号与物理块号的映射）<ul><li>为了便于在内存中找到进程的每个页面所对应的物理块，系统为每个进程建立一张页表，<br>它记录页面在内存中对应的物理块号，页表一般存放在内存的PCB中。</li><li>一个进程对应一张页表，页表由页表项组成，每个页表项由页号和块号（页框号）组成<br>页号由0到1，块号是与之对应的在内存中的物理地址。</li><li>每个页表项的长度是相同的</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/19/pP37tiT.png" alt="pP37tiT.png"></li></ul></li></ul></li><li>确定页表的相关数据<ul><li>确定页表项的大小（页面长度）<ul><li>页表项连续存放，因此页号可以是隐含的，不占存储空间（类比数组）</li><li>可通过计算机中内存块的数量$\rightarrow$ 页表项中的块号至少占多少个字节 </li><li>假设某系统物理内存大小为4GB，页面大小为4KB，则每个页表项至少应该为多少字节？<ul><li>内存块大小=页面大小=4KB=$2^{12}$B</li><li>4GB的内存总共会被分为$2^{32}/2^{12}=2^{20}$个内存块</li><li>内存块号的范围应该是${0 \sim 2^{20}-1}$</li><li>内存块号至少要用 20 bit 来表示</li><li>即至少要用3B来表示块号${(3 * 8=24 \mathrm{bit})}$<ul><li><img src="https://s1.ax1x.com/2023/08/19/pP3xSiR.png" alt="pP3xSiR.png"></li><li><strong>页表记录的只是内存块号，而不是内存块的起始地址</strong></li><li>$j$号内存块的起始地址=$j*内存块大小$</li></ul></li><li>由于页号是隐含的，因此每个页表项占3B，存储页号为n的整个页表至少需要$3*(n+1)B$</li></ul></li></ul></li><li>找到页号所对应的块号的存储地址<ul><li>假设页表中的各页表项从内存地址为X的地方开始连续存放，此时ⅰ号页表项的存放地址=$X+内存块大小*i$</li></ul></li></ul></li><li>实现逻辑地址到物理地址的转换<ul><li>1.计算出逻辑地址对应的[页号，页内偏移量]<ul><li>页号=逻辑地址/页面长度（取除法的整数部分）</li><li>页内偏移量=逻辑地址%页面长度（取除法的余数部分）</li></ul></li><li>2.找到对应页面在内存中的起始地址（查页表）</li><li>3.物理地址=页面起始始址+页内偏移量</li><li>假设在某计算机系统中，页面大小是50B。某进程逻辑地址空间大小为200B，则逻辑地址110对应的页号、页内偏移量是多少？<ul><li>页号=110/50=2</li><li>页内偏移量=110%50=10</li><li>通过页号查询页表，可知页面在内存中物理块号，从而知道页面的起始地址</li><li>页面在内存中的起始地址+页内偏移量=实际的物理地址</li></ul></li></ul></li><li>逻辑地址结构（页号P，页内偏移量W）<ul><li>如果页面大小刚好是<strong>2的整数幂</strong>，则计算机硬件可快速拆分逻辑地址，</li><li><strong>如果每个页面的大小为$2^{k}$B，用二进制数表示逻辑地址，则末尾K位即为页内偏移量，其余部分为页号</strong></li><li>地址长度为 32 位时，其中${0 \sim 11}$位为页内地址，即每页大小为${4 \mathrm{KB} 、 12 \sim 31}$位为页号, 即最多允许${2^{20}}$页</li><li>计算机直接将页号所对应的块的起始地址与页内偏移量的二进制拼接起来即可得到物理地址<ul><li>注：这里的块号与页号的关系需要查询相应的页表<img src="https://s1.ax1x.com/2023/08/19/pP3OJ8f.jpg" alt="pP3OJ8f.jpg"></li></ul></li><li>页面大小&lt;-&gt;页内偏移量位数-&gt;逻辑地址结构<ul><li>如果有K位表示“页内偏移量”，则说明该系统中一个页面的大小是$2^k$个内存单元</li><li>如果有M位表示“页号”，则说明在该系统中，一个进程最多允许有$2^M$个页面</li></ul></li><li>特点<ul><li>在分页存储管理（页式管理）的系统中，<strong>只要确定了每个页面的大小，逻辑地址结构就确定了</strong>。因此，页式管理中地址是一维的。</li><li>即只要给出一个逻辑地址，系统就可以自动地算出页号、页内偏移量两个部分，并不需要显式地告诉系统这个逻辑地址中，页内偏移量占多少位。</li></ul></li></ul></li></ul></li><li>基本地址变换机构<ul><li>基本地址变换机构的构成<ul><li>通常会在系统中设置一个页表寄存器(PTR)，<strong>存放页表在内存中的起始地址F和页表长度M</strong></li><li>进程未执行时，页表的始址和页表长度放在进程控制块(PCB)中</li><li>当进程被调度时，操作系统内核会把它们放到页表寄存器中</li></ul></li><li>逻辑地址到物理地址的转换过程（由硬件自动完成）<ul><li>将页表始址和页表长度装入页表寄存器中。设页面大小为L，逻辑地址A到物理地址E的变换过程如下<ul><li>①计算页号P (P=A/L) 和页内偏移量W (W=A%L).</li><li>②比较页号P和页表长度M，若P≥M，则产生越界中断，否则继续执行。</li><li>③页表中页号P对应的页表项地址=页表始址F+页号P×页表项长度，取出该页表项内容b，即为物理块号。<br>注意区分页表长度和页表项长度。页表长度是指一共有多少页，页表项长度是指页地址占多大的存储空间。</li><li>④计算E=b×L+W，用得到的物理地址E去访问内存。</li></ul></li><li>实例<ul><li>若页面大小L为1KB，页号2对应的物理块为b=8，计算逻辑地址A=2500的物理地址E的过程如下<ul><li>P=2500/1K=2，W=2500%1K=452，查找得到页号2对应的物理块的块号为8，E=8×1024+452=8644。</li></ul></li></ul></li><li>图片<ul><li><strong>重定位寄存器：整个系统中设置一个即可</strong></li><li><img src="https://s1.ax1x.com/2023/08/19/pP3jbHH.png" alt="pP3jbHH.png"></li></ul></li></ul></li><li>关于页表项的大小问题<ul><li>理论上，页表项长度为3B即可表示内存块号的范围，但是，为了方便页表的查询，<br>常常会让一个页表项占更多的字节，使得每个页面恰好可以装得下整数个页表项，因此选择长度为4B</li></ul></li><li>如何加快地址转换<ul><li>设置快表（TLB）并增大快表容量</li><li>让页表常驻内存</li></ul></li></ul></li><li><p>具有快表的地址变换机构</p><ul><li>快表，又称联想寄存器(TLB)，是一种访问速度比内存快很多的高速缓存(TLB不是内存！)，用来存放最近访问的页表项的副本<br>可以加速地址变换的速度。与此对应，内存中的页表常称为慢表。</li><li><p>快表的工作过程</p><ul><li><p>过程</p><ul><li>①CPU给出逻辑地址，由某个硬件算得页号、页内偏移量，将页号与快表中的所有页号进行比较。</li><li>②如果找到匹配的页号，说明要访问的页表项在快表中有副本，则直接从中取出该页对应的内存块号，<br>再将内存块号与页内偏移量拼接形成物理地址，最后访问该物理地址对应的内存单元。<br>因此，若快表命中，则访问某个逻辑地址仅需一次访存即可。</li><li>③如果没有找到匹配的页号，则需要访问内存中的页表，找到对应页表项，得到页面存放的内存块号，<br>并且将页表项复制到快表中，再将内存块号与页内偏移量拼接形成物理地址，最后访问该物理地址对应的内存单元。<br>因此，若快表未命中，则访问某个逻辑地址需要两次访存</li><li>在找到页表项后，应同时将其存入快表，以便后面可能的再次访问。<br>但若快表已满，则必须按照一定的算法对旧的页表项进行替换</li></ul></li><li><p>图片</p><ul><li><img src="https://s1.ax1x.com/2023/08/19/pP8k1OI.png" alt="pP8k1OI.png"></li></ul></li><li><p>例子</p><ul><li><img src="https://s1.ax1x.com/2023/08/19/pP8Ak9g.png" alt="pP8Ak9g.png"></li></ul></li></ul></li><li><p>局部性原理</p><ul><li>由于查询快表的速度比查询页表的速度快很多，因此只要快表命中，就可以节省很多时间。<br>因为局部性原理，一般来说快表的命中率可以达到90%以上。</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/19/pP8AEcj.png" alt="pP8AEcj.png"></li></ul></li></ul></li></ul></li><li><p>两级页表</p><ul><li>单机页表的问题<ul><li>问题一：页表必须连续存放，因此当页表很大时，需要占用很多个连续的页框。<ul><li>多级页表可以减少页表所占的连续内存空间</li></ul></li><li>问题二：没有必要让整个页表常驻内存，因为进程在一段时间内可能只需要访问某几个特定的页面。<ul><li>可以由虚拟存储器解决：作业不必全部装入内存且不用一直驻留在内存</li></ul></li></ul></li><li>两级页表的原理、地址结构<ul><li>把页表再分页并离散存储，然后再建立一张页表记录页表各个部分的存放位置，称为页目录表，或称外层页表，或称顶层页表</li><li>再把原来的逻辑地址结构的页号划分为为10位的一级页号与10位的二级页号</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/19/pP8ZzRS.png" alt="pP8ZzRS.png"></li></ul></li><li>两级页表的访问过程<ul><li>按照地址结构将逻辑地址拆分成三部分</li><li>从PCB中读出页目录表始址，根据一级页号查页目录表，找到下一级页表在内存中的存放位置</li><li>根据二级页号查表，找到最终想访问的内存块号</li><li>结合页内偏移量得到物理地址</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/19/pP8e9MQ.png" alt="pP8e9MQ.png"></li></ul></li></ul></li></ul></li><li>多级页表<ul><li>多级页表中，各级页表的大小不能超过一个页面。若两级页表不够，可以分更多级</li><li>多级页表的访存次数（假设没有快表机构）N级页表访问一个逻辑地址需要N+1次访存</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/19/pP8mZTI.png" alt="pP8mZTI.png"></li></ul></li><li>例题<ul><li><img src="https://s1.ax1x.com/2023/08/22/pPJPI76.png" alt="pPJPI76.png"></li><li><img src="https://s1.ax1x.com/2023/08/22/pPJPHhD.png" alt="pPJPHhD.png"></li></ul></li></ul></li></ul></li></ul><h4 id="5-基本分段存储管理（♚）"><a href="#5-基本分段存储管理（♚）" class="headerlink" title="5.基本分段存储管理（♚）"></a>5.基本分段存储管理（♚）</h4><ul><li><p>分段存储管理的概述</p><ul><li>引入分段存储管理可以满足：方便编程、分段共享、分段保护、动态链接与动态增长</li><li>进程的地址空间：<strong>按照程序自身的逻辑关系划分为若干个段，每个段都有一个段名，每段从0开始编址</strong><br><strong>一个程序如何分段在用户编程时决定</strong></li><li>内存分配规则：以段为单位进行分配，每个段在内存中占据连续空间，但各段之间可以不相邻<br><strong>分段不会产生内部碎片，会产生外部碎片</strong></li><li>分段的逻辑地址结构<ul><li>由段号（段名）和段内地址（段内偏移量）所组成。</li><li>段号的位数决定了每个进程最多可以分几个段</li><li>段内地址位数决定了每个段的最大长度是多少</li></ul></li><li>分段存储管理将程序按照逻辑段进行划分，有利于程序的动态链接</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/19/pP8n38K.png" alt="pP8n38K.png"></li></ul></li></ul></li><li><p>段表</p><ul><li>程序分多个段，各段离散地装入内存，为了保证程序能正常运行，就必须能从物理内存中找到各个逻辑段的存放位置。<br>需为每个进程建立一张段映射表，简称“段表”。</li><li>每个段对应一个段表项，其中记录了该段在内存中的起始位置（又称“基址”）和段的长度。</li><li>各个段表项的长度是相同的。由于段表项长度相同，因此段号可以是隐含的，不占存储空间。<br>若段表存放的起始地址为M，则K号段对应的段表项存放的地址为M+K*6（段表项长度为6字节）</li></ul></li><li><p>地址转换</p><ul><li><img src="https://s1.ax1x.com/2023/08/19/pP8n4P0.png" alt="pP8n4P0.png"></li></ul></li><li><p>分段地址变换机构</p><ul><li>过程<ul><li>①从逻辑地址A中取出前几位为段号S，后几位为段内偏移量W。<br>注意，在地址变换的题目中，要注意逻辑地址是用二进制数还是用十进制数给出的。</li><li>②比较段号S和段表长度M，若S≥M，则产生越界中断，否则继续执行。</li><li>③段表中段号S对应的段表项地址=段表始址F+段号S×段表项长度，取出该段表项的前几位得到段长C。<br>若段内偏移量≥C，则产生越界中断，否则继续执行。</li><li>④取出段表项中该段的始址b，计算E=b+W，用得到的物理地址E去访问内存。</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/19/pP8uVit.png" alt="pP8uVit.png"></li></ul></li></ul></li><li><p>分页和分段的对比</p><ul><li><strong>页是信息的物理单位</strong>。分页的主要目的是为了实现离散分配，提高内存利用率。分页仅仅是系统管理上的需要，完全是系统行为，<strong>对用户是不可见的</strong>。</li><li><strong>段是信息的逻辑单位</strong>。分段的主要目的是更好地满足用户需求。一个段通常包含着一组属于一个逻辑模块的信息。<strong>分段对用户是可见的</strong>，用户编程时需要显式地给出段名。</li><li><strong>分页会产生内部碎片，不会产生外部碎片（段页式有内部碎片）</strong><br><strong>分段会产生外部碎片，不会产生内部碎片</strong></li><li><strong>页的大小固定且由系统决定。段的长度却不固定，决定于用户编写的程序</strong>。</li><li><strong>分页的用户进程地址空间是一维的</strong>，程序员只需给出一个记忆符即可表示一个地址。</li><li><strong>分段的用户进程地址空间是二维的</strong>，程序员在标识一个地址时，既要给出段名，也要给出段内地址。</li><li><strong>分段比分页更容易实现信息的共享和保护</strong>。<ul><li>在分段系统中，段的共享是通过两个作业的段表中相应表项指向被共享的段的同一个物理副本来实现的。</li><li>分段管理的保护方法主要有两种：一种是存取控制保护，另一种是地址越界保护。<ul><li>地址越界保护将段表寄存器中的段表长度与逻辑地址中的段号比较，若段号大于段表长度，则产生越界中断</li><li>再将段表项中的段长和逻辑地址中的段内偏移进行比较，若段内偏移大于段长，也会产生越界中断。</li><li>分页管理只需要判断页号是否越界，页内偏移是不可能越界的。</li></ul></li><li>关于可重入程序<ul><li>当一个作业正从共享段中读取数据时，必须防止另一个作业修改此共享段中的数据。</li><li><strong>不能被修改的代码</strong>称为纯代码或<strong>可重入代码</strong>（不属于临界资源），可重入代码<strong>是可以共享的</strong>。</li><li><strong>可修改的代码是不能共享的</strong>（比如，有一个代码段中有很多变量，各进程并发地同时访问可能造成数据不一致）</li></ul></li><li>可重入程序主要是通过共享来使用同一块存储空间的，或通过动态链接的方式将所需的程序映射到相关进程中去<br>可以<strong>减少对程序段的调入与调出，因此减少了对换数量</strong></li></ul></li><li>访问一个逻辑地址需要几次访存？<ul><li>分页（单级页表）：第一次访存一一查内存中的页表，第二次访存一一访问目标内存单元。总共两次访存</li><li>分段：第一次访存一一查内存中的段表，第二次访存一一访问目标内存单元。总共两次访存</li><li>与分页系统类似，分段系统中也可以引入快表机构，将近期访问过的段表项放到快表中，这样可以少一次访问，加快地址变换速度。</li></ul></li></ul></li></ul><h4 id="6-段页式管理"><a href="#6-段页式管理" class="headerlink" title="6.段页式管理"></a>6.段页式管理</h4><ul><li>分页与分段的优缺点分析<ul><li>分页管理<ul><li>内存空间利用率高，不会产生外部碎片，只会有少量的页内碎片</li><li>不方便按照逻辑模块实现信息的共享和保护</li></ul></li><li>分段管理<ul><li>很方便按照逻辑模块实现信息的共享和保护，不会产生内部碎片</li><li>如果段长过大，为其分配很大的连续空间会很不方便。另外，段式管理会产生外部碎片</li></ul></li><li>段页式管理会产生内部碎片</li></ul></li><li><p>段页式管理</p><ul><li>段页式系统中，进程首先划分为段，每段再进一步划分为页，每个进程一张段表，每个段一张页表</li><li>将地址空间按照程序自身的逻辑关系划分为若干个段，在将各段分为大小相等的页面</li><li>将内存空间分为与页面大小相等的一个个内存块，系统以块为单位为进程分配内存</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/19/pP8MMUs.png" alt="pP8MMUs.png"></li></ul></li></ul></li><li><p>段页式管理的逻辑地址结构</p><ul><li>段页式系统的逻辑地址结构由段号、页号、页内地址（页内偏移量）组成。<ul><li><img src="https://s1.ax1x.com/2023/08/19/pP8MBP1.png" alt="pP8MBP1.png"></li></ul></li><li>段号的位数决定了每个进程最多可以分几个段，页号位数决定了每个段最大有多少页</li><li>页内偏移量决定了页面大小、内存块大小是多少</li><li>若系统是按字节寻址的，则段号占16位，因此在该系统中，每个进程最多有$2^{16}=64K$个段</li><li>页号占4位，因此每个段最多有$2^4=16$页<br>页内偏移量占12位，因此每个页面/每个内存块大小为$2^{12}=4096=4KB$</li></ul></li><li>段页式管理中的段表与页表<ul><li>每个段对应一个段表项，每个段表项由段号、页表长度、页表存放块号（页表起始地址）组成。<br>每个段表项长度相等，段号是隐含的。</li><li>每个页面对应一个页表项，每个页表项由页号、页面存放的内存块号组成。每个页表项长度相等，页号是隐含的。</li><li>图片<ul><li><img src="https://s21.ax1x.com/2024/08/12/pApQw3q.png" alt="pApQw3q.png"></li></ul></li></ul></li><li>段页式管理的逻辑地址转换<ul><li>过程（需进行三次访存，也可用快表解决）<ul><li>1.由逻辑地址得到段号、页号、页内偏移量</li><li>2.段号与段表寄存器中的段长度比较，检查是否越界</li><li>3.由段表始址、段号找到对应段表项</li><li>4.根据段表中记录的页表长度，检查页号是否越界</li><li>5.由段表中的页表地址、页号得到查询页表，找到相应页表项</li><li>6.由页面存放的内存块号、页内偏移量得到最终的物理地址</li><li>7.访问目标单元</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/20/pP8dOXj.png" alt="pP8dOXj.png"></li></ul></li></ul></li></ul><h3 id="二-虚拟内存管理（✪）"><a href="#二-虚拟内存管理（✪）" class="headerlink" title="二.虚拟内存管理（✪）"></a>二.虚拟内存管理（✪）</h3><h4 id="1-虚拟内存的基本概念"><a href="#1-虚拟内存的基本概念" class="headerlink" title="1.虚拟内存的基本概念"></a>1.虚拟内存的基本概念</h4><ul><li>传统存储管理方式的缺点<ul><li>许多在程序运行中不用或暂时不用的程序（数据）占据了大量的内存空间，而一些需要运行的作业又无法装入运行，显然浪费了宝贵的内存资源。</li><li>一次性<ul><li><strong>作业必须一次性全部装入内存后，才能开始运行</strong>。</li><li>当作业很大而不能全部被装入内存时，将使该作业无法运行</li><li>当大量作业要求运行时，由于内存不足以容纳所有作业，只能使少数作业先运行，导致多道程序度的下降。</li></ul></li><li>驻留性<ul><li><strong>作业被装入内存后，就一直驻留在内存中，其任何部分都不会被换出，直至作业运行结束</strong>。</li><li>运行中的进程会因等待I/O而被阻塞，可能处于长期等待状态。</li></ul></li></ul></li><li><p>虚拟内存的概述</p><ul><li><p>概述</p><ul><li>虚拟内存在物理上扩展内存相对有限的条件下，在逻辑上扩充内存，实际的物理内存大小没有变</li><li><strong>虚拟内存使用外存上的空间来扩充内存空间，通过一定的换入/换出，使得整个系统在逻辑上能够使用一个远远超出其物理内存大小的内存容量。</strong></li><li>因为虚拟内存技术调换页面时需要访问外存，会导致平均访存时间增加，若使用了不合适的替换算法，则会大大降低系统性能。</li></ul></li><li><p>虚拟内存空间的大小限制（区分实际容量和最大容量的决定因素）</p><ul><li>虚存的<strong>实际容量</strong> ≤ 内存容量和外存容量之和<ul><li>这是硬件的硬性条件规定的，若虚存的实际容量超过了这个容量，则没有相应的空间来供虚存使用。</li></ul></li><li>虚存的<strong>最大容量</strong> ≤ 计算机的地址位数能容纳的最大容量<ul><li><strong>虚拟内存的最大容量由计算机的地址结构决定</strong></li><li>假设地址是32位的，按字节编址，一个地址代表1B存储空间，则虚存的最大容量≤4GB($2^{32}$B)。</li><li>这是因为若虚存的最大容量超过4GB，则32位的地址将无法访问全部虚存，也就是说4GB以后的空间被浪费了，相当于没有一样，没有任何意义。</li></ul></li><li>实际虚存的容量是取条件以上两种情况的交集，即两个条件都要满足，仅满足一个条件是不行的。</li></ul></li></ul></li><li>虚拟内存的特征<ul><li>多次性：无需在作业运行时一次性全部装入内存，而是允许被分成多次调入内存。</li><li>对换性：在作业运行时无需一直常驻内存，而是允许在作业运行过程中，将作业换入、换出。</li><li>虚拟性：从逻辑上扩充了内存的容量，使用户看到的内存容量，远大于实际的容量。</li><li>注<strong>：还有离散性（基于非连续分配方式）</strong></li></ul></li><li><p>虚拟内存技术的实现（<strong>只能基于非连续分配的技术</strong>）</p><ul><li><p>虚拟内存管理方式的基础</p><ul><li>虚拟内存的实现需要建立在<strong>离散分配</strong>的内存管理方式的基础上</li><li>局部性原理（<strong>虚拟存储管理系统的基础，实现多次性</strong>）<ul><li>在程序装入时，可以将程序中很快会用到的部分装入内存，暂时用不到的部分留在外存，就可以让程序开始执行。</li><li>概念<ul><li>时间局部性：如果执行了程序中的某条指令，那么不久后这条指令很有可能再次执行；<br>如果某个数据被访问过，不久之后该数据很可能再次被访问。（因为程序中存在大量的循环）</li><li>空间局部性：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也很有可能被访问。<br>(因为很多数据在内存中都是连续存放的，并且程序的指令也是顺序地在内存中存放的)</li><li>高速缓存技术：使用频繁的数据放到更高速的存储器中</li></ul></li></ul></li></ul></li><li><p>虚拟内存技术的三个方式：</p><ul><li>请求分页存储管理</li><li>请求分段存储管理</li><li>请求段页式存储管理</li></ul></li><li>实现虚拟内存中操作系统提供的功能<ul><li>请求调页（或请求调段）功能<ul><li><strong>在程序执行过程中，当所访问的信息不在内存时，由操作系统负责将所需信息从外存调入内存，然后继续执行程序。</strong></li></ul></li><li>页面置换（或段置换）的功能<ul><li><strong>若内存空间不够，由操作系统负责将内存中暂时用不到的信息换出到外存。</strong></li></ul></li></ul></li><li>实现虚拟内存的硬件支持<ul><li>一定容量的内存和外存。</li><li>页表机制（或段表机制），作为主要的数据结构。</li><li>中断机构，当用户程序要访问的部分尚未调入内存时，则产生中断。</li><li>地址变换机构，逻辑地址到物理地址的变换。</li></ul></li></ul></li></ul><h4 id="2-缺页中断"><a href="#2-缺页中断" class="headerlink" title="2.缺页中断"></a>2.缺页中断</h4><ul><li>缺页中断是访存指令引起的，说明所要访问的页面不在内存中，进行缺页中断处理并调入所要访问的页后，<strong>访存指令显然应该重新执行被中断的那一条指令</strong></li><li>关于缺页中断的次数<ul><li><strong>缺页中断的次数等于其访问的页帧数。若页面尺寸增大而可容纳的最大页数不变，存放程序需要的页帧数就会减少，因此缺页中断的次数也会减少</strong>。</li><li><strong>在请求分页存储管理中，若采用FIFO页面置换算法，则当可供分配的页帧数增加时，缺页中断的次数可能增加也可能减少</strong><br><strong>其它页面置换算法的缺页中断次数则会随着可供分配的页帧数的增加而减少</strong></li><li>无论采用什么页面置换算法，每种页面第一次访问时不可能在内存中，必然发生缺页，所以缺页次数大于或等于页号数量。</li></ul></li><li>影响缺页率的因素<ul><li>页置换算法<ul><li>页缓冲队列是将淘汰的页面缓存下来，暂时不写回磁盘，队列长度会影响页面置换的长度，但是不会影响缺页率</li></ul></li><li>工作集的大小</li><li>进程的数量</li></ul></li></ul><h4 id="3-请求分页管理方式"><a href="#3-请求分页管理方式" class="headerlink" title="3.请求分页管理方式"></a>3.请求分页管理方式</h4><ul><li><p>引入的原因</p><ul><li><p>与基本分页管理相比，请求分页管理中，为了实现“请求调页”，操作系统需要知道每个页面是否已经调入内存；如果还没调入，那么也需要知道。并且还需要页面在外存中存放的位置</p></li><li><p>当内存空间不够时，要实现“页面置换”，操作系统需要通过某些指标来决定到底换出哪个页面；有的页面没有被修改过，就不用再浪费时间写回外存。有的页面修改过，就需要将外存中的旧数据覆盖，因此，需要记录各个页面是否被修改的信息。</p></li></ul></li><li><p>请求分页存储管理的主要特点</p><ul><li>请求分页存储管理就是为了解决内存容量不足而使用的方法，它基于局部性原理实现了以时间换取空间的目的。</li><li><strong>主要特点是间接扩充了内存</strong>。</li></ul></li><li><p>请求页表项的结构</p><ul><li>新增四个字段<ul><li>状态位P（合法位）。<strong>用于指示该页是否已调入内存，供程序访问时参考</strong>。<br><strong>决定了是否会发生页面故障</strong></li><li>访问字段A。用于记录本页在一段时间内被访问的次数，或记录本页最近已有多长时间未被访问，<br><strong>供置换算法换出页面时参考</strong>。</li><li>修改位M。<strong>标识该页在调入内存后是否被修改过，以确定页面置换时是否写回外存</strong>。</li><li>外存地址。用于指出该页在外存上的地址，通常是物理块号，供调入该页时参考。</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/22/pPGOmKU.png" alt="pPGOmKU.png"></li></ul></li></ul></li><li><p>缺页中断机构</p><ul><li>过程<ul><li>在请求分页系统中，每当要访问的页面不在内存时，便产生一个缺页中断，然后由操作系统的缺页中断处理程序处理中断</li><li>此时缺页的进程阻塞，放入阻塞队列，调页完成后再将其唤醒，放回就绪队列。</li><li>如果内存中有空闲块，则为<strong>进程分配一个空闲块（分配页框）</strong>，将所缺页面从外存装入该块（<strong>磁盘I/O</strong>），并<strong>修改页表中相应的页表项</strong>。</li><li>如果内存中没有空闲块，则由页面置换算法选择一个页面淘汰，若该页面在内存期间被修改过，则要将其写回外存。<br>未修改过的页面不用写回外存。</li></ul></li><li>特点<ul><li>在指令执行期间而非一条指令执行完后产生和处理中断信号，属于内部异常（故障）。</li><li>一条指令在执行期间，可能产生多次缺页中断。</li></ul></li></ul></li><li><p>地址变换机构</p><ul><li>与基本分页存储管理的不同之处<ul><li>需要检查所需页面是否在内存中，查看快表和请求页表</li><li>若页面不在内存中则产生缺页中断，需要请求调页</li><li>若内存空间不够，还需换出页面</li><li>页面调入内存后，需要修改相应页表项</li></ul></li><li>基本流程<ul><li><img src="https://s1.ax1x.com/2023/08/22/pPGOub4.png" alt="pPGOub4.png"></li></ul></li><li>注意事项<ul><li>在具有快表机构的请求分页系统中，访问一个逻辑地址时，若发生缺页，则地址变换步骤是<ul><li>查快表（未命中）一一查慢表（发现未调入内存）一一调页（调入的页面对应的表项会直接加入快表）一一查快表（命中）一一访问目标内存单元</li></ul></li><li>①只有“写指令”才需要修改“修改位”。并且，一般来说只需修改快表中的数据，只有要将快表项删除时才需要写回内存中的慢表。这样可以减少访存次数。</li><li>②和普通的中断处理一样，缺页中断处理依然需要保留CPU现场。</li><li>③需要用某种“页面置换算法”来决定一个换出页面</li><li>④换入/换出页面都需要启动慢速的I/O操作，可见，如果换入/换出太频繁，会有很大的开销</li><li>⑤页面调入内存后，需要修改慢表，同时也需要将表项复制到快表中。</li><li>⑥快表中有的页面一定是在内存中的。若某个页面被换出外存则快表中的相应表项也要删除，否则可能访问错误的页面</li></ul></li></ul></li></ul><h4 id="4-页框分配"><a href="#4-页框分配" class="headerlink" title="4.页框分配"></a>4.页框分配</h4><ul><li>驻留集<ul><li><strong>指请求分页存储管理中给进程分配的物理块的集合</strong>。</li><li>在采用了虚拟存储技术的系统中，驻留集大小一般小于进程的总大小。</li><li>若驻留集太小，会导致缺页频繁，系统要花大量的时间来处理缺页，实际用于进程推进的时间很少：</li><li>驻留集太大，又会导致多道程序并发度下降，资源利用率降低。所以应该选择一个合适的驻留集大小。</li></ul></li><li>分配策略<ul><li>固定分配：操作系统为每个进程分配一组固定数目的物理块，在进程运行期间不再改变。<strong>即驻留集大小不变</strong></li><li>可变分配：先为每个进程分配一定数目的物理块，在进程运行期间，可根据情况做适当的增加或减少。即驻留集大小可变</li></ul></li><li>置换策略<ul><li>局部置换：发生缺页时只能选进程自己的物理块进行置换。</li><li>全局置换：可以将操作系统保留的空闲物理块分配给缺页进程，也可以将别的进程持有的物理块置换到外存，再分配给缺页进程。<ul><li>全局置换意味着一个进程拥有的物理块数量必然会改变，因此不可能是固定分配</li></ul></li></ul></li><li>三种内存分配置换策略<ul><li>固定分配局部置换<ul><li>系统为每个进程分配一定数量的物理块，在整个运行期间都不改变。若进程在运行中发生缺页，则只能从该进程在内存中的页面中选出一页换出，然后再调入需要的页面。</li><li>缺点：很难在刚开始就确定应为每个进程分配多少个物理块才算合理。</li><li>采用这种策略的系统可以根据进程大小、优先级、或是根据程序员给出的参数来确定为一个进程分配的内存块数</li></ul></li><li>可娈分配全局置换<ul><li>过程<ul><li>刚开始会为每个进程分配一定数量的物理块。操作系统会保持一个空闲物理块队列。</li><li>当某进程发生缺页时，从空闲物理块中取出一块分配给该进程；</li><li>若已无空闲物理块，则可选择一个未锁定的页面换出外存，再将该物理块分配给缺页的进程。<ul><li>系统会锁定一些页面，这些页面中的内容不能置换出外存（如：重要的内核数据可以设为锁定）</li></ul></li></ul></li><li>缺点：采用这种策略时，只要某进程发生缺页，都将获得新的物理块，仅当空闲物理块用完时，系统才选择一个未锁定的页面调出。<br><strong>被选择调出的页可能是系统中任何一个进程中的页，因此这个被选中的进程拥有的物理块会减少，缺页率会增加</strong>。</li></ul></li><li>可变分配局部置换<ul><li>刚开始会为每个进程分配一定数量的物理块。当某进程发生缺页时，只允许从该进程自己的物理块中选出一个进行换出外存。</li><li>如果进程在运行中频繁地缺页，系统会为该进程多分配几个物理块，直至该进程缺页率趋势适当程度；<br>反之，如果进程在运行中缺页率特别低，则可适当减少分配给该进程的物理块。</li><li>区别<ul><li><strong>可变分配全局置换：只要缺页就给分配新物理块</strong></li><li><strong>可变分配局部置换：要根据发生缺页的频率来动态地增加或减少进程的物理块</strong></li></ul></li></ul></li></ul></li><li>页面调入的时机<ul><li>预调页策略<ul><li>根据局部性原理，一次调入若干个相邻的页面可能比一次调入一个页面更高效。但如果提前调入的页面中大多数都没被访问过，则测又是低效的。（主要是空间局部性原理）<br>因此可以预测不久之后可能访问到的页面，将它们预先调入内存，但目前预测成功率只有50%左右</li><li><strong>这种策略主要用于进程的首次调入，由程序员指出应该先调入哪些部分，在运行前调入</strong></li></ul></li><li>请求调页策略<ul><li>进程在运行期间发现缺页时才将所缺页面调入内存。由这种策略调入的页面一定会被访问到，在运行时调入</li><li><strong>但由于每次只能调入一页，而每次调页都要磁盘I/O操作，因此I/O开销较大。</strong></li></ul></li></ul></li><li>页面调入的区域<ul><li>外存中的交换区与文件区<ul><li>交换区：读/写速度更快，采用连续分配方式</li><li>文件区：读/写速度更慢，采用离散分配方式</li><li>若交换区的磁盘利用率过高，说明物理内存非常短缺，此时的处理策略如下<ul><li><strong>增大物理内存容量</strong>，使每个程序得到更多的页框，能减少缺页率，进而减少换入换出过程，提高CPU的利用率</li><li><strong>减少多道程序的度数</strong>，可减少主存中的进程数，减少换入换出频率，可以提高CPU的利用率</li><li><strong>使用更快的磁盘交换区和更快速的CPU没有用</strong></li></ul></li></ul></li><li>三种调入调出区域处理<ul><li>系统拥有足够的对换区空间<ul><li><strong>在进程运行前，需将进程相关的数据从文件区复制到对换区</strong>。</li><li><strong>页面的调入、调出都是在内存与对换区之间进行，这样可以保证页面的调入、调出速度很快</strong>。</li></ul></li><li>系统缺少足够的对换区空间<ul><li><strong>凡是不会被修改的数据都直接从文件区调入，由于这些页面不会被修改，因此换出时不必写回磁盘，下次需要时再从文件区调入即可。</strong></li><li><strong>对于可能被修改的部分，换出时需写回磁盘对换区，下次需要时再从对换区调入</strong>。</li></ul></li><li>UNIX方式<ul><li><strong>运行之前进程有关的数据全部放在文件区，故未使用过的页面，都可从文件区调入</strong>。</li><li><strong>若被使用过的页面需要换出，则写回对换区，下次需要时从对换区调入</strong>。</li></ul></li></ul></li><li>页面调入的方法<ul><li>当进程所访问的页面不在内存中时(存在位为0)，便向CPU发出缺页中断，中断响应后便转入缺页中断处理程序。</li><li>该程序通过查找页表得到该页的物理块，此时如果内存未满，则启动磁盘I/O，将所缺页调入内存，并修改页表。</li><li>如果内存已满，则先按某种置换算法从内存中选出一页准备换出<ul><li>如果该页未被修改过(修改位为0)，则无须将该页写回磁盘</li><li>如果该页已被修改(修改位为1)，则必须将该页写回磁盘</li></ul></li><li>然后将所缺页调入内存，并修改页表中的相应表项，置其存在位为1。<br>调入完成后，进程就可利用修改后的页表形成所要访问数据的内存地址。</li></ul></li></ul></li></ul><h4 id="5-页面置换算法（♚）"><a href="#5-页面置换算法（♚）" class="headerlink" title="5.页面置换算法（♚）"></a>5.页面置换算法（♚）</h4><ul><li>页面置换的概述<ul><li>进程运行时，若其访问的页面不在内存中而需将其调入，但内存已无空闲空间时，就需要从内存中调出一页程序或数据，送入磁盘的对换区。</li><li>选择调出页面的算法就称为页面置换算法。好的页面置换算法应有较低的页面更换频率，也就是说，应将以后不会再访问或以后较长时间内不会再访问的页面先调出。</li></ul></li><li>最佳置换算法（OPT）<ul><li>概述<ul><li>选择的被淘汰页面是以后永不使用的页面，或是在最长时间内不再被访问的页面，以便保证获得最低的缺页率。</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/22/pPGjbE4.png" alt="pPGjbE4.png"></li></ul></li><li>特点<ul><li>最佳置换算法可以保证最低的缺页率</li><li><strong>但实际上，操作系统无法提前预判页面访问序列。因此，最佳置换算法是无法实现的。但可利用该算法去评价其他算法</strong></li></ul></li></ul></li><li>先进先出页面置换算法（FIFO）<ul><li>概述<ul><li><strong>每次选择淘汰的页面是最早进入内存的页面</strong></li><li><strong>把调入内存的页面根据调入的先后顺序排成一个队列，需要换出页面时选择队头页面即可。</strong><br><strong>队列的最大长度取决于系统为进程分配了多少个内存块</strong>。</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/22/pPGjXCR.png" alt="pPGjXCR.png"></li></ul></li><li>特点<ul><li>Belady异常一一当为进程分配的物理块数增大时，缺页次数不减反增的异常现象。<strong>只有FIFO算法会产生Belady异常</strong>。</li><li>FIFO算法虽然实现简单，但是该算法与进程实际运行时的规律不适应，因为先进入的页面也有可能最经常被访问。<br><strong>算法性能差</strong></li></ul></li></ul></li><li>最近最久未使用置换算法（LRU）<ul><li>概述<ul><li><strong>每次淘汰的页面是最近最久未使用的页面</strong></li><li>赋予每个页面对应的页表项中，用访问字段记录该页面自上次被访问以来所经历的<strong>时间t</strong></li><li><strong>当需要淘汰一个页面时，选择现有页面中访问值最大的，即最近最久未使用的页面（进行排序）。</strong></li><li>在手动做题时，若需要淘汰页面，可以逆向检查此时在内存中的几个页面号。在逆向扫描过程中最后一个出现的页号就是要淘汰的页面。</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/22/pPGjvgx.png" alt="pPGjvgx.png"></li></ul></li><li>特点<ul><li>该算法的实现需要专门的硬件支持，虽然<strong>算法性能好，但是实现困难，开销大（需要对所有页进程排序）</strong></li></ul></li></ul></li><li><p>时钟置换算法（CLOCK）</p><ul><li>简单CLOCK置换算法（又称最近未用算法_NRU）<ul><li>概述<ul><li>为每个页面设置一个访问位，将内存中的所有页面视为一个链式循环队列，并有一个替换指针与之相关联。<br><strong>访问位为1，表示最近访问过，访问位为0，表示最近没访问过</strong></li><li><strong>当某页首次被装入或被访问时，其访问位置为1，访问时在内存中存在页面则指针不动</strong></li><li><strong>在选择一页淘汰时，只需检查页的访问位</strong>。<ul><li><strong>若为0，就选择该页换出</strong></li><li><strong>若为1，则将它置为0，暂不换出，给予该页第二次驻留内存的机会，再依次顺序检查下一个页面。</strong></li><li><strong>当检查到队列中的最后一个页面时，若其访问位仍为1，则返回到队首去循环检查。</strong></li></ul></li><li><strong>当某一页被替换时，该指针被设置指向被替换页面的下一页。</strong></li></ul></li><li>图片<ul><li>访问序列：7、0、1、2、0、3、0、4<br><img src="https://s1.ax1x.com/2023/08/22/pPGvZxP.png" alt="pPGvZxP.png"></li></ul></li><li>特点<ul><li>若第一轮扫描中所有页面都是1，则将这些页面的访问位依次置为0后，再进行第二轮扫描</li><li><strong>第二轮扫描中一定会有访问位为0的页面，因此简单的CLOCK算法选择一个淘汰页面最多会经过两轮扫描</strong></li></ul></li></ul></li><li><p>改进的时钟置换算法</p><ul><li>改进位与修改位<ul><li><strong>简单的时钟置换算法仅考虑到一个页面最近是否被访问过。如果被淘汰的页面没有被修改过，就不需要执行I/O操作写回外存。只有被淘汰的页面被修改过时，才需要写回外存</strong>。</li><li>因此，除了考虑一个页面最近有没有被访问过之外，操作系统还应考虑页面有没有被修改过。在其他条件都相同时，应优先淘汰没有修改过的页面，避免I/O操作。</li><li>修改位=0，表示页面没有被修改过；修改位=1，表示页面被修改过。</li><li>用(访问位，修改位)的形式表示各页面状态。如(1,1)表示一个页面近期被访问过，且被修改过。</li></ul></li><li><p>规则</p><ul><li>将所有可能被置换的页面排成一个循环队列</li><li><p>第一轮：从当前位置开始扫描到第一个(0,0)的帧用于替换。本轮扫描不修改任何标志位</p><ul><li>第一优先级：最近没访问且没修改的页面</li></ul></li><li>第二轮：若第一轮扫描失败，则重新扫描，查找第一个(0,1)的帧用于替换。本轮将所有扫描过的帧访问位设为0<ul><li>第二优先级：最近没访问但修改过的页面</li></ul></li><li>第三轮：若第二轮扫描失败，则重新扫描，查找第一个(0,0)的帧用于替换。本轮扫描不修改任何标志位<ul><li>第三优先级：最近访问过但没修改的页面</li></ul></li><li>第四轮：若第三轮扫描失败，则重新扫描，查找第一个(0,1)的帧用于替换。<ul><li>第四优先级：最近访问过且修改过的页面</li></ul></li><li>由于第二轮已将所有帧的访问位设为0，因此经过第三轮、第四轮扫描一定会有一个帧被选中，<strong>因此改进型CLOCK置换算法选择一个淘汰页面最多会进行四轮扫描</strong></li></ul></li></ul></li><li>例<ul><li>首先处理这个16进程数，后缀为H，1个数字为4位，此时前后12位为偏移量（页面大小4KB），此时页号为02，在表中存在位为0需要从外存调入，此时内存大小不够采用置换算法优先置换页号为3的（1,0），此时选C<br><img src="https://s1.ax1x.com/2023/08/22/pPJFMZt.png" alt="pPJFMZt.png"></li></ul></li></ul></li><li>四种算法的比较<ul><li><img src="https://s1.ax1x.com/2023/08/22/pPGvgsK.jpg" alt="pPGvgsK.jpg"></li></ul></li></ul><h4 id="6-抖动与工作集"><a href="#6-抖动与工作集" class="headerlink" title="6.抖动与工作集"></a>6.抖动与工作集</h4><ul><li>抖动（颠簸，高缺页率）<ul><li>刚刚换出的页面马上又要换入内存，刚刚换入的页面马上又要换出外存，这种频繁的页面调度行为称为抖动，或颠簸。</li><li>频繁的置换页面导致大部分时间都花在页面置换之上，导致系统性能下降</li><li><strong>产生抖动的主要原因是<u>页面置换算法不合理</u>，进程频繁访问的页面数目高于可用的物理块数</strong></li><li>撤销部分进程可以防止产生抖动</li></ul></li><li>驻留集与工作集<ul><li><strong>驻留集：指请求分页存储管理中给进程分配的内存块的集合。</strong></li><li><strong>工作集：指在某段时间间隔里，进程<u>实际访问</u>页面的集合。</strong></li><li><strong>驻留集大小需要大于工作集大小，否则进程运行过程中将频繁缺页</strong></li></ul></li><li>工作集的特点<ul><li>在进程运行时，若其工作集页面都在主存储器内，则能够使该进程有效地运行，否则会出现频繁的页面调入调出现象</li><li>操作系统会根据“窗口尺寸”来算出工作集，工作集大小可能小于窗口尺寸<ul><li><img src="https://s1.ax1x.com/2023/08/22/pPGXwOU.png" alt="pPGXwOU.png"></li></ul></li><li>实际应用中，操作系统可以统计进程的工作集大小，根据工作集大小给进程分配若干内存块。<ul><li>如窗口尺寸为5，经过一段时间的监测发现某进程的工作集最大为3，</li><li>那么说明该进程有很好的局部性，可以给这个进程分配3个以上的内存块即可满足进程的运行需要。</li></ul></li><li>基于局部性原理可知，进程在一段时间内访问的页面与不久之后会访问的页面是有相关性的。<br>因此，可以根据进程近期访问的页面集合（工作集）来设计一种页面置换算法一一选择一个不在工作集中的页面进行淘汰。</li></ul></li></ul><h4 id="7-内存映射文件"><a href="#7-内存映射文件" class="headerlink" title="7.内存映射文件"></a>7.内存映射文件</h4><ul><li><p>概述</p><ul><li>将磁盘文件的全部或部分内容与进程虚拟地址空间的某个区域建立映射关系，便可以直接访问被映射的文件，<br>而不必执行文件I/O操作，也无须对文件内容进行缓存处理。这种特性非常适合用来管理大尺寸文件。</li><li><strong>进程可使用系统调用，请求操作系统将文件映射到进程的虚拟地址空间，以访问内存的方式读写文件</strong></li><li><strong>进程关闭文件时，操作系统负责将文件数据写回磁盘，并解除内存映射</strong></li><li><strong>多个进程可以映射同一个文件，方便共享</strong></li></ul></li><li><p>传统的文件文件访问方式</p><ul><li><img src="https://s1.ax1x.com/2023/08/22/pPGxEo4.png" alt="pPGxEo4.png"></li></ul></li><li><p>内存映射文件访问方式</p><ul><li><img src="https://s1.ax1x.com/2023/08/22/pPGxuS1.png" alt="pPGxuS1.png"></li></ul></li><li><p>多个进程允许并发地内存映射同一个文件，实现数据共享</p><ul><li><img src="https://s1.ax1x.com/2023/08/22/pPGxQOK.png" alt="pPGxQOK.png"></li></ul></li><li><p>优点</p><ul><li>程序员编程更简单，已建立映射的文件，只需按访问内存的方式读写即可</li><li>文件数据的读入/写出完全由操作系统负责，I/O效率可以由操作系统负责优化</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;操作系统第三章-内存管理&quot;&gt;&lt;a href=&quot;#操作系统第三章-内存管理&quot; class=&quot;headerlink&quot; title=&quot;操作系统第三章 内存管理&quot;&gt;&lt;/a&gt;操作系统第三章 内存管理&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;计算机学科基础：操作系统第三章内存管理的学习笔记&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="计算机基础学习笔记" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="操作系统" scheme="http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>操作系统第二章-进程与线程</title>
    <link href="http://example.com/2024/08/12/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/"/>
    <id>http://example.com/2024/08/12/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/</id>
    <published>2024-08-11T17:32:32.000Z</published>
    <updated>2024-08-11T18:09:00.732Z</updated>
    
    <content type="html"><![CDATA[<h2 id="操作系统第二章-进程与线程"><a href="#操作系统第二章-进程与线程" class="headerlink" title="操作系统第二章 进程与线程"></a>操作系统第二章 进程与线程</h2><blockquote><p>计算机学科基础：操作系统第二章进程与线程的学习笔记</p></blockquote><span id="more"></span><h3 id="一-进程与线程（✪）"><a href="#一-进程与线程（✪）" class="headerlink" title="一.进程与线程（✪）"></a>一.进程与线程（✪）</h3><h4 id="1-进程的概念"><a href="#1-进程的概念" class="headerlink" title="1.进程的概念"></a>1.进程的概念</h4><ul><li>进程与程序<ul><li>程序：是静态的，一组有序的指令集合，是存放在磁盘里的可执行文件，如：QQ.exe</li><li>进程：是动态的，是程序的一次执行过程，如：可同时启动多次QQ程序<br>同一个程序多次执行会对应多个进程</li><li>进程是具有独立功能的程序在一个数据集合上运行的过程，它是系统进行资源分配和调度的一个独立单位</li></ul></li><li>进程实体<ul><li>进程实体又称进程映像，由程序段、相关数据段和PCB三部分构成<br>PCB是给操作系统用的。程序段、数据段是给进程自己用的。</li><li><strong>进程是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位</strong><br>进程实体是静态的，而进程则是动态的</li><li>PCB（进程控制块）<ul><li>为了使参与并发执行的每个程序（含数据）都能独立地运行，必须为之配置一个专门的<u>数据结构</u>，称为进程控制块 PCB</li><li><strong>PCB是进程存在的唯一标志</strong>，系统利用PCB来描述进程的基本情况和运行状态，进而控制和管理进程<br>当进程被创建时，操作系统为其创建PCB，当进程结束时，会回收其PCB</li></ul></li><li>程序段：程序的代码（指令序列）</li><li>数据段：运行过程中产生的各种数据  (如程序中定义的变量)</li></ul></li></ul><h4 id="2-进程的组成"><a href="#2-进程的组成" class="headerlink" title="2.进程的组成"></a>2.进程的组成</h4><ul><li>由PCB，程序段，数据段组成</li><li>PCB的组成<ul><li>进程描述信息<ul><li>进程标识符（PID）：标志各个进程，每个进程都有一个唯一的标识号</li><li>用户标识符（UID）：进程归属的用户，用户标识符主要为共享和保护服务</li></ul></li><li>进程控制和管理信息<ul><li>进程当前状态：描述进程的状态信息，作为处理机分配调度的依据</li><li>进程优先级：描述进程抢占处理机的优先级，优先级高的进程可优先获得处理机</li></ul></li><li>资源分配清单<ul><li>用于说明有关内存地址空间或虚拟地址空间的状况，所打开文件的列表和所使用的输入输出设备信息</li></ul></li><li>处理机相关信息<ul><li>也称处理机的上下文，主要指处理机中各寄存器的值。当进程处于执行态时，处理机的许多信息都在寄存器中。</li><li>当进程被切换时，处理机状态信息都必须保存在相应的PCB中，以便在该进程重新执行时，能从断点继续执行。</li></ul></li></ul></li></ul><h4 id="3-进程的特征"><a href="#3-进程的特征" class="headerlink" title="3.进程的特征"></a>3.进程的特征</h4><ul><li>动态性：是进程最基本的特征，进程是程序的一次执行过程，是动态地产生、变化和消亡的</li><li>并发性：内存中有多个进程实体，各进程可并发执行</li><li>独立性：进程是能独立运行、独立获得资源、独立接受调度的基本单位</li><li>异步性：各进程按各自独立的、不可预知的速度向前推进，操作系统要提供“进程同步机制”来解决异步问题</li><li>结构性：每个进程都会配置一个PCB。结构上看，进程由程序段、数据段、PCB组成</li></ul><h4 id="4-进程的状态与转换（♚）"><a href="#4-进程的状态与转换（♚）" class="headerlink" title="4.进程的状态与转换（♚）"></a>4.进程的状态与转换（♚）</h4><ul><li>进程的五状态（就绪态、运行态、阻塞态是进程的三种基本状态）<ul><li>创建态：进程正在被创建，操作系统为进程分配资源、初始化PCB</li><li>就绪态：进程已经具备运行条件，但由于没有空闲CPU，而暂时不能运行</li><li>运行态：进程占有CPU，并在CPU上运行</li><li>阻塞态（等待态）：在进程运行的过程中，可能会请求等待某个事件的发生（如等待某种系统资源的分配，或者等待其他进程的响应）<br>在这个事件发生之前，进程无法继续往下执行，此时操作系统会让这个进程下CPU，并让它进入“阻塞态”</li><li>终止态：进程正在从系统中撤销，操作系统会回收进程拥有的资源、撤销PCB</li></ul></li><li>进程状态之间的转换<ul><li>就绪态一&gt;运行态：进程被调度</li><li>运行态一&gt;就绪态：时间片到，或CPU被其他高优先级的进程抢占</li><li>运行态一&gt;阻塞态：等待系统资源分配，或等待某事件发生（主动行为）</li><li>阻塞态一&gt;就绪态：资源分配到位，等待的事件发生（被动行为）</li><li>创建态一&gt;就绪态：系统完成创建进程相关的工作</li><li>运行态一&gt;终止态：进程运行结束，或运行过程中遇到不可修复的错误、</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPKTpGR.png" alt="pPKTpGR.png"></li></ul></li></ul></li><li>进程的组织<ul><li>进程PCB中，会有一个变量state来表示进程的当前状态。如：1表示创建态、2表示就绪态、3表示运行态。<br>为了对同一个状态下的各个进程进行统一的管理，操作系统会将各个进程的PCB组织起来。</li><li>链接方式<ul><li>链接方式将同一状态的PCB链接成一个队列，不同状态对应不同的队列；<br>也可把处于阻塞态的进程的PCB，根据其阻塞原因的不同，排成多个阻塞队列</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPKHFED.png" alt="pPKHFED.png"></li></ul></li></ul></li><li>索引方式<ul><li>索引方式将同一状态的进程组织在一个索引表中，索引表的表项指向相应的PCB，<br>不同状态对应不同的索引表，如就绪索引表和阻塞索引表等。</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPKHmvt.png" alt="pPKHmvt.png"></li></ul></li></ul></li></ul></li></ul><h4 id="5-进程控制（♚）"><a href="#5-进程控制（♚）" class="headerlink" title="5.进程控制（♚）"></a>5.进程控制（♚）</h4><ul><li>进程控制就是要实现进程状态转换，用原语实现<br>原语是一种特殊的程序，它的执行具有原子性也就是说，这段程序的运行必须一气呵成，不可中断</li><li>原语的实现（使用关中断指令与开中断指令这两个特权指令实现原语）<ul><li>正常情况下，CPU每执行完一条指令都会例行检查是否有中断信号需要处理，<br>如果有，则暂停运行当前这段程序，转而执行相应的中断处理程序。<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPK7E60.png" alt="pPK7E60.png"></li></ul></li><li>CPU执行了关中断指令之后，就不再例行检查中断信号，直到执行开中断指令之后才会恢复检查。<br>此时关中断、开中断之间的这些指令序列就是不可被中断的，这就实现了“原子性”<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPK7m0U.png" alt="pPK7m0U.png"></li></ul></li></ul></li><li>进程控制相关的原语（♚）<ul><li>进程的创建<ul><li>创建原语的过程<ul><li>为新进程分配一个唯一的PID，并申请空白PCB</li><li>为新进程分配所需资源，如内存、文件、I/O设备和CPU时间等（在PCB中体现）<ul><li>这些资源或从操作系统获得，或仅从其父进程获得。</li><li>如果资源不足（如内存），则并不是创建失败，而是处于创建态，等待内存资源。</li></ul></li><li>初始化PCB，主要包括初始化标志信息、初始化处理机状态信息和初始化处理机控制信息，以及设置进程的优先级等。</li><li>将PCB插入就绪队列，此时创建态→就绪态，等待被调度运行</li></ul></li><li>引起进程创立的事件<ul><li>用户登录：分时系统中，用户登录成功，系统会建立为其建立一个新的进程</li><li>作业调度：多道批处理系统中，有新的作业放入内存时，会为其建立一个新的进程</li><li>提供服务：用户向操作系统提出某些请求时，会新建一个进程处理该请求</li><li>应用请求：由用户进程主动请求创建一个子进程</li><li>启动程序执行</li></ul></li></ul></li><li>进程的终止<ul><li>可以使进程由就绪态/阻塞态/运行态→终止态→无</li><li>终止原语的过程<ul><li>根据被终止进程的标识符，检索出该进程的PCB，从中读出该进程的状态。</li><li>若被终止进程处于运行状态，立即终止该进程的执行，将处理机资源分配给其他进程</li><li>若该进程还有子孙进程，则应将其所有子孙进程终止。<br>进程间的关系是树形结构</li><li>将该进程所拥有的全部资源，或归还给其父进程，或归还给操作系统。</li><li>将该PCB从所在队列（链表）中删除。</li></ul></li><li>引起进程终止的过程<ul><li>正常结束：进程自己请求终止（exit系统调用）</li><li>异常结束：整数除以0、非法使用特权指令，然后被操作系统强行杀掉</li><li>外界干预：Ctrl+AIt+delete，用户选择杀掉进程</li></ul></li></ul></li><li>进程的阻塞与唤醒<ul><li>阻塞原语和唤醒原语必须成对使用</li><li>进程的阻塞<ul><li>阻塞是进程自身的一种主动行为，使用阻塞原语可以将运行态→阻塞态</li><li>阻塞原语的过程<ul><li>找到要阻塞的进程对应的PCB</li><li>保护进程运行现场，将PCB状态信息设置为“阻塞态”，暂时停止进程运行</li><li>将PCB插入相应事件的等待队列</li></ul></li><li>引起进程阻塞的事件<ul><li>需要等待系统分配某种资源（如申请临界资源，执行P(wait)操作）</li><li>需要等待相互合作的其他进程完成工作：等待I/O操作（如从磁盘读数据）</li></ul></li></ul></li><li>进程的唤醒<ul><li>将阻塞态转变为就绪态，因何事阻塞，就应由何事唤醒<ul><li>如退出临界区，I/O结束</li></ul></li><li>唤醒原语的过程<ul><li>在该事件等待队列中找到PCB</li><li>将PCB从等待队列移除，设置进程为就绪态</li><li>将PCB插入就绪队列，等待被调度</li></ul></li></ul></li></ul></li><li>进程的切换<ul><li>进程的切换可以将运行态→就绪态，就绪态→运行态</li><li>切换原语的过程<ul><li>将运行环境信息（进程上下文）存入PCB</li><li>将PCB移入相应队列</li><li>选择另一个进程执行，并更新其PCB</li><li>根据PCB恢复新进程所需的运行环境</li></ul></li><li>引起进程切换的事件<ul><li>当前进程时间片到</li><li>有更高优先级的进程到达</li><li>当前进程主动阻塞</li><li>当前进程终止</li></ul></li></ul></li></ul></li></ul><h4 id="6-进程的通信"><a href="#6-进程的通信" class="headerlink" title="6.进程的通信"></a>6.进程的通信</h4><ul><li>进程通信是指进程之间的信息交换。PV操作是低级通信方式，高级通信方式是指以较高的效率传输大量数据的通信方式</li><li>共享存储<ul><li>共享存储的概述<ul><li>进程是分配系统资源的单位（包括内存地址空间），因此各进程拥有的内存地址空间相互独立。<br>为了保证安全，一个进程不能直接访问另一个进程的地址空间，因此需要采用共享存储</li><li>设置一块可直接访问的共享空间，并映射到进程的虚拟地址空间<br>可通过对这片共享空间进行写/读操作实现进程之间的信息交换</li></ul></li><li>共享存储的特点<ul><li>为避免出错，各个进程对共享空间的访问应该是互斥的，各个进程可使用操作系统内核提供的同步互斥工具(如P、V操作)</li><li>操作系统只负责为通信进程提供可共享使用的存储空间和同步互斥工具，而数据交换则由用户自己安排读/写指令完成。</li><li>进程空间一般都是独立的，进程运行期间一般不能访问其他进程的空间，<br>想让两个进程共享空间，必须通过特殊的系统调用实现，而进程内的线程是自然共享进程空间的</li></ul></li><li>基于数据结构的共享（低级）<ul><li>比如共享空间里只能放一个长度为10的数组。这种共享方式速度慢、限制多，是一种低级通信方式</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPKbTTU.png" alt="pPKbTTU.png"></li></ul></li></ul></li><li>基于存储区的共享（高级）<ul><li>操作系统在内存中划出块共享存储区。数据的形式和存放位置都由通信进程控制，而不是操作系统。<br>这种共享方式速度很快，是一种高级通信方式。</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPKbfln.png" alt="pPKbfln.png"></li></ul></li></ul></li></ul></li><li>消息传递<ul><li>进程间的数据交换以格式化的消息(Message)为单位。进程通过操作系统提供的“发送消息/接收消息”两个<strong>原语</strong>进行数据交换</li><li>消息包括消息头与消息体，消息头包括：发送进程ID、接受进程ID、消息长度等格式化的信息</li><li>直接通信方式<ul><li>消息发送进程要指明接收进程的ID，发送进程直接把消息发送给接收进程，并将它挂在接收进程的消息缓冲队列上，<br>接收进程从消息缓冲队列中取得消息</li><li>图片：随后msg会移动到进程Q的地址空间上<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPKq1pj.png" alt="pPKq1pj.png"></li></ul></li></ul></li><li>间接通信方式<ul><li>发送进程把消息发送到某个中间实体，接收进程从中间实体取得消息<br>以“信箱”作为中间实体进行消息传递，广泛运用在计算机网络中</li><li>可以多个进程往同一个信箱send消息，也可以多个进程从同一个信箱中receive消息</li><li>图片：随后msg会移动到进程Q的地址空间上<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPKqB9J.png" alt="pPKqB9J.png"></li></ul></li></ul></li><li>在微内核操作系统中，微内核与服务器之间的通信就采用了消息传递机制。<br>由于该机制能很好地支持多处理机系统、分布式系统和计算机网络，因此也成为这些领域最主要的通信工具。</li></ul></li><li>管道通信<ul><li>“管道”是一个特殊的<strong>共享文件</strong>，又名pipe文件。其实就是在内存中开辟一个大小固定的内存缓冲区</li><li>写进程向管道的一端写，读进程从管道的另一端读。数据在管道中是先进先出的。</li><li>各进程要互斥地访问管道（由操作系统实现）</li><li>管道的特征<ul><li>从管道读数据是一次性操作，数据一旦被读取，就释放空间以便写更多数据。<br>普通管道只允许半双工通信，若要实现全双工通信，则需要定义两个管道。</li><li>只要管道非空，读进程就能从管道中读出数据，若数据被读空，则读进程阻塞，<br>直到写进程往管道中写入新的数据，再将读进程唤醒</li><li>只要管道不满，写进程就能往管道中写入数据，若管道写满，则写进程阻塞，<br>直到读进程读出数据，再将写进程唤醒</li><li>管道中的数据一旦被读出，就彻底消失。因此，当多个进程读同一个管道时，可能会错乱。<br>对此，通常有两种解决方案<ul><li><strong>①一个管道允许多个写进程，一个读进程</strong></li><li>②允许有多个写进程，多个读进程，但系统会让各个读进程轮流从管道中读数据</li></ul></li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPKLMb6.png" alt="pPKLMb6.png"></li></ul></li></ul></li></ul><h4 id="7-线程与多线程模型（♚）"><a href="#7-线程与多线程模型（♚）" class="headerlink" title="7.线程与多线程模型（♚）"></a>7.线程与多线程模型（♚）</h4><ul><li><p>线程的概念</p><ul><li><p>线程的基本概念</p><ul><li><strong>线程可以理解为“轻量级进程”，在引入线程后，线程是一个基本的CPU执行单元，也是程序执行流的最小单位</strong></li><li><strong>引入线程后，进程的内涵发生了改变，进程只作为除CPU外的系统资源的分配单元，而线程则作为处理机的分配单元。</strong><br>由于一个进程内部有多个线程，若线程的切换发生在同一个进程内部，则只需要很少的时空开销。</li></ul></li><li><p>引入线程的目的</p><ul><li><p>引入进程的目的是更好地使多道程序并发执行，提高资源利用率和系统吞吐量<br><strong>而引入线程的目的则是减小程序在并发执行时所付出的时空开销，提高操作系统的并发性能。</strong></p></li><li><p>引入线程之后，不仅是进程之间可以并发，进程内的各线程之间也可以并发，从而进一步提升了系统的并发度，<br>使得一个进程内也可以并发处理各种任务（如QQ视频、文字聊天、传文件）</p></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPKOUW4.png" alt="pPKOUW4.png"></li></ul></li></ul></li></ul></li><li><p>线程与进程的比较（♚）</p><ul><li>调度<ul><li><strong>在传统的进程机制中，拥有资源和独立调度的基本单位都是进程，每次调度都要进行上下文切换，开销较大。</strong></li><li><strong>在引入线程后，线程是独立调度的基本单位，而线程切换的代价远低于进程。</strong><ul><li><strong>此时进程仍然是资源分配的基本单位，而线程不是</strong></li></ul></li><li>在同一进程中，线程的切换不会引起进程切换。但从一个进程中的线程切换到另一个进程中的线程时，会引起进程切换。</li></ul></li><li>并发性<ul><li>在传统的进程机制中，只能进程间并发</li><li>在引入线程的操作系统中，不仅进程之间可以并发执行，而且一个进程中的多个线程之间亦可并发执行，甚至不同进程中的线程也能并发执行，从而使操作系统具有更好的并发性，提高了系统资源的利用率和系统的吞吐量。</li></ul></li><li>拥有资源<ul><li><strong>进程是系统中拥有资源的基本单位，而线程不拥有系统资源（仅有一点必不可少、能保证独立运行的资源）</strong><ul><li>若线程也是拥有资源的单位，则切换线程就需要较大的时空开销，线程这个概念的提出就没有意义。</li></ul></li><li><strong>线程可以与同属一个进程的其他线程共享进程所拥有的全部资源（可以访问其隶属进程的系统资源），</strong><br><strong>这主要表现在属于同一进程的所有线程都具有相同的地址空间</strong></li><li><strong>线程不能共享其他线程的资源</strong></li></ul></li><li>独立性<ul><li><strong>每个进程都拥有独立的地址空间和资源，除了共享全局变量，不允许其他进程访问</strong>。</li><li>某进程中的线程对其他进程不可见。同一进程中的不同线程是为了提高并发性及进行相互之间的合作而创建的，它们共享进程的地址空间和资源。</li></ul></li><li>系统开销<ul><li>在创建或撤销进程时，系统都要为之分配或回收进程控制块PCB及其他资源，如内存空间、I/O设备等<br>而创建线程和撤销线程时所用的开销则较小</li><li>传统的进程间实现并发时，需要切换进程的运行环境，涉及进程上下文的切换，系统开销很大，<br>而线程切换时只需保存和设置少量寄存器内容，开销很小。</li><li>此外，由于同一进程内的多个线程共享进程的地址空间，因此这些线程之间的同步与通信非常容易实现，甚至无须操作系统的干预。</li></ul></li><li>支持多处理机系统<ul><li>对于传统单线程进程，不管有多少处理机，进程只能运行在一个处理机上</li><li>对于多线程进程，可以将进程中的多个线程分配到多个处理机上执行。</li></ul></li></ul></li><li><p>线程的属性</p><ul><li>线程是处理机的独立调度单位，多个线程是可以并发执行的。<ul><li>在单CPU的计算机系统中，各线程可交替地占用CPU</li><li>在多CPU的计算机系统中，各线程可同时占用不同的CPU</li></ul></li><li>线程是一个轻型实体，它不拥有系统资源，同一进程中的各个线程共享该进程所拥有的资源<br>由于共享内存地址空间，同一进程中的线程间通信甚至无需系统干预</li><li>每个线程都应有一个唯一的标识符和一个线程控制块（TCB），线程控制块记录了线程执行的寄存器和栈等现场状态。</li><li>不同的线程可以执行相同的程序，即同一个服务程序被不同的用户调用时，操作系统把它们创建成不同的线程。<ul><li>同一进程中的线程切换，不会引起进程切换，不同进程中的线程切换，会引起进程切换</li><li>切换同进程内的线程，系统开销很小，切换进程，系统开销较大</li></ul></li><li>一个线程被创建后，便开始了它的生命周期，直至终止。<br>线程在生命周期内会经历阻塞态、就绪态和运行态等各种状态变化</li></ul></li></ul><h4 id="8-线程的实现方式"><a href="#8-线程的实现方式" class="headerlink" title="8.线程的实现方式"></a>8.线程的实现方式</h4><ul><li>用户级线程（ULT）<ul><li>用户级线程的特点<ul><li><strong>用户级线程由应用程序通过线程库实现</strong>，所有的线程管理工作都由应用程序负责并在用户空间中完成（包括线程切换)<br>应用层序可以通过线程库设计成多线程程序</li><li>用户级线程中，线程切换可以在用户态下即可完成，无需操作系统干预。</li><li>在用户看来，是有多个线程。但是在操作系统内核看来，并意识不到线程的存在</li><li><strong>对于设置了用户级线程的系统，其调度仍是以进程为单位进行的，各个进程轮流执行一个时间片</strong></li></ul></li><li>用户级线程的优点（节省开销）<ul><li>线程切换在用户空间就可完成，不需要转换到内核空间，节省了模式切换的开销</li><li>调度算法可以是进程专用的，不同的进程可根据自身的需要，对自己的线程选择不同的调度算法。</li><li>用户级线程的实现与操作系统平台无关，对线程管理的代码是属于用户程序的一部分</li></ul></li><li>用户级线程的缺点（并发度不高）<ul><li>系统调用的阻塞问题，当线程执行一个系统调用时，不仅该线程被阻塞，而且进程内的所有线程都被阻塞</li><li>不能发挥多处理机的优势，内核每次分配给一个进程的仅有一个CPU,因此进程中仅有一个线程能执行。</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPMCEeH.png" alt="pPMCEeH.png"></li></ul></li></ul></li><li>内核级线程（KLT）<ul><li>内核级线程的特点<ul><li><strong>内核级线程的管理工作由操作系统内核完成。</strong><br><strong>线程调度、切换等工作都由内核负责，因此内核级线程的切换必然需要在核心态下才能完成。</strong></li><li>操作系统会为每个内核级线程建立相应的TCB(线程控制块)通过TCB对线程进行管理。</li><li>“内核级线程”就是“从操作系统内核视角看能看到的线程</li></ul></li><li>内核级线程的优点<ul><li>多线程可在多核处理机上并行执行，内核能同时调度同一进程中的多个线程并行执行，</li><li>如果进程中的一个线程被阻塞，内核可以调度该进程中的其他线程占用处理机，也可运行其他进程中的线程，并发能力强</li><li>内核支持线程具有很小的数据结构和堆栈，线程切换比较快、开销小</li><li>内核本身也可采用多线程技术，可以提高系统的执行速度和效率。</li></ul></li><li>内核级线程的缺点<ul><li>一个用户进程会占用多个内核级线程，线程切换由操作系统内核完成，需要切换到核心态，因此线程管理的成本高，开销大</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPMCEeH.png" alt="pPMCEeH.png"></li></ul></li></ul></li><li>组合方式（多线程模型）<ul><li>一对一模型<ul><li>一个用户级线程映射到一个内核级线程。每个用户进程有与用户级线程同数量的内核级线程。</li><li>优点：当一个线程被阻塞后，别的线程还可以继续执行，并发能力强。多线程可在多核处理机上并行执行</li><li>缺点：一个用户进程会占用多个内核级线程，线程切换由操作系统内核完成，需要切换到核心态，因此线程管理的成本高，开销大。</li></ul></li><li>多对一模型<ul><li>多个用户级线程映射到一个内核级线程。且一个进程只被分配一个内核级线程</li><li>优点：用户级线程的切换在用户空间即可完成，不需要切换到核心态，线程管理的系统开销小，效率高</li><li>缺点：当一个用户级线程被阻塞后，整个进程都会被阻塞，并发度不高。多个线程不可在多核处理机上并行运行</li><li>操作系统只“看得见”内核级线程，因此只有内核级线程才是处理机分配的单位。</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPMPKE9.png" alt="pPMPKE9.png"></li></ul></li></ul></li><li>多对多模型<ul><li>n个用户及线程映射到m个内核级线程(n&gt;=m)。每个用户进程对应m个内核级线程</li><li>克服了多对一模型并发度不高的缺点（一个阻塞全体阻塞）</li><li>又克服了一对一模型中一个用户进程占用太多内核级线程，开销太大的缺点</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/14/pPMPREj.png" alt="pPMPREj.png"></li></ul></li></ul></li></ul></li></ul><h3 id="二-处理机调度（✪）"><a href="#二-处理机调度（✪）" class="headerlink" title="二.处理机调度（✪）"></a>二.处理机调度（✪）</h3><h4 id="1-调度的概念"><a href="#1-调度的概念" class="headerlink" title="1.调度的概念"></a>1.调度的概念</h4><ul><li>处理机调度是对处理机进行分配，即从就绪队列中按照一定的算法(公平、高效的原则)选择一个进程并将处理机分配给它运行，以实现进程并发地执行。</li><li>调度的层次（三级调度）<ul><li>高级调度（作业调度）<ul><li>内存空间有限，有时无法将用户提交的作业全部由外存放入内存<br>此时按一定的原则<strong>从外存的作业后备队列</strong>中挑选一个作业<strong>调入内存</strong>，分配相关资源并创建进程。</li><li>为外存与内存之间的调度，每个作业只调入一次，调出一次。作业调入时会建立PCB，调出时才撤销PCB。</li><li>注：作业=一个具体的任务，用户向系统提交一个作业≈用户让操作系统启动一个程序（来处理一个具体的任务）</li></ul></li><li>中级调度（内存调度）<ul><li>挂起状态的概念<ul><li>内存不够时，可将某些进程调出外存。等内存空闲或者进程需要运行时再重新调入内存。</li><li>暂时调到外存等待的进程状态为<strong>挂起态</strong>。被挂起的进程PCB会被组织成挂起队列</li></ul></li><li>按照某种策略决定将哪个处于<strong>挂起状态的进程重新调入内存，此时修改其状态为就绪态，送入就绪队列</strong>。</li><li>一个进程可能会被多次调出、调入内存，因此中级调度发生的频率要比高级调度更高。</li><li>中级调度可以提高内存利用率和系统吞吐量</li></ul></li><li>低级调度（进程、处理机调度）<ul><li>按照某种策略从就绪队列中选取一个进程，将处理机分配给它</li><li>进程调度是操作系统中最基本的一种调度，在一般的操作系统中都必须配置进程调度。</li><li>进程调度的频率很高，一般几十毫秒一次。</li></ul></li><li>区别<ul><li><img src="https://s1.ax1x.com/2023/08/15/pPMhyqg.png" alt="pPMhyqg.png"></li></ul></li></ul></li><li>阻塞挂起与就绪挂起以及七状态模型：<strong>挂起是在外存中等待的状态</strong><ul><li><img src="https://s1.ax1x.com/2023/08/15/pPMhwGt.png" alt="pPMhwGt.png"></li></ul></li></ul><h4 id="2-调度的目标"><a href="#2-调度的目标" class="headerlink" title="2.调度的目标"></a>2.调度的目标</h4><ul><li>CPU利用率：指CPU“忙碌”的时间占总时间的比例。</li><li>系统吞吐量：单位时间内完成作业的数量</li><li>周转时间：<strong>是指从作业被提交给系统开始，到作业完成为止的这段时间间隔</strong>。<ul><li>包括四个部分（后三项在一个作业的整个处理过程中，可能发生多次<ul><li>作业在外存后备队列上等待作业调度（高级调度）的时间</li><li>进程在就绪队列上等待进程调度（低级调度）的时间</li><li>进程在CPU上执行的时间</li><li>进程等待I/O操作完成的时间</li></ul></li><li><strong>周转时间=作业完成时间一作业提交时间</strong></li><li>平均周转时间=$\large\frac{各作业周转时间之和}{作业数}$</li><li><strong>带权周转时间=$\large \frac{作业周转时间}{作业实际运行时间}=\frac{作业完成时间一作业提交时间}{作业实际运行时间}$（一定大于1，越小越好）</strong></li><li>平均带权周转时间=$\large \frac{各作业带权周转时间之和}{作业数}$（越小越好）</li><li>对于周转时间相同的两个作业，实际运行时间长的作业在相同时间内被服务的时间更多，带权周转时间更小，用户满意度更高</li><li>对于实际运行时间相同的两个作业，周转时间短的带权周转时间更小，用户满意度更高。</li></ul></li><li>等待时间：指进程/作业处于<strong>等待处理机状态</strong>时间之和，等待时间越长，用户满意度越低。<ul><li>对于进程来说，等待时间就是指进程建立后等待被服务的时间之和，<strong>在等待I/O完成的期间其实进程也是在被服务的，所以不计入等待时间。</strong>（<strong>等待时间=周转时间-运行时间-等待I/O操作的时间</strong>）</li><li>对于作业来说，不仅要考虑建立进程后的等待时间，<strong>还要加上作业在外存后备队列中等待的时间</strong>。</li><li>处理机调度算法实际上并不影响作业执行或输入/输出操作的时间，只影响作业在就绪队列中等待所花的时间。<br>因此，衡量一个调度算法的优劣，常常只需简单地考察等待时间</li></ul></li><li>响应时间：指从用户提交请求到系统首次产生响应所用的时间。<ul><li>在交互式系统中，周转时间不是最好的评价准则，一般采用响应时间作为衡量调度算法的重要准则之一</li></ul></li></ul><h4 id="3-调度的实现"><a href="#3-调度的实现" class="headerlink" title="3.调度的实现"></a>3.调度的实现</h4><ul><li>进程调度的时机<ul><li>需要进行进程调度与切换的情况<ul><li>当前运行的进程主动放弃处理机<ul><li>进程正常终止</li><li>运行过程中发生异常而终止</li><li>进程主动请求阻塞(如等待I/O)</li></ul></li><li>当前运行的进程被动放弃处理机<ul><li>分给进程的时间片用完</li><li>有更紧急的事需要处理(如I/O中断)</li><li>有更高优先级的进程进入就绪队列</li></ul></li></ul></li><li>不能进行进程调度与切换的情况<ul><li>在处理中断的过程中：中断处理过程复杂，与硬件密切相关，很难做到在中断处理过程中进行进程切换。</li><li>进程在操作系统内核程序临界区中（普通的临界区可以进行进程调度）</li><li>在原子操作过程中（原语），原子操作不可中断，要一气呵成（如修改PCB中进程状态标志，并把PCB放到相应队列）</li></ul></li></ul></li><li>进程调度的方式<ul><li>非剥夺调度方式，又称非抢占方式<ul><li>即只允许进程主动放弃处理机，在运行过程中即便有更紧迫的任务到达，当前进程依然会继续使用处理机，直到该进程终止或主动要求进入阻塞态。</li><li>实现简单，系统开销小但是无法及时处理紧急任务，适合于早期的批处理系统</li></ul></li><li>剥夺调度方式，又称抢占方式。<ul><li>当一个进程正在处理机上执行时，如果有一个更重要或更紧迫的进程需要使用处理机，则立即暂停正在执行的进程，将处理机分配给更重要紧迫的那个进程。</li><li>可以优先处理更紧急的进程，也可实现让各进程按时间片轮流执行的功能（通过时钟中断）。</li><li>适合于分时操作系统、实时操作系统</li></ul></li></ul></li><li>进程的切换与过程<ul><li>狭义的进程调度”与“广义的进程调度”的区别<ul><li>狭义的进程调度指的是从就绪队列中选中一个要运行的进程。<br>这个进程可以是刚刚被暂停执行的进程，也可能是另一个进程，后一种情况就需要进程切换</li><li>进程切换是指一个进程让出处理机，由另一个进程占用处理机的过程。</li><li>广义的进程调度包含了选择一个进程和进程切换两个步骤</li></ul></li><li>进程切换的过程<ul><li>对原来运行进程各种数据的保存</li><li>对新的进程各种数据的恢复<br>如：程序计数器、程序状态字、各种数据寄存器等处理机现场信息，这些信息一般保存在进程控制块</li></ul></li><li>进程切换是有代价的，因此如果过于频繁的进行进程调度与切换，必然会使整个系统的效率降低使系统大部分时间都花在了进程切换上，而真正用于执行进程的时间减少。</li></ul></li></ul><h4 id="4-典型的调度算法（♚）"><a href="#4-典型的调度算法（♚）" class="headerlink" title="4.典型的调度算法（♚）"></a>4.典型的调度算法（♚）</h4><ul><li>前三种算法适用于批处理系统，后三种算法适用于交互式系统</li><li>先来先服务(FCFS，First Come First Serve)<ul><li>算法思想：主要从“公平”的角度考虑（类似于我们生活中排队买东西的例子），<strong>考虑的是等待时间</strong></li><li>算法规则：<strong>按照作业/进程到达的先后顺序进行服务</strong></li><li>可用于作业调度与进程调度<ul><li>用于作业调度时，考虑的是哪个作业先到达后备队列</li><li>用于进程调度时，考虑的是哪个进程先到达就绪队列</li></ul></li><li>是非抢占式的算法</li><li>优缺点<ul><li>优点：公平、算法实现简单</li><li>缺点：排在长作业（进程）后面的短作业需要等待很长时间，带权周转时间很大，对短作业来说用户体验不好。<br>FCFS算法对长作业有利，对短作业不利</li></ul></li><li>是否会导致饥饿：不会<br>饥饿：某进程/作业长期得不到服务</li><li>有利于CPU繁忙型作业，不利于I/O繁忙型作业</li></ul></li><li>短作业优先(SJF，Shortest Job First)<ul><li>算法思想：<strong>追求最少的平均等待时间，最少的平均周转时间、最少的平均平均带权周转时间</strong></li><li>算法规则<ul><li>短作业/进程优先调度算法：<strong>每次调度时选择当前已到达且运行时间最短的作业/进程</strong>。（非抢占式）</li><li>最短剩余时间优先算法<ul><li><strong>每当有进程加入就绪队列改变时就需要调度，如果新到达的进程剩余时间比当前运行的进程剩余时间更短，</strong><br><strong>则由新进程抢占处理机，当前运行进程重新回到就绪队列。另外，当一个进程完成时也需要调度</strong>（抢占式）</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/15/pPMHpOP.png" alt="pPMHpOP.png"></li></ul></li></ul></li></ul></li><li>可用于作业和进程调度，用于进程调度时称为短进程优先(SPF，Shortest Process First)算法</li><li>SJF和SPF是非抢占式的算法，但也有适用于可抢占式的算法：短剩余时间优先算法(SRTN，Shortest Remaining Time Next)</li><li>优点：“最短的”平均等待时间、平均周转时间</li><li>缺点：不公平。对短作业有利，对长作业不利。</li><li>会产生饥饿现象。</li><li><strong>SJF调度算法的平均等待时间、平均周转时间最少</strong></li></ul></li><li>高响应比优先(HRRN，Highest Response Ratio Next)<ul><li>算法思想：<strong>要综合考虑作业/进程的等待时间和要求服务的时间</strong><br><strong>满足短作业优先且不会导致饥饿</strong></li><li>算法规则<ul><li><strong>只有当前运行的程主动放弃CPU时(正常/异常完成，或主动阻塞)，才需要行调度，</strong><br><strong>调度时计算所有就绪进程的响应比，选响应比最高进程上处理机。</strong></li><li>响应比=$\large \frac{等待时间+要求服务时间}{要求服务时间}$   (响应比≥1)</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/15/pPMb0uq.png" alt="pPMb0uq.png"></li></ul></li></ul></li><li>可用于作业调度与进程调度</li><li>是非抢占式的算法。只有当前运行的作业/进程主动放弃处理机时，才需要调度，才需要计算响应比</li><li>优点<ul><li>综合考虑了等待时间和运行时间（要求服务时间）</li><li>等待时间相同时，要求服务时间短的优先(SJF的优点)</li><li>要求服务时间相同时，等待时间长的优先(FCFS的优点)</li><li>对于长作业来说，随着等待时间越来越久，其响应比也会越来越大，从而避免了长作业饥饿的问题</li></ul></li><li>不会导致饥饿</li></ul></li><li>时间片轮转调度算法(RR)<ul><li>算法思想：公平地、轮流地为各个进程服务，让每个进程在一定时间间隔内都可以得到响应<br><strong>更注重响应时间，不计算周转时间，适用于分时系统，多个用户能及时干预系统</strong></li><li>算法规则<ul><li><strong>按照各进程到达就绪队列的顺序，轮流让各个进程执行一个时间片，</strong><br><strong>若进程未在一个时间片内执行完，则剥夺处理机，将进程重新放到就绪队列队尾重新排队。</strong></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/15/pPMq8MR.png" alt="pPMq8MR.png"></li></ul></li></ul></li><li>只用于进程调度：只有作业放入内存建立了相应的进程后，才能被分配处理机时间片</li><li>属于抢占式的算法：若进程未能在时间片内运行完，将被强行剥夺处理机使用权，回到就绪队列<br><strong>由时钟装置发出时钟中断来通知CPU时间片已到</strong></li><li>优点：公平；响应快，适用于分时操作系统</li><li>缺点：由于高频率的进程切换，因此有一定开销且不区分任务的紧急程度</li><li>不会导致饥饿</li><li>时间片的选择<ul><li>如果时间片太大，使得每个进程都可以在一个时间片内就完成，<br>则时间片轮转调度算法退化为先来先服务调度算法，并且会增大进程响应时间。因此时间片不能太大。</li><li>如果时间片太小，会导致进程切换过于频繁，系统会花大量的时间来处理进程切换，<br>从而导致实际用于进程执行的时间比例减少。进程调度、切换是有时间代价的(保存、恢复运行环境)</li></ul></li></ul></li><li>优先级调度算法<ul><li>算法思想：随着计算机的发展，特别是<strong>实时操作系统</strong>的出现，越来越多的应用场景需要根据任务的紧急程度来决定处理顺序</li><li>算法规则<ul><li>对于非抢占式的算法：每次调度时选择当前已到达且优先级最高的进程，当前进程主动放弃处理机时发生调度。</li><li>对于抢占式的算法：每次调度时选择当前已到达且优先级最高的进程。当前进程主动放弃处理机时发生调度。<br>另外，当就绪队列发生改变时也需要检查是会发生抢占。<ul><li><img src="https://s1.ax1x.com/2023/08/15/pPMqxSJ.png" alt="pPMqxSJ.png"></li></ul></li></ul></li><li>既可用于作业调度，也可用于进程调度，还可用于I/O调度</li><li>抢占式、非抢占式都有。非抢占式只需在进程主动放弃处理机时进行调度即可，<br>而抢占式还需在就绪队列变化时，检查是否会发生抢占。</li><li>优点：用优先级区分紧急程度、重要程度，适用于实时操作系统。可灵活地调整对各种作业/进程的偏好程度。</li><li>缺点：若源源不断地有高优先级进程到来，则可能导致饥饿</li><li>会导致饥饿</li><li>关于优先级<ul><li>就绪队列未必只有一个，可以按照不同优先级来组织。另外，也可以把优先级高的进程排在更靠近队头的位置</li><li>根据优先级是否可以动态改变，可将优先级分为静态优先级和动态优先级两种<ul><li>静态优先级：创建进程时确定，之后一直不变</li><li>动态优先级：创建进程时有一个初始值，之后会根据情况动态地调整优先级</li></ul></li><li>优先级的设置<ul><li>系统进程优先级高于用户进程</li><li>前台进程优先级高于后台进程（交互型进程&gt;非交互型进程）</li><li>I/O型进程(或称I/O繁忙型进程)优先级高于计算型进程(或称CPU繁忙型进程)<ul><li>操作系统更偏好I/O型进程，I/O设备（如打印机）的处理速度要比CPU慢得多，<br>因此若将I/O型进程的优先级设置得更高，就更有可能让I/O设备尽早开始工作，</li><li>I/O设备和CPU可以并行工作。如果优先让I/O繁忙型进程运行则越有可能让I/O设备尽早地投入工作，<br>则资源利用率、系统吞吐量都会得到提升，进而提升系统的整体效率。</li></ul></li></ul></li><li>采用动态优先级时，何时调整（可以从追求公平、提升资源利用率等角度考虑）<ul><li>如果某进程在就绪队列中等待了很长时间，则可以适当提升其优先级</li><li>如果某进程占用处理机运行了很长时间，则可适当降低其优先级</li><li>如果发现一个进程频繁地进行I/O操作，则可适当提升其优先级</li></ul></li></ul></li></ul></li><li>多级反馈队列调度算法<ul><li>算法思想：对其它调度算法的折中权衡，不必事先估计进程的执行时间</li><li>算法规则<ul><li>设置多级就绪队列，各级队列优先级从高到低，时间片从小到大</li><li>新进程到达时先进入第1级队列，按FCFS原则排队等待被分配时间片，若用完时间片进程还未结束，则进程进入下一级队列队尾。如果此时已经是在最下级的队列，则重新放回该队列队尾</li><li>只有第k级队列为空时，才会为k+1级队头的进程分配时间片</li></ul></li><li>用于进程调度</li><li>是抢占式的算法。在k级队列的进程运行过程中，若更上级的队列(1~k-1级)中进入了一个新进程，则由于新进程处于优先级更高的队列中，因此新进程会抢占处理机，原来运行的进程放回k级队列队尾。</li><li>优点<ul><li>对各类型进程相对公平(FCFS的优点)</li><li>每个新到达的进程都可以很快就得到响应(RR的优点)</li><li>短进程只用较少的时间就可完成(SPF的优点)</li><li>不必实现估计进程的运行时间（避免用户作假）</li><li>可灵活地调整对各类进程的偏好程度，比如CPU密集型进程、I/O密集型进程<br>可以将因I/O而阻塞的进程重新放回原队列，这样I/O型进程就可以保持较高优先级</li></ul></li><li>会导致饥饿</li></ul></li><li>几种进程调度算法的区别<ul><li><img src="https://s1.ax1x.com/2023/08/15/pPMv7IP.png" alt="pPMv7IP.png"></li></ul></li><li><p>例题</p><ul><li><p>例1，通过画甘特图来求解</p><ul><li><img src="https://s1.ax1x.com/2023/08/16/pPQ2yiq.png" alt="pPQ2yiq.png"></li></ul></li><li><p>例2，在操作系统中CPU计算和I/O操作可以同时进行</p><ul><li><img src="https://s1.ax1x.com/2023/08/16/pPQ24eJ.png" alt="pPQ24eJ.png"></li></ul></li><li><p>例3，如果单独说输入设备和输出设备的话，那么这三种设备是可以并行进行的</p><ul><li><img src="https://s1.ax1x.com/2023/08/16/pPQ2xwd.png" alt="pPQ2xwd.png"></li></ul></li><li><p>例4，关于多级队列的题目</p><ul><li><img src="https://s1.ax1x.com/2023/08/16/pPQReTs.png" alt="pPQReTs.png"></li></ul></li></ul></li></ul><h3 id="三-同步与互斥（✪）"><a href="#三-同步与互斥（✪）" class="headerlink" title="三.同步与互斥（✪）"></a>三.同步与互斥（✪）</h3><h4 id="1-同步与互斥的基本概念"><a href="#1-同步与互斥的基本概念" class="headerlink" title="1.同步与互斥的基本概念"></a>1.同步与互斥的基本概念</h4><ul><li><p>进程同步：进程具有异步性的特征。异步性：各并发执行的进程以各自独立的、不可预知的速度向前推进。<br>操作系统要提供“进程同步机制”来解决异步问题</p></li><li><p>临界资源</p><ul><li><p>临界资源的概念</p><ul><li>共享是操作系统的基本特征之一，资源共享的方式分为了同时共享方式和互斥共享方式<br>互斥共享方式指：系统中的某些资源，虽然可以提供给多个进程使用，但一个时间段内只允许一个进程访问该资源</li><li><strong>临界资源：一个时间段内只允许一个进程使用的资源，属于互斥共享资源</strong></li><li>许多物理设备(比如摄像头、打印机)都属于临界资源。共享变量、公用队列、内存缓冲区等都属于临界资源。<br>非共享数据不属于临界资源（私有的），在一段时间内能被并发使用的资源不属于临界资源，如可重入的程序代码</li><li>对临界资源的访问，必须互斥地进行。<ul><li>互斥，亦称间接制约关系。进程互斥指当一个进程访问某临界资源时，另一个想要访问该临界资源的进程必须等待。</li><li>当前访问临界资源的进程访问结束，释放该资源之后，另一个进程才能去访问临界资源。</li><li>例题：此时两个进程内部的线程同享变量x；对于A/B选项，两段代码执行顺序的先后不影响最终的结果，D跨越进程不是；因此选C（执行顺序先后会影响结果）<ul><li><img src="https://s1.ax1x.com/2023/08/16/pPlJvKf.png" alt="pPlJvKf.png"></li></ul></li></ul></li></ul></li><li><p>临界资源的访问过程</p><ul><li><p>进入区：负责检查是否可进入临界区，若可进入，则应设置正在访问临界资源的标志<br>可理解为“上锁”，以阻止其他进程同时进入临界区</p></li><li><p><strong>临界区：也可称为临界段，临界区是进程中访问临界资源的代码段</strong></p></li><li><p>退出区：负责解除正在访问临界资源的标志(可理解为“解锁”)</p></li><li><p>剩余区：代码中的其它部分</p></li><li><p><strong>进入区和退出区是负责实现互斥的代码段</strong></p></li><li><p>代码</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">do</span>&#123;</span><br><span class="line">  entry section; <span class="comment">//进入区（上锁）</span></span><br><span class="line">  critical section; <span class="comment">//临界区（访问临界资源）</span></span><br><span class="line">  exit section; <span class="comment">//退出区 （解锁）</span></span><br><span class="line">  remainder section; <span class="comment">//剩余区</span></span><br><span class="line">&#125;<span class="keyword">while</span>(<span class="literal">true</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>同步机制的规则</p><ul><li>空闲让进：临界区空闲时，可以允许一个请求进入临界区的进程立即进入临界区</li><li>忙则等待：当已有进程进入临界区时，其他试图进入临界区的进程必须等待</li><li>有限等待：对请求访问的进程，应保证能在有限时间内进入临界区（保证不会饥饿）</li><li>让权等待：当进程不能进入临界区时，应立即释放处理机，<strong>防止进程忙等待</strong>。</li></ul></li></ul><h4 id="2-实现临界区互斥的基本方法"><a href="#2-实现临界区互斥的基本方法" class="headerlink" title="2.实现临界区互斥的基本方法"></a>2.实现临界区互斥的基本方法</h4><ul><li><p>软件实现方法</p><ul><li><p>单标志法（谦让）</p><ul><li><p>算法思想</p><ul><li>该算法设置一个公用整型变量，用于指示被允许进入临界区的进程编号，若为0则允许P0进程进入临界区。该算法可确保每次只允许一个进程进入临界区。</li><li>两个进程在访问完临界区后会把使用临界区的权限转交给另一个进程。也就是说每个进程进入临界区的权限只能被另一个进程赋予</li><li>因此，该算法可以实现“同一时刻最多只允许一个进程访问临界区”</li></ul></li><li><p>代码与运行逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> turn=<span class="number">0</span>; <span class="comment">//turn 表示当前允许进入临界区的进程号，初始值设为0，允许P0进程访问</span></span><br><span class="line"><span class="comment">//P0进程：               //P1进程：     </span></span><br><span class="line"><span class="keyword">while</span>(turn!=<span class="number">0</span>);<span class="keyword">while</span>(turn!=<span class="number">1</span>);<span class="comment">//检查，如果不能由自己使用则一直循环等待（只检查，不上锁）</span></span><br><span class="line">critical section;critical section;</span><br><span class="line"><span class="comment">//退出区，此时将使用权转交给另一个进程（相当于在退出区既给另一进程“解锁”，又给自己“上锁”）</span></span><br><span class="line">turn=<span class="number">1</span>;turn=<span class="number">0</span>;</span><br><span class="line">remainder section;remainder section;</span><br></pre></td></tr></table></figure></li><li><p>算法的特点：该算法可确保每次只允许一个进程进入临界区。但两个进程必须交替进入临界区。<br>若某个进程不再进入临界区，则另一个进程也将无法进入临界区(<strong>违背“空闲让进”</strong>)。这样很容易造成资源利用不充分。</p></li></ul></li><li><p>双标志先检查法（表达访问意愿）</p><ul><li><p>算法思想</p><ul><li>在每个进程访问临界区资源之前，先查看临界资源是否正被访问，若正被访问，该进程需等待；<br>否则，进程才进入自己的临界区。</li><li>为此，设置一个布尔型数组flag，如第i个元素flag[i]为FALSE，表示Pi进程未进入临界区，<br>如为TRUE，表示Pi进程进入临界区</li></ul></li><li><p>代码与运行逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">bool</span> flag[<span class="number">2</span>];       <span class="comment">//创建表达意愿的数组</span></span><br><span class="line">flag[<span class="number">0</span>]=<span class="literal">false</span>;<span class="comment">//刚开始两个进程都没有意愿进入临界区</span></span><br><span class="line">flag[<span class="number">1</span>]=<span class="literal">false</span>;</span><br><span class="line">Pi进程：Pj进程：</span><br><span class="line"><span class="comment">//在进入区先&quot;检查&quot;后&quot;上锁”</span></span><br><span class="line"><span class="keyword">while</span>(flag[j]);  <span class="keyword">while</span>(flag[i]);<span class="comment">//检查对方是否在访问临界资源,如果在则一直循环等待,</span></span><br><span class="line">flag[i]=TRUE;         flag[j]=TRUE;<span class="comment">//如果临界资源没有被访问则上锁，表达访问意愿</span></span><br><span class="line">critical section;  critical section;</span><br><span class="line">flag[i]=FALSE;  flag[j]=FALSE;   <span class="comment">//访问结束之后开锁</span></span><br><span class="line">remainder section;  remainder section;</span><br></pre></td></tr></table></figure></li><li><p>优点：不用交替进入，可连续使用</p></li><li><p>缺点：<strong>可能将将会同时访问临界区。违反“忙则等待”原则</strong>。<br>原因在于，进入区的“检查”和“上锁”两个处理<strong>不是一气呵成</strong>的。“检查”后，“上锁”前可能发生进程切换。</p></li></ul></li><li><p>双标志后检查法（先斩后奏）</p><ul><li><p>算法思想：<strong>在前一个算法的基础上实现先上锁再检查，可以确保同一时间内最多只有一个进程访问临界资源</strong></p></li><li><p>代码与运行逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">P进程：</span><br><span class="line"><span class="comment">//在进入区先“加锁&quot;后“检查”</span></span><br><span class="line">flag[i]=TRUE;   <span class="comment">//要访问临界区时先上锁，表达访问意愿</span></span><br><span class="line"><span class="keyword">while</span>(flag[j]);   <span class="comment">//此时再来检查是否临界区被人使用</span></span><br><span class="line">critical section;</span><br><span class="line">flag[i]=FALSE;<span class="comment">//访问结束之后，开锁</span></span><br><span class="line">remainder section;</span><br></pre></td></tr></table></figure></li><li><p>算法的缺点</p><ul><li>两个进程几乎同时都想进入临界区时，它们分别将自己的标志值flag设置为TRUE，并且同时检测对方的状态<br>执行while语句，发现对方也要进入临界区时，此时谁也进不了临界区</li><li><strong>双标志后检查法虽然解决了“忙则等待”的问题，但是又违背了“空闲让进”和“有限等待”原则，</strong><br><strong>会因各进程都长期无法访问临界资源而产生“饥饿”现象</strong>。</li></ul></li></ul></li><li><p>Peterson算法（结合以上算法的特点）</p><ul><li><p>算法思想</p><ul><li>结合双标志法、单标志法的思想。如果双方都争着想进入临界区，那可以让进程尝试“孔融让梨”（谦让）<br>做一个有礼貌的进程</li><li>此时最后表示谦让的一方失去主动权，则另外一方将执行</li></ul></li><li><p>代码与运行逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">bool</span> flag[<span class="number">2</span>];   <span class="comment">//表示进入临界区意愿的数组，初始值都是false</span></span><br><span class="line"><span class="type">int</span> turn <span class="number">0</span>;    <span class="comment">//turn表示优先让哪个进程进入临界区（表达谦让）</span></span><br><span class="line"><span class="comment">//P0进程：</span></span><br><span class="line">flag[<span class="number">0</span>]=<span class="literal">true</span>;     <span class="comment">//表示自己想进入临界区</span></span><br><span class="line">turn=<span class="number">1</span>; <span class="comment">//主动谦让，给对方优先信号</span></span><br><span class="line"><span class="keyword">while</span>(flag[<span class="number">1</span>]&amp;&amp;turn==<span class="number">1</span>); <span class="comment">//检查对方是否也想使用，如果对方不想使用则自己使用；如果对方想使用且表达了其谦让意愿（最后一次）则自己使用；</span></span><br><span class="line">critical section;</span><br><span class="line">flag[<span class="number">0</span>]=<span class="literal">false</span>;   <span class="comment">//访问完临界区之后表达自己不想使用了</span></span><br><span class="line">remainder section;</span><br><span class="line"></span><br><span class="line"><span class="comment">//P1进程：e</span></span><br><span class="line">flag[<span class="number">1</span>]=<span class="literal">true</span>;</span><br><span class="line">turn=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (flag[<span class="number">0</span>]&amp;&amp;turn==<span class="number">0</span>); <span class="comment">//只有同时满足对方想要使用且自己是最后的谦让方时，此时才开始等待</span></span><br><span class="line">critical section;</span><br><span class="line">flag[<span class="number">1</span>]=<span class="literal">false</span>;</span><br><span class="line">remainder section;</span><br></pre></td></tr></table></figure></li><li><p><strong>算法的特点：Peterson算法用软件方法解决了进程互斥问题，遵循了空闲让进、忙则等待、有限等待的原则，</strong><br><strong>但是依然未遵循让权等待的原则（不能进入临界区时还在还在处理机上，造成忙等待）。</strong></p></li></ul></li></ul></li><li><p>硬件实现方法</p><ul><li><p>中断屏蔽方法</p><ul><li>利用“开/关中断指令”实现，与原语的实现思想相同，即在某进程开始访问临界区到结束访问为止都不允许被中断，<br>也就不能发生进程切换，因此也不可能发生两个同时访问临界区的情况</li><li>优点：简单、高效</li><li>缺点：不适用于多处理机；只适用于操作系统内核进程，不适用于用户进程，<br>开/关中断指令只能运行在内核态，这组指令如果能让用户随意使用会很危险</li></ul></li><li><p>硬件指令方法</p><ul><li><p>TestAndSet指令（TSL指令)</p><ul><li><p>指令思想</p><ul><li>TSL指令是用硬件实现的，执行的过程不允许被中断，只能一气呵成。</li><li>若刚开始Iock是false，则TSL返回的old值为false，while循环条件不满足，直接跳过循环，进入临界区。</li><li>若刚开始Iock是true，则执行TLS后old返回的值为true，while循环条件满足，会一直循环，<br>直到当前访问临界区的进程在退出区进行“解锁”。</li></ul></li><li><p>代码逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//布尔型共享变量lock表示当前临界区是否被加锁</span></span><br><span class="line"><span class="comment">//true表示已加锁，false表示末加锁</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">TestAndSet</span> <span class="params">(<span class="type">bool</span> *lock)</span></span>&#123;</span><br><span class="line"><span class="type">bool</span> old;</span><br><span class="line">oLd=*Lock;     <span class="comment">//old用来存放Lock原来的值</span></span><br><span class="line">*Lock=<span class="literal">true</span>;    <span class="comment">//无论之前是否已加锁，都将Lock设为true</span></span><br><span class="line"><span class="keyword">return</span> old;    <span class="comment">//返回Lock原来的值</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//以下是使用TSL指令实现互斥的算法逻辑</span></span><br><span class="line"><span class="keyword">while</span>(<span class="built_in">TestAndSet</span>(&amp;lock);<span class="comment">// 实现&quot;上锁并&quot;检查&quot;</span></span><br><span class="line">临界区代码段</span><br><span class="line">lock <span class="literal">false</span>;   <span class="comment">//解锁</span></span><br><span class="line">剩余区代码段</span><br></pre></td></tr></table></figure><ul><li><p>优点：实现简单，无需像软件实现方法那样严格检查是否会有逻辑漏洞；<strong>适用于多处理机环境</strong></p></li><li><p>缺点：不满足“让权等待”原则，暂时无法进入临界区的进程会占用CPU并循环执行TSL指令，<strong>从而导致“忙等”。</strong></p></li></ul></li></ul></li><li><p>SWAP指令</p><ul><li><p>指令思想</p><ul><li>Swap指令是用硬件实现的，执行的过程不允许被中断，只能一气呵成。</li><li>逻辑上来看Swap和TSL并无太大区别，都是先记录下此时临界区是否已经被上锁（记录在old上），<br>再将上锁标记Iock设置为true，最后检查old</li><li>如果old为false则说明之前没有别的进程对临界区上锁，则可跳出循环，进入临界区。</li></ul></li><li><p>代码逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Swap指令的作用是交换两个变量的值</span></span><br><span class="line"><span class="built_in">Swap</span> (<span class="type">bool</span> *a,<span class="type">bool</span> *b)&#123;</span><br><span class="line"><span class="type">bool</span> temp;</span><br><span class="line">temp =*a;</span><br><span class="line">*a=*b;</span><br><span class="line">*b temp;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//以下是用Swap指令实现互斥的算法逻辑</span></span><br><span class="line"><span class="comment">//1ock表示当前临界区是否被加锁</span></span><br><span class="line"><span class="type">bool</span> old <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">while</span> (old==<span class="literal">true</span>)</span><br><span class="line"><span class="built_in">Swap</span> (&amp;lock,&amp;old);</span><br><span class="line">临界区代码段，，，</span><br><span class="line">lock <span class="literal">false</span>;</span><br><span class="line">剩余区代码段，，，</span><br></pre></td></tr></table></figure></li><li><p>优点：实现简单，无需像软件实现方法那样严格检查是否会有逻辑漏洞；<strong>适用于多处理机环境</strong></p></li><li><p>缺点：不满足“让权等待”原则，暂时无法进入临界区的进程会占用CPU并循环执行TSL指令，<strong>从而导致“忙等”。</strong></p></li></ul></li></ul></li></ul></li></ul><h4 id="3-信号量（♚）"><a href="#3-信号量（♚）" class="headerlink" title="3.信号量（♚）"></a>3.信号量（♚）</h4><ul><li><p>信号量的概念</p><ul><li>信号量其实就是一个变量(可以是一个整数，也可以是更复杂的记录型变量)，<br><strong>可以用一个信号量来表示系统中某种资源的数量</strong>，比如：系统中只有一台打印机，就可以设置一个初值为1的信号量。</li><li>信号量的分类<ul><li>信号量是一个特殊的整型变量，只有初始化和PV操作才能改变其值。通常，信号量分为互斥量和资源量</li><li>互斥量的<ul><li>初值一般为1，表示临界区只允许一个进程进入，从而实现互斥。</li><li>当互斥量等于0时，表示临界区已有一个进程进入，临界区外尚无进程等待</li><li><strong>当互斥量小于0时，表示临界区中有一个进程，互斥量的绝对值表示在临界区外等待进入的进程数</strong>。</li></ul></li><li>资源信号量<ul><li>初值可以是任意整数，表示可用的资源数</li><li><strong>当资源量小于0时，表示所有资源已全部用完，而且还有进程正在等待使用该资源，等待的进程数就是资源量的绝对值</strong>。</li></ul></li></ul></li><li>PV操作的概念<ul><li>用户进程可以通过使用操作系统提供的一对原语来对信号量进行操作，可以满足在进入区，退出区的操作一气呵成，<br>从而很方便的实现了进程互斥、进程同步。</li><li>wait(S)原语和signal(S)原语，wait、signal原语常简称为P、V操作，可以分别写为P(S)、V(S)</li><li><strong>PV操作是一种低级进程通信原语，由两个不可被中断的过程组成，但不是系统调用命令</strong></li></ul></li></ul></li><li><p>整型信号量（非重点）</p><ul><li><p>代码逻辑</p><ul><li><p>整型信号量被定义为一个用于表示资源数目的整型量S</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">wait</span>(S)&#123;<span class="comment">//wait原语，相当于“进入区”</span></span><br><span class="line"><span class="keyword">while</span>(S&lt;=<span class="number">0</span>);  <span class="comment">//如果资源数不够，就一直循环等待</span></span><br><span class="line">S=S<span class="number">-1</span>;   <span class="comment">//如果资源数够，则占用一个资源</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">signal</span>(S)&#123;  <span class="comment">//signal原语，相当于“退出区”</span></span><br><span class="line">S=S<span class="number">+1</span>;<span class="comment">//使用完资源后，在退出区释放资源</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>优点：检查”和“上锁”一气呵成避免了并发、异步导致的问题</p></li><li><p>存在的问题：不满足<strong>让权等待</strong>原则，会发生“忙等”</p></li></ul></li><li><p>记录型信号量（♛）</p><ul><li><p>记录型信号量的定义以及申请资源和释放资源</p></li><li><p><strong>P操作中，一定是先S.value—，之后可能需要执行block原语</strong><br><strong>V操作中，一定是先S.value++，之后可能需要执行wakeup原语</strong></p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span>&#123;</span><br><span class="line">    <span class="type">int</span> value; <span class="comment">//剩余资源数，S.value的初值表示系统中某种资源的数目</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">process</span> *L;  <span class="comment">//定义等待队列</span></span><br><span class="line">&#125;semaphore;   <span class="comment">//记录型信号量的定义</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//P操作：wait(s):某进程需要使用资源时 用wait原语申请资源</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">wait</span><span class="params">(semaphore S)</span></span>&#123;</span><br><span class="line">    S.value--;  </span><br><span class="line">    <span class="keyword">if</span>(S.value&lt;<span class="number">0</span>)&#123;</span><br><span class="line">        <span class="built_in">block</span>(S.L); <span class="comment">//如果剩余资源数不够，则使用block原语使进程从运行态进入阻塞态，并把其挂到信号量S的等待队列中（阻塞队列）</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//V操作：进程使用完资源后，通过signal原语释放</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">signal</span><span class="params">(semaphore S)</span></span>&#123;</span><br><span class="line">    S.value++;</span><br><span class="line">    <span class="keyword">if</span>(S.value&lt;=<span class="number">0</span>)&#123;</span><br><span class="line">        <span class="built_in">wakeup</span>(S.L);  <span class="comment">//释放资源后，若还有别的进程在等待这种资源，则使用wakeup原语唤醒等待队列中一个进程,该进程从阻塞态变为就绪态</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>该机制遵循了“让权等待”原则，不会出现“忙等”现象</strong></li></ul></li><li><p>利用信号量实现进程互斥</p><ul><li><p>过程</p><ul><li>分析并发进程的关键活动，划定临界区(如：对临界资源打印机的访问就应放在临界区)</li><li><strong>设置互斥信号量mutex，初值为1</strong>，（<strong>信号量表示进入临界区的名额</strong>）对不同的临界资源需要设置不同的互斥信号量。</li><li><strong>在临界区之前对信号量实行P操作，在临界区之后对信号量实行V操作，P、V操作必须成对出现</strong></li></ul></li><li><p>代码逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">semaphore mutex=<span class="number">1</span>;<span class="comment">//初始化信号量</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">P1</span>()&#123;</span><br><span class="line"><span class="built_in">P</span>(mutex); <span class="comment">//使用临界资源前需要加锁，信号量减一表示占用了一个名额</span></span><br><span class="line">临界区代码段,,,</span><br><span class="line"><span class="built_in">V</span>(mutex);<span class="comment">//使用临界资源后需要解锁，信号量加一表示归还了一个名额，若此时信号量小于等于零，则将阻塞队列中的进程唤醒进入就绪队列</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">P2</span>()&#123;</span><br><span class="line"><span class="built_in">P</span>(mutex);<span class="comment">//在没有访问临界区的名额时，此时将进入阻塞态（信号量减一后小于0则会触发阻塞）</span></span><br><span class="line">临界区代码段,,,</span><br><span class="line"><span class="built_in">V</span>(mutex);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>利用信号量实现同步</p><ul><li><p>过程</p><ul><li>进程同步问题是让本来异步并发的进程互相配合，有序推进。<br>需要分析什么地方需要实现“同步关系”，即必须保证“一前一后”执行的两个操作</li><li><strong>设置同步信号量S，初始为0</strong>，信号量S代表“某种资源”，刚开始是没有这种资源的。<br>P2需要使用这种资源，而又只能由P1产生这种资源</li><li><strong>在“前操作”之后执行V(S)，在“后操作”之前执行P(S)，前V后P</strong></li></ul></li><li><p>代码逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">semaphore S=<span class="number">0</span>;<span class="comment">//初始化同步信号量，初始值为0</span></span><br><span class="line"><span class="comment">//此时可以保证代码4一定在代码2之后执行</span></span><br><span class="line"><span class="built_in">P1</span>()&#123; <span class="built_in">P2</span>()&#123;</span><br><span class="line">代码<span class="number">1</span>;   <span class="built_in">P</span>(S);<span class="comment">//如果不按照顺序来执行的话，此时信号量减一后将会由于信号量小于等于0而被阻塞</span></span><br><span class="line">代码<span class="number">2</span>;   代码<span class="number">4</span>;</span><br><span class="line"><span class="built_in">V</span>(S);&#125;</span><br><span class="line"><span class="comment">//执行完V操作之后，信号量才能由0变为1</span></span><br><span class="line"><span class="comment">//如果执行完V操作之后，发现信号量小于等于0，说明已经有进程尝试先发运行，则通过唤醒原语将被阻塞的进程变为就绪态</span></span><br><span class="line">代码<span class="number">3</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>利用信号量实现前驱关系</p><ul><li><img src="https://s1.ax1x.com/2023/08/16/pPlpR5n.png" alt="pPlpR5n.png"></li></ul></li></ul></li></ul><h4 id="4-管程"><a href="#4-管程" class="headerlink" title="4.管程"></a>4.管程</h4><ul><li>引入管程的目的：解决信号量机制编程麻烦，易出错的问题</li><li>管程是由编程语言支持的进程同步机制</li><li>管程的组成<ul><li>局部于管程的<strong>共享数据结构</strong>说明</li><li>对该数据结构进行操作的<strong>一组过程（函数）</strong></li><li>对局部于管程的共享数据<strong>设置初始值</strong>的语句</li></ul></li><li>基本特征<ul><li><strong>局部于管程的数据只能被局部于管程的过程所访问</strong></li><li><strong>一个进程只有通过调用管程内的过程才能进入管程访问共享数据</strong></li><li><strong>每次仅允许一个进程在管程内执行某个内部过程</strong>。</li></ul></li><li>补充<ul><li>管程是被进程调用的，管程是语法范围，无法创建和撤销</li><li><strong>各进程必须互斥访问管程的特性是由编译器实现的</strong></li><li>可在管程中设置条件变量及等待/唤醒操作以解决同步问题</li></ul></li></ul><h4 id="5-经典同步问题（♚）"><a href="#5-经典同步问题（♚）" class="headerlink" title="5.经典同步问题（♚）"></a>5.经典同步问题（♚）</h4><ul><li><p>生产者-消费者问题</p><ul><li><p>问题分析</p><ul><li>系统中有一组生产者进程和一组消费者进程，生产者进程每次生产一个产品放入缓冲区，<br>消费者进程每次从缓冲区中取出一个产品并使用。(注：这里的“产品”理解为某种数据)</li><li>生产者、消费者共享一个初始为空、大小为n的缓冲区。<br>缓冲区是临界资源，各进程必须互斥地访问</li><li>只有缓冲区没满时，生产者才能把产品放入缓冲区，否则必须等待</li><li>只有缓冲区不空时，消费者才能从中取出产品，否则必须等待</li></ul></li><li><p>代码逻辑</p><ul><li><p>实现互斥是在同一进程中进行PV操作</p></li><li><p>实现两进程的同步关系，是在其中一个进程中执行P，另外一个进程中执行V（一前一后，前V后P）</p></li><li><p>相关信号量设置</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">semaphore mutex <span class="number">1</span>;</span><br><span class="line"><span class="comment">//互斥信号量，实现对缓冲区的互斥访问</span></span><br><span class="line">semaphore empty n;</span><br><span class="line"><span class="comment">//同步信号量，表示空闲缓冲区的数量</span></span><br><span class="line">semaphore full <span class="number">0</span>;</span><br><span class="line"><span class="comment">//同步信号量，表示产品的数量，也即非空缓冲区的数量</span></span><br></pre></td></tr></table></figure></li><li><p>生产者逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">producer</span>()&#123;</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">        生产一个产品;</span><br><span class="line">        <span class="built_in">P</span>(empty); <span class="comment">//消耗一个空闲缓冲区</span></span><br><span class="line">        <span class="built_in">P</span>(mutex);</span><br><span class="line">        把产品放入缓冲区;</span><br><span class="line">        <span class="built_in">V</span>(mutex);</span><br><span class="line">        <span class="built_in">V</span>(pull);   <span class="comment">//增加一个产品</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>消费者逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">consumer</span>()&#123;</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="built_in">P</span>(full); <span class="comment">//消耗一个产品</span></span><br><span class="line">        <span class="built_in">P</span>(mutex);</span><br><span class="line">        从缓冲区取出一个产品;</span><br><span class="line">        <span class="built_in">V</span>(mutex);</span><br><span class="line">        <span class="built_in">V</span>(empty);   <span class="comment">//增加一个空闲缓冲区</span></span><br><span class="line">        使用产品;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>相关的注意事项</p><ul><li>实现互斥的P操作一定要在实现同步的P操作之后，如果互换位置，出现“死锁”。</li><li>V操作不会导致进程阻塞，因此两个V操作顺序可以交换。</li></ul></li><li><p>PV操作题目的解题思路</p><ul><li>关系分析。找出题目中描述的各个进程，分析它们之间的同步、互斥关系</li><li>整理思路。根据各进程的操作流程确定P、V操作的大致顺序</li><li>设置信号量。设置需要的信号量，并根据题目条件确定信号量初值。<br>互斥信号量初值一般为1，同步信号量的初始值要看对应资源的初始值是多少</li></ul></li></ul></li><li><p>多生产者-多消费者问题</p><ul><li><p>问题描述</p><ul><li>桌子上有一只盘子，每次只能向其中放入一个水果。爸爸专向盘子中放苹果，妈妈专向盘子中放橘子，<br>儿子专等着吃盘子中的橘子，女儿专等着吃盘子中的苹果。</li><li>只有盘子空时，爸爸或妈妈才可向盘子中放一个水果。</li><li>仅当盘子中有自己需要的水果时，儿子或女儿可以从盘子中取出水果。</li></ul></li><li><p>问题分析</p><ul><li>互斥关系(mutex=1)：对缓冲区（盘子）的访问要互斥地进行</li><li>同步关系（一前一后）<ul><li>父亲将苹果放入盘子后，女儿才能取苹果</li><li>母亲将橘子放入盘子后，儿子才能取橘子</li><li>只有盘子为空时，父亲或母亲才能放入水果<br>“盘子为空”这个事件可以由儿子或女儿触发，事件发生后才允许父亲或母亲放水果</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/16/pPlZ0oT.png" alt="pPlZ0oT.png"></li></ul></li></ul></li><li><p>代码逻辑</p><ul><li><p>信号量设置</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">semaphore mutex=<span class="number">1</span>;<span class="comment">//实现互斥访问盘子（缓冲区）</span></span><br><span class="line">semaphore apple=<span class="number">0</span>;<span class="comment">//盘子中有几个苹果</span></span><br><span class="line">semaphore orange=<span class="number">0</span>;<span class="comment">//盘子中有几个橘子</span></span><br><span class="line">semaphore plate=<span class="number">1</span>;  <span class="comment">//盘子中还可以放多少个水果（缓冲区最多一个）</span></span><br></pre></td></tr></table></figure></li><li><p>生产者逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">dad</span>()&#123;</span><br><span class="line">  <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">准备一个苹果；</span><br><span class="line"><span class="built_in">P</span>(plate);  <span class="comment">//消耗盘子的一个空位</span></span><br><span class="line"><span class="built_in">P</span>(mutex);</span><br><span class="line">把苹果放入盘子；</span><br><span class="line"><span class="built_in">V</span>(mutex);</span><br><span class="line"><span class="built_in">V</span>(apple);<span class="comment">//生产一个苹果</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">mom</span>()&#123;</span><br><span class="line"> <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">    准备一个橘子；</span><br><span class="line"><span class="built_in">P</span>(plate); <span class="comment">//消耗盘子的一个空位</span></span><br><span class="line"><span class="built_in">P</span>(mutex);</span><br><span class="line">把橘子放入盘子；</span><br><span class="line"><span class="built_in">V</span>(mutex);</span><br><span class="line"><span class="built_in">V</span>(orange);  <span class="comment">//生产一个橘子</span></span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><pre><code>- 消费者逻辑  <figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">daughter</span>()&#123;</span><br><span class="line">  <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line"><span class="built_in">P</span>(apple);  <span class="comment">//消耗一个苹果</span></span><br><span class="line"><span class="built_in">P</span>(mutex);</span><br><span class="line">从盘中取出苹果；</span><br><span class="line"><span class="built_in">V</span>(mutex);</span><br><span class="line"><span class="built_in">V</span>(plate);   <span class="comment">//释放一个空位</span></span><br><span class="line">吃掉苹果；</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">son</span>()&#123;</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">  <span class="built_in">P</span>(orange);  <span class="comment">//消耗一个橘子</span></span><br><span class="line">  <span class="built_in">P</span>(mutex);</span><br><span class="line">  从盘中取出橘子;</span><br><span class="line">  <span class="built_in">V</span>(mutex);</span><br><span class="line">  <span class="built_in">V</span>(plate); <span class="comment">//释放一个空位</span></span><br><span class="line">  吃掉橘子;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></code></pre><ul><li><p>相关的注意事项</p><ul><li>即使不设置专门的互斥变量mutex,也不会出现多个进程同时访问盘子的现象</li><li>在生产者-消费者问题中，如果缓冲区大小为1，那么有可能不需要设置互斥信号量就可以实现互斥访问缓冲区的功能</li></ul></li></ul><ul><li><p>吸烟者问题</p><ul><li><p>问题描述</p><ul><li>假设一个系统有三个抽烟者进程和一个供应者进程。每个抽烟者不停地卷烟并抽掉它，但是要卷起并抽掉一支烟，抽烟者需要有三种材料：烟草、纸和胶水。<br>三个抽烟者中，第一个拥有烟草、第二个拥有纸、第三个拥有胶水。</li><li>供应者进程无限地提供三种材料，供应者每次将两种材料放桌子上，拥有剩下那种材料的抽烟者卷一根烟并抽掉它，并给供应者进程一个信号告诉完成了，供应者就会放另外两种材料再桌上，这个过程一直重复（<strong>让三个吸烟者轮流吸烟</strong>）</li></ul></li><li><p>问题分析</p><ul><li>属于可生产多种产品的单生产者-多消费者问题</li><li>互斥关系：互斥的访问桌子（容量为1的缓冲空间）</li><li>同步关系<ul><li>桌上有组合一→第一个抽烟者取走东西</li><li>桌上有组合二→第二个抽烟者取走东西</li><li>桌上有组合三→第三个抽烟者取走东西</li><li>发出完成信号→供应者将下一个组合放到桌上</li></ul></li><li>由于缓冲区大小为1，同一时刻四个同步信号量中至多有一个的值为1<br>因此不需要设置互斥的信号量</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/16/pPle1n1.png" alt="pPle1n1.png"></li></ul></li></ul></li><li><p>代码逻辑</p><ul><li><p>信号量设置</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">semaphore offerl=<span class="number">0</span>  <span class="comment">//桌上组合一的数量</span></span><br><span class="line">semaphore offer2=<span class="number">0</span>; <span class="comment">//桌上组合二的数量</span></span><br><span class="line">semaphore offer3=<span class="number">0</span>  <span class="comment">//桌上组合三的数量</span></span><br><span class="line">semaphore finish=<span class="number">0</span>; <span class="comment">//抽烟是否完成</span></span><br><span class="line"><span class="type">int</span> i=<span class="number">0</span>; <span class="comment">//用于实现三个抽烟者轮流抽烟”</span></span><br></pre></td></tr></table></figure></li><li><p>生产者逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">provider</span>()&#123;</span><br><span class="line">  <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">    <span class="keyword">if</span>(i==<span class="number">0</span>)&#123;</span><br><span class="line">    将组合一放桌上;</span><br><span class="line">  <span class="built_in">V</span>(offer1);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(i==<span class="number">1</span>)&#123;</span><br><span class="line">  将组合二放桌上;</span><br><span class="line">  <span class="built_in">V</span>(offer2);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(i==<span class="number">2</span>)&#123;</span><br><span class="line">  将组合三放桌上;</span><br><span class="line">  <span class="built_in">V</span>(offer3);</span><br><span class="line">    &#125;</span><br><span class="line">    i=(i<span class="number">+1</span>)%<span class="number">3</span>;</span><br><span class="line">    <span class="built_in">P</span>(finish);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>消费者逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">smoker1</span>()&#123;</span><br><span class="line">  <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line"><span class="built_in">P</span>(offer1);</span><br><span class="line">从桌上拿走组合一;卷烟抽掉;</span><br><span class="line"><span class="built_in">V</span>(finish);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>读者-写者问题</p><ul><li><p>问题描述</p><ul><li>有读者和写者两组并发进程，共享一个文件，当两个或两个以上的读进程同时访问共享数据时不会产生副作用，<br>但若某个写进程和其他进程（读进程或写进程）同时访问共享数据时则可能导致数据不一致的错误。</li><li>①允许多个读者可以同时对文件执行读操作</li><li>②只允许一个写者往文件中写信息</li><li>③任一写者在完成写操作之前不允许其他读者或写者工作</li><li>④写者执行写操作前，应让已有的读者和写者全部退出。</li></ul></li><li><p>问题分析</p><ul><li>两类进程：写进程、读进程</li><li>互斥关系：写进程一写进程、写进程一读进程。<br>读进程与读进程不存在互斥问题。</li></ul></li><li><p>代码逻辑</p><ul><li><p>信号量设置</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">semaphore rw=<span class="number">1</span>;  <span class="comment">//用于实现对共享文件的互斥访问</span></span><br><span class="line"><span class="type">int</span> count=<span class="number">0</span>;   <span class="comment">//记录当前有几个读进程在访问文件</span></span><br><span class="line">semaphore mutex=l;<span class="comment">//用于保证对count变量的互斥访问</span></span><br><span class="line"><span class="comment">//count变量的检查和赋值无法一气呵成，因此可以设置另一个互斥信号量来保证各读进程对count的访问是互斥的</span></span><br><span class="line">semaphore w=<span class="number">1</span>;<span class="comment">//用于实现“写优先”,可以避免由于读者连续访问而导致的写者的饥饿</span></span><br><span class="line"><span class="comment">//读者1→写者1→读者2,出现此类访问时，可以将文件访问权转移给写者</span></span><br></pre></td></tr></table></figure></li><li><p>写者逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">writer</span>()&#123;</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="built_in">p</span>(w);   <span class="comment">//实现写优先</span></span><br><span class="line">        <span class="built_in">p</span>(rw);   <span class="comment">//写之前加锁</span></span><br><span class="line">        写文件;</span><br><span class="line">        <span class="built_in">V</span>(rw);<span class="comment">//写之后解锁</span></span><br><span class="line">        <span class="built_in">V</span>(w);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>读者逻辑</p><figure class="highlight cc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">reader</span>()&#123;</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="built_in">p</span>(w);        <span class="comment">//实现写优先</span></span><br><span class="line">        <span class="built_in">p</span>(mutex);    <span class="comment">//各读进程互斥访问count</span></span><br><span class="line">        <span class="keyword">if</span>(count==<span class="number">0</span>) <span class="comment">//由第一个读进程负责控制与写进程的互斥访问的加锁工作</span></span><br><span class="line">            <span class="built_in">p</span>(rw); <span class="comment">//读之前加锁</span></span><br><span class="line">        count++; <span class="comment">//访问文件的读进程数+1</span></span><br><span class="line">        <span class="built_in">V</span>(mutex);  </span><br><span class="line">        <span class="built_in">V</span>(w);</span><br><span class="line">        读文件;</span><br><span class="line">        <span class="built_in">P</span>(mutex);   <span class="comment">//各读进程互斥访问count</span></span><br><span class="line">        count--;<span class="comment">//访问文件的读进程数-1</span></span><br><span class="line">        <span class="keyword">if</span>(count==<span class="number">0</span>) <span class="comment">//由最后一个读进程负责控制与与写进程的互斥访问的解锁工作</span></span><br><span class="line">            <span class="built_in">V</span>(rw);   <span class="comment">//当前没有读文件访问文件后才解锁</span></span><br><span class="line">        <span class="built_in">V</span>(mutex);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>注意事项</p><ul><li>其核心思想在于设置了一个计数器count用来记录当前正在访问共享文件的读进程数。<br>可以用count的值来判断当前进入的进程是否是第一个/最后一个读进程，从而做出不同的处理，从而实现写者与读者互斥访问，但是读者间不互斥访问</li><li>对count变量的检查和赋值不能一气呵成导致了一些错误，如果需要实现“一气呵成”，自然应该想到用互斥信号量</li><li>最后，还要认真体会如何解决“写进程饥饿”问题的</li></ul></li></ul></li></ul><h3 id="四-死锁（✪）"><a href="#四-死锁（✪）" class="headerlink" title="四.死锁（✪）"></a>四.死锁（✪）</h3><h4 id="1-死锁的概念"><a href="#1-死锁的概念" class="headerlink" title="1.死锁的概念"></a>1.死锁的概念</h4><ul><li>死锁、饥饿与死循环<ul><li>死锁：<strong>在并发环境下，各进程因竞争资源而造成的一种互相等待对方手里的资源，导致各进程都阻塞，都无法向前推进的现象</strong><br><strong>发生死锁后若无外力干涉，这些进程都将无法向前推进</strong></li><li>饥饿：由于长期得不到想要的资源，某进程无法向前推进的现象。<br>比如：在短进程优先(SPF)算法中，若有源源不断的短进程到来，则长进程将一直得不到处理机，从而发生长进程“饥饿”</li><li>死循环：某进程执行过程中一直跳不出某个循环的现象。有时是因为程序逻辑bug导致的，有时是程序员故意设计的。</li><li>三者的区别<ul><li><img src="https://s1.ax1x.com/2023/08/17/pPlyeLd.png" alt="pPlyeLd.png"></li></ul></li></ul></li><li>死锁产生的必要条件<ul><li><strong>产生死锁必须同时满足一下四个条件，只要其中任一条件不成立，死锁就不会发生</strong>。</li><li>互斥条件：只有对必须互斥使用的资源的争抢才会导致死锁 (如哲学家的筷子、打印机设备)<br>像内存、扬声器这样可以同时让多个进程使用的资源是不会导致死锁的（因为进程不用阻塞等待这种资源）</li><li>不剥夺条件：进程所获得的资源在未使用完之前，不能由其他进程强行夺走，只能主动释放。</li><li>请求和保持条件：<strong>进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源又被其他进程占有，</strong><br><strong>此时请求进程被阻塞，但又对自己己有的资源保持不放。只需要一个资源的进程不会进入死锁</strong></li><li>循环等待条件：<strong>存在一种进程资源的循环等待链，链中的每一个进程已获得的资源同时被下一个进程所请求</strong><br><strong>死锁发生至少有两个进程</strong><ul><li><strong>发生死锁时一定有循环等待，但是发生循环等待时未必死锁</strong></li><li>如果同类资源数大于1，则即使有循环等待，也未必发生死锁</li><li>但如果系统中每类资源都只有一个，那循环等待就是死锁的充分必要条件了</li></ul></li></ul></li><li>死锁产生的原因（对不可剥夺资源的不合理分配，可能导致死锁）<ul><li>对系统资源的竞争<ul><li>各进程对不可剥夺的资源（如打印机）的竞争可能引起死锁，对可剥夺的资源(CPU)的竞争是不会引起死锁的</li></ul></li><li>进程推进顺序非法，请求和释放资源的顺序不当，也同样会导致死锁。<ul><li>如并发执行的进程P1、P2分别申请并占有了资源R1、R2，之后进程P1又紧接着申请资源R2，而进程P2又申请资源R1<br>两者会因为申请的资源被对方占有而阻塞，从而发生死锁。</li></ul></li><li>信号量的使用不当也会造成死锁。<ul><li>如生产者消费者问题中，如果实现互斥的P操作在实现同步的P操作之前，就有可能导致死锁。<br>可以把互斥信号量、同步信号量也看做是一种抽象的系统资源</li></ul></li></ul></li><li><p>死锁的处理策略</p><ul><li><p>不允许死锁发生</p><ul><li>静态策略：预防死锁，破坏死锁产生的四个必要条件中的一个或几个。</li><li>动态策略：避免死锁，用某种方法防止系统进入不安全状态，从而避免死锁（银行家算法）</li></ul></li><li><p>允许死锁发生</p><ul><li>死锁的检测和解除，允许死锁的发生，不过操作系统会负责检测出死锁的发生，然后采取某种措施解除死锁。</li></ul></li><li><p>预防死锁和避免死锁都属于事先预防策略，预防死锁的限制条件比较严格，实现起来较为简单，但往往导致系统的效率低，资源利用率低，<br>避免死锁的限制条件相对宽松，资源分配后需要通过算法来判断是否进入不安全状态，实现起来较为复杂。</p></li></ul></li><li><p>例题（✪）</p><ul><li><p><strong>重要定则：n个进程，每个进程需要k个资源，则当总资源数至少为$n(k-1)+1$时，不会发生死锁</strong></p><ul><li><img src="https://s1.ax1x.com/2023/08/17/pP1FwsP.png" alt="pP1FwsP.png"></li></ul></li><li><p>例2，本题即为各自需要的资源数-1之和再加一：2+3+4+1=10</p><ul><li><img src="https://s1.ax1x.com/2023/08/17/pP1F5ZT.png" alt="pP1F5ZT.png"></li></ul></li><li><p>例3，此时结合题目来看，算出至少得资源数为3*(2-1)+1=4，此时所拥有的资源数刚好满足，不会发生死锁</p><ul><li><img src="https://s1.ax1x.com/2023/08/17/pP1kAyt.png" alt="pP1kAyt.png"></li></ul></li><li><p>例4，本题选C，此时为循环等待的经典例子</p><ul><li><img src="https://s1.ax1x.com/2023/08/17/pP1A89H.png" alt="pP1A89H.png"></li></ul></li></ul></li></ul><h4 id="2-死锁的预防"><a href="#2-死锁的预防" class="headerlink" title="2.死锁的预防"></a>2.死锁的预防</h4><ul><li>破坏互斥条件（无法破坏）<ul><li>互斥条件：只有对必须互斥使用的资源的争抢才会导致死锁。<br>如果把只能互斥使用的资源改造为允许共享使用，则系统不会进入死锁状态。</li><li>操作系统可以采用SPOOLing技术把独占设备在逻辑上改造成共享设备。比如，用SPOOLing技术将打印机改造为共享设备</li><li>缺点：并不是所有的资源都可以改造成可共享使用的资源。并且为了系统安全，很多地方还必须保护这种互斥性。</li></ul></li><li>破坏不剥夺条件<ul><li>不剥夺条件：进程所获得的资源在未使用完之前，不能由其他进程强行夺走，只能主动释放。</li><li>方案一：<strong>当某个进程请求新的资源得不到满足时，它必须立即释放保持的所有资源，待以后需要时再重新申请。</strong><br>即使某些资源尚未使用完，也需要主动释放，从而破坏了不可剥夺条件。</li><li>方案二：<strong>当某个进程需要的资源被其他进程所占有的时候，可以由操作系统协助，将想要的资源强行剥夺。</strong><br>这种方式一般需要考虑各进程的优先级（比如：剥夺调度方式，就是将处理机资源强行剥夺给优先级更高的进程使用）</li><li>策略的缺点<ul><li>实现起来比较复杂</li><li>释放已获得的资源可能造成前一阶段工作的失效。因此这种方法一般只适用于易保存和恢复状态的资源，如CPU.</li><li>反复地申请和释放资源会增加系统开销，降低系统吞吐量</li><li><strong>若采用方案一，意味着只要暂时得不到某个资源，之前获得的那些资源就都需要放弃，以后再重新申请。</strong><br><strong>如果一直发生这样的情况，就会导致进程饥饿。</strong></li></ul></li></ul></li><li>破坏请求和保持条件<ul><li>请求和保持条件：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源又被其他进程占有，<br>此时请求进程被阻塞，但又对自己已有的资源保持不放。</li><li>可以采用静态分配方法，<strong>即进程在运行前一次申请完它所需要的全部资源，在它的资源未满足前，不让它投入运行</strong>。<br>一旦投入运行后，这些资源就一直归它所有，该进程就不会再请求别的任何资源了。</li><li>该策略的缺点<ul><li>有些资源可能只需要用很短的时间，因此如果进程的整个运行期间都一直保持着所有资源，就会造成严重的资源浪费，资源利用率极低。</li><li>该策略也有可能导致某些进程饥饿</li></ul></li></ul></li><li><p>破坏循环等待条件</p><ul><li>循环等待条件：存在一种进程资源的循环等待链，链中的每一个进程已获得的资源同时被下一个进程所请求。</li><li>可采用顺序资源分配法。首先<strong>给系统中的资源编号，规定每个进程必须按编号递增的顺序请求资源</strong>，同类资源（即编号相同的资源）一次申请完，<strong>限制用户申请资源的顺序</strong></li><li><p>原理分析</p><ul><li>一个进程只有已占有小编号的资源时，才有资格申请更大编号的资源。<br>按此规则，已持有大编号资源的进程不可能逆向地回来申请小编号的资源，从而就不会产生循环等待的现象</li><li>在任何一个时刻，总有一个进程拥有的资源编号是最大的，那这个进程申请之后的资源必然畅通无阻。<br>不可能出现所有进程都阻塞的死锁现象</li></ul></li><li><p>该策略的缺点</p><ul><li>不方便增加新的设备，因为可能需要重新分配所有的编号</li><li>进程实际使用资源的顺序可能和编号递增顺序不一致，会导致资源浪费</li><li>必须按规定次序申请资源，用户编程麻烦。</li></ul></li></ul></li></ul><h4 id="3-死锁避免（♚）"><a href="#3-死锁避免（♚）" class="headerlink" title="3.死锁避免（♚）"></a>3.死锁避免（♚）</h4><ul><li>安全序列与安全状态<ul><li><strong>安全序列是指如果系统按照这种序列分配资源，则每个进程都能顺利完成，安全序列可能有多个</strong>。</li><li><strong>只要能找出一个安全序列，系统就是安全状态。如果分配了资源之后，系统中找不出任何一个安全序列，</strong><br><strong>系统就进入了不安全状态。这就意味着之后可能所有进程都无法顺利的执行下去。</strong></li><li>当如果有进程提前归还了一些资源，那系统也有可能重新回到安全状态，不过在分配资源之前总是要考虑到最坏的情况。</li></ul></li><li>安全状态与死锁的联系<ul><li><strong>如果系统处于安全状态，就一定不会发生死锁。如果系统进入不安全状态，就可能发生死锁</strong><br><strong>处于不安全状态未必就是发生了死锁，但发生死锁时一定是在不安全状态</strong></li><li>因此可以在资源分配之前预先判断这次分配是否会导致系统进入不安全状态，以此决定是否答应资源分配请求。<br>这也是“银行家算法”的核心思想。</li></ul></li><li>银行家算法<ul><li>核心思想（避免系统进入不安全状态）<ul><li>在每次进行资源分配时，它首先检查系统是否有足够的资源满足要求，若有则先进行试分配，并对分配后的新状态进行安全性检查。</li><li><strong>在试分配时，利用安全性算法判断此次分配是否会导致系统进入不安全状态。<br>若新状态安全，则正式分配上述资源，如果会进入不安全状态，就暂时不答应这次请求，让该进程先阻塞等待。</strong></li><li><strong>银行家算法只能避免系统进入死锁，不能用于判断系统是否处于死锁</strong></li></ul></li><li>银行家算法手算规则（核心是找到安全序列）<ul><li>首先可以找到P1,P3满足当前剩余分配的资源，可以顺利分配并回收资源，此时将P1/P3加入安全序列<ul><li><img src="https://s1.ax1x.com/2023/08/17/pPlolY4.png" alt="pPlolY4.png"></li></ul></li><li>回收之后，即可以满足所有的需求，此时得出安全序列，说明处于安全状态，不会发生死锁<ul><li><img src="https://s1.ax1x.com/2023/08/17/pPlo4hQ.png" alt="pPlo4hQ.png"></li></ul></li></ul></li><li>银行家算法的数据结构描述（系统中有n个进程,m种资源）<ul><li>可利用资源向量Available<ul><li>含有m个元素的数组，其中每个元素代表一类可用的资源数目。$Available[j]=K$表示系统中现有$R_j$类资源$K$个</li></ul></li><li>最大需求矩阵Max<ul><li>n×m矩阵，定义系统中n个进程中的每个进程对m类资源的最大需求。</li><li>一行代表一个进程，一列代表一类资源，$Max[i,j]=K$表示进程$i$需要$R_j$类资源的最大数目为K.</li></ul></li><li>分配矩阵Allocation<ul><li>n×m矩阵，定义系统中每类资源当前已分配给每个进程的资源数。</li><li>$Allocation[i,j]=K$表示进程$i$当前已分得$R_j$类资源的数目为K。</li></ul></li><li>需求矩阵Need<ul><li>n×m矩阵，表示每个进程接下来最多还需要多少资源。</li><li>$Need[i,j]=K$表示进程$i$还需要$R_j$类资源的数目为K.</li></ul></li><li>上述三个矩阵间存在下述关系：$Need=Max-Allocation$</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/17/pPl7dRH.png" alt="pPl7dRH.png"></li></ul></li></ul></li><li>银行家算法步骤（给出一个请求：Request）<ul><li>检查此次申请是否超过了之前声明的最大需求数（Need）</li><li>检查此时系统剩余的可用资源是否还能满足这次请求（Available）</li><li>试探着分配，更改各数据结构<ul><li>Available=Available-Request</li><li>Allocation=Allocation+Request</li><li>Need=Need-Request</li></ul></li><li>用安全性算法检查此次分配是否会导致系统进入不安全状态，若安全，才正式分配<br>否则，恢复相应数据，让进程阻塞等待。</li></ul></li><li>安全性算法步骤<ul><li>检查当前的剩余可用资源是否能满足某个进程的最大需求，如果可以，就把该进程加入安全序列，<br>并把该进程持有的资源全部回收。不断重复上述过程，看最终是否能让所有进程都加入安全序列。</li></ul></li></ul></li></ul><h4 id="4-死锁检测与解除"><a href="#4-死锁检测与解除" class="headerlink" title="4.死锁检测与解除"></a>4.死锁检测与解除</h4><ul><li><p><strong>在死锁的检测和解除中，系统为进程分配资源时不采取任何措施，但提供死锁的检测和解除手段</strong></p></li><li><p>死锁检测</p><ul><li>检测死锁的方法<ul><li>用某种数据结构来保存资源的请求和分配信息</li><li>提供一种算法，利用上述信息来检测系统是否已进入死锁状态</li></ul></li><li>资源分配图<ul><li>用圆圈代表一个进程，用框代表一类资源</li><li>由于一种类型的资源可能有多个，因此用框中的一个圆代表一类资源中的一个资源。</li><li>从进程到资源的有向边称为请求边，表示该进程申请一个单位的该类资源；</li><li>从资源到进程的边称为分配边，表示该类资源已有一个资源分配给了该进程</li><li>图片<ul><li>进程P1已经分得了两个R1资源，并又请求一个R2资源；<br>进程P2分得了一个R1资源和一个R2资源，并又请求一个R1资源</li><li><img src="https://s1.ax1x.com/2023/08/17/pPlqedP.png" alt="pPlqedP.png"></li></ul></li></ul></li><li>死锁检测算法<ul><li>在资源分配图中，找出既不阻塞又不是孤点的进程P<br>即找出一条有向边与它相连，且该有向边对应资源的申请数量小于等于系统中己有空闲资源数量（资源数量-出度）</li><li>若所有的连接该进程的边均满足上述条件，则这个进程能继续运行直至完成，然后释放它所占有的所有资源<br>消去它所有的请求边和分配边，使之称为孤立的结点。</li><li>进程P所释放的资源，可以唤醒某些因等待这些资源而阻塞的进程，原来的阻塞进程可能变为非阻塞进程。<br>进行一系列简化后，若能消去途中所有的边，则称该图是可完全简化的。</li><li><strong>如果最终不能消除所有边，那么此时就是发生了死锁，最终还连着边的那些进程就是处于死锁状态的进程</strong><br><strong>资源分配图是不可完全简化的，称为死锁定理</strong></li><li><strong>死锁的充分必要条件是每种资源只有一个，并出现环路</strong></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/17/pPlqqFf.png" alt="pPlqqFf.png"></li></ul></li></ul></li></ul></li><li>死锁解除<ul><li>资源剥夺法：挂起（暂时放到外存上）某些死锁进程，并抢占它的资源，将这些资源分配给其他的死锁进程。<br>但是应防止被挂起的进程长时间得不到资源而饥饿。</li><li>撤销进程法（或称终止进程法）<ul><li>强制撤销部分、甚至全部死锁进程，并剥夺这些进程的资源。这种方式的优点是实现简单，但所付出的代价可能会很大<br>因为有些进程可能己经运行了很长时间，已经接近结束了，一旦被终止可谓功亏一篑，以后还得从头再来。</li></ul></li><li>进程回退法。让一个或多个死锁进程回退到足以避免死锁的地步。这就要求系统要记录进程的历史信息，设置还原点。</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;操作系统第二章-进程与线程&quot;&gt;&lt;a href=&quot;#操作系统第二章-进程与线程&quot; class=&quot;headerlink&quot; title=&quot;操作系统第二章 进程与线程&quot;&gt;&lt;/a&gt;操作系统第二章 进程与线程&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;计算机学科基础：操作系统第二章进程与线程的学习笔记&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="计算机基础学习笔记" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="操作系统" scheme="http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>操作系统第一章-计算机系统概述</title>
    <link href="http://example.com/2024/08/12/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/"/>
    <id>http://example.com/2024/08/12/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/</id>
    <published>2024-08-11T17:32:00.000Z</published>
    <updated>2024-08-11T17:58:29.211Z</updated>
    
    <content type="html"><![CDATA[<h2 id="操作系统第一章-计算机系统概述"><a href="#操作系统第一章-计算机系统概述" class="headerlink" title="操作系统第一章 计算机系统概述"></a>操作系统第一章 计算机系统概述</h2><blockquote><p>计算机学科基础：操作系统第一章计算机系统概述的学习笔记</p></blockquote><span id="more"></span><h3 id="一-操作系统的基本概念（✠）"><a href="#一-操作系统的基本概念（✠）" class="headerlink" title="一.操作系统的基本概念（✠）"></a>一.操作系统的基本概念（✠）</h3><h4 id="1-操作系统的基本概念"><a href="#1-操作系统的基本概念" class="headerlink" title="1.操作系统的基本概念"></a>1.操作系统的基本概念</h4><ul><li>操作系统是指控制和管理整个计算机系统的硬件与软件资源，合理地组织、调度计算机的工作与资源的分配，<br>进而为用户和其他软件提供方便接口与环境的程序集合。</li><li>操作系统是计算机系统中最基本的系统软件。</li></ul><h4 id="2-操作系统的特征（✪）"><a href="#2-操作系统的特征（✪）" class="headerlink" title="2.操作系统的特征（✪）"></a>2.操作系统的特征（✪）</h4><ul><li>1.并发<ul><li>并发是指两个或多个事件在同一时间间隔内发生，这些事件宏观上是同时发生的，但微观上是交替发生的</li><li>并行：指两个或多个事件在同一时刻同时发生。</li><li>操作系统的并发性是指计算机系统中同时存在多个运行的程序，因此它具有处理和调度多个程序同时执行的能力。<br>在操作系统中，引入进程的目的是使程序能并发执行。</li><li>多道技术需要进程间能够并发，并发性的实现需要中断功能的支持</li><li>单核CPU同一时刻只能执行一个程序，各个程序只能并发地执行<br>多核CPU同一时刻可以同时执行多个程序，多个程序可以并行地执行</li></ul></li><li>2.共享<ul><li>共享即资源共享，是指系统中的资源可供内存中多个并发执行的进程共同使用</li><li>资源共享的两种方式<ul><li>互斥共享方式<ul><li>规定在一段时间内只允许一个进程访问该资源。</li><li>在一段时间内只允许一个进程访问的资源称为临界资源</li><li>访问资源过程（互斥式共享）<ul><li>进程A访问某个资源时，必须先提出请求，若此时该资源空闲，则系统便将之分配给进程A使用，</li><li>此后有其他进程也要访问该资源时(只要A未用完)就必须等待。</li><li>仅当进程A访问完并释放该资源后，才允许另一个进程对该资源进行访问。</li></ul></li></ul></li><li>同时访问方式<ul><li>这类资源允许在一段时间内由多个进程“同时”访问，如磁盘设备。</li><li>“同时”通常是宏观上的，<br>而在微观上，这些进程可能是交替地对该资源进行访问即“分时共享”的。</li></ul></li></ul></li><li><strong>并发和共享是操作系统的两个最基本的特征，两者之间互为存在的条件</strong></li></ul></li><li>3.虚拟<ul><li>虚拟是指把一个物理上的实体变为若干逻辑上的对应物，<br>物理实体（前者）是实际存在的，而逻辑上对应物（后者）是用户感受到的。</li><li>虚拟技术<ul><li>时分复用技术（虚拟处理器）<ul><li>通过多道程序设计技术，采用让多道程序并发执行的方法，来分时使用一个处理器</li><li>虽然只有一个处理器，但它能同时为多个用户服务，使每个终端用户都感觉有一个中央处理器在专门为它服务。</li><li>利用多道程序设计技术把一个物理上的CPU虚拟为多个逻辑上的CPU，称为虚拟处理器。</li></ul></li><li>空分复用技术（虚拟存储器）<ul><li>将一台机器的物理存储器变为虚拟存储器，以便从逻辑上扩充存储器的容量。</li></ul></li></ul></li></ul></li><li>4.异步<ul><li>多道程序环境允许多个程序并发执行，但由于资源有限，进程的执行并不是一贯到底的，而是走走停停的，<br>它以不可预知的速度向前推进，这就是进程的异步性。</li><li>异步性使得操作系统运行在一种随机的环境下，可能导致进程产生与时间有关的错误<br>但是只要运行环境相同，操作系统就须保证多次运行进程后都能获得相同的结果。</li></ul></li></ul><h4 id="3-操作系统的目标与功能"><a href="#3-操作系统的目标与功能" class="headerlink" title="3.操作系统的目标与功能"></a>3.操作系统的目标与功能</h4><ul><li>操作系统作为计算机系统资源的管理者<ul><li>处理机管理<ul><li>在多道程序环境下，处理机的分配和运行都以进程（或线程）为基本单位，<br>因而对处理机的管理可归结为对进程的管理。</li><li>并发是指在计算机内同时运行多个进程，因此进程何时创建、何时撤销、如何管理、如何避免冲突、合理共享就是进程管理的最主要的任务。</li><li>进程管理的主要功能包括进程控制、进程同步、进程通信、死锁处理、处理机调度等。</li></ul></li><li>存储器管理<ul><li>存储器管理是为了给多道程序的运行提供良好的环境，方便用户使用及提高内存的利用率</li><li>主要包括内存分配与回收、地址映射、内存保护与共享和内存扩充等功能。</li></ul></li><li>文件管理<ul><li>计算机中的信息都是以文件的形式存在的，操作系统中负责文件管理的部分称为文件系统。</li><li>文件管理包括文件存储空间的管理、目录管理及文件读写管理和保护等。</li></ul></li><li>设备管理<ul><li>设备管理的主要任务是完成用户的I/O请求，方便用户使用各种设备，并提高设备的利用率，</li><li>主要包括缓冲管理、设备分配、设备处理和虚拟设备等功能。</li></ul></li></ul></li><li>操作系统提供给用户和其他软件方便的接口和环境<br>向上层提供方便易用的服务<ul><li>为了让用户方便、快捷、可靠地操纵计算机硬件并运行自己的程序，操作系统提供了用户接口，分为两类。</li><li>命令接口（直接给用户使用）<ul><li>用户利用这些操作命令来组织和控制作业的执行</li><li>联机命令接口（强调交换性，说一句做一句）<ul><li><strong>又称交互式命令接口</strong>，适用于分时或实时系统的接口，由一组键盘操作命令组成。</li><li>用户通过控制台或终端输入操作命令，向系统提出各种服务要求。</li><li>用户每输入一条命令，控制权就转给操作系统的命令解释程序，<br>然后由命令解释程序解释并执行输入的命令，完成指定的功能</li><li>之后，控制权转回控制台或终端，此时用户又可输入下一条命令</li></ul></li><li>脱机命令接口（说一堆做一堆）<ul><li><strong>又称批处理命令接口</strong>，适用于批处理系统，它由一组作业控制命令组成。</li><li>脱机用户不能直接干预作业的运行，而应事先用相应的作业控制命令写成一份作业操作说明书，<br>连同作业一起提交给系统。</li><li>系统调度到该作业时，由系统中的命令解释程序逐条解释执行作业说明书上的命令，从而间接地控制作业的运行</li></ul></li></ul></li><li><strong>程序接口（也称系统调用，给软件和程序员使用）</strong><ul><li><strong>程序接口由一组系统调用（也称广义指令）组成</strong>。用户通过在程序中使用这些系统调用来请求操作系统为其提供服务<br>如使用各种外部设备、申请分配和回收内存及其他各种要求。</li><li>当前最为流行的是图形用户界面(GUI)，即图形接口。GUI最终是通过调用程序接口实现的<br>严格来说，图形接口不是操作系统的一部分，但图形接口所调用的系统调用命令是操作系统的一部分。</li></ul></li></ul></li><li>操作系统实现了对计算机资源的扩充<ul><li>没有任何软件支持的计算机称为裸机，它仅构成计算机系统的物质基础，<br>而实际呈现在用户面前的计算机系统是经过若干层软件改造的计算机。裸机在最里层，其外面是操作系统。</li><li>操作系统所提供的资源管理功能和方便用户的各种服务功能，将裸机改造成功能更强、使用更方便的机器</li><li>因此，我们通常把覆盖了软件的机器称为扩充机器或虚拟机</li></ul></li></ul><h3 id="二-操作系统的发展历程"><a href="#二-操作系统的发展历程" class="headerlink" title="二.操作系统的发展历程"></a>二.操作系统的发展历程</h3><h4 id="1-批处理阶段（操作系统开始出现：多道）"><a href="#1-批处理阶段（操作系统开始出现：多道）" class="headerlink" title="1.批处理阶段（操作系统开始出现：多道）"></a>1.批处理阶段（操作系统开始出现：多道）</h4><ul><li>单道批处理系统<ul><li>每次主机内存中仅存放一道作业，每当它在运行期间发出输入/输出请求后<br>高速的CPU便处于等待低速的I/O完成的状态。</li><li>资源的利用率和系统的吞吐量太低</li></ul></li><li>多道批处理系统（具有并发和并行的特点）<ul><li>多道程序设计技术（多道，宏观上并行，微观上串行）<ul><li><strong>多道程序设计技术允许多个程序同时进入内存并允许它们在CPU中交替地运行（并发）</strong><br><strong>这些程序共享系统中的各种硬/软件资源，需要实现对共享资源的保护</strong></li><li>当一道程序因I/O请求而暂停运行时，CPU便立即转去运行另一道程序</li></ul></li><li><strong>中断技术使得I/O设备可以与CPU并行工作，I/O设备利用率高</strong></li><li>优点：资源利用率高，多道程序共享计算机资源，从而使各种资源得到充分利用<br>系统吞吐量大，CPU和其他资源保持“忙碌”状态</li><li>缺点：<strong>用户响应的时间较长，不提供人机交互能力</strong><br>用户既不能了解自己的程序的运行情况，又不能控制计算机。</li></ul></li></ul><h4 id="2-分时操作系统"><a href="#2-分时操作系统" class="headerlink" title="2.分时操作系统"></a>2.分时操作系统</h4><ul><li>分时操作系统的概述<ul><li>分时操作系统：计算机以时间片为单位轮流为各个用户/作业服务，各个用户可通过终端与计算机进行交互。</li><li>若某个作业在分配给它的时间片内不能完成其计算，则该作业暂时停止运行<br>把处理器让给其他作业使用，等待下一轮再继续运行。</li><li>由于计算机速度很快，作业运行轮转得也很快，因此给每个用户的感觉就像是自己独占一台计算机。<br>可以及时响应用户</li></ul></li><li>分时操作系统的特点<ul><li>分时系统也是支持多道程序设计的系统，用户请求可以被即时响应，<strong>解决了人机交互问题</strong>。</li><li>允许多个用户同时使用一台计算机，并且用户对计算机的操作相互独立，感受不到别人的存在。</li><li>缺点：<strong>不能优先处理一些紧急任务</strong>。操作系统对各个用户/作业都是完全公平的，<br>循环地为每个用户/作业服务一个时间片，不区分任务的紧急性。</li></ul></li></ul><h4 id="3-实时操作系统"><a href="#3-实时操作系统" class="headerlink" title="3.实时操作系统"></a>3.实时操作系统</h4><ul><li><p>实时操作系统的特点</p><ul><li><p>为了能在某个时间限制内完成某些紧急任务而不需要时间片排队，诞生了实时操作系统。</p></li><li><p>在实时操作系统的控制下，计算机系统接收到外部信号后及时进行处理，<br>并在严格的时限内处理完接收的事件</p></li><li><p><strong>实时操作系统的主要特点是及时性和可靠性</strong>。</p></li></ul></li><li><p>实时操作系统的分类</p><ul><li><p>硬实时系统</p><ul><li>某个动作必须绝对地在规定的时刻（或规定的时间范围）发生的系统<br>自动控制系统，导弹发射系统</li></ul></li><li><p>软实时系统</p><ul><li>若能够接受偶尔违反时间规定且不会引起任何永久性的损害<br>如飞机订票系统、银行管理系统。</li></ul></li></ul></li></ul><h3 id="三-操作系统运行环境（✪）"><a href="#三-操作系统运行环境（✪）" class="headerlink" title="三.操作系统运行环境（✪）"></a>三.操作系统运行环境（✪）</h3><h4 id="1-操作系统运行的机制（✠）"><a href="#1-操作系统运行的机制（✠）" class="headerlink" title="1.操作系统运行的机制（✠）"></a>1.操作系统运行的机制（✠）</h4><ul><li><p>内核程序与应用程序</p><ul><li>通常CPU执行两种不同性质的程序，一种是操作系统内核程序，另一种是用户自编程序(即应用程序)</li><li>对操作系统而言，这两种程序的作用不同，前者是后者的管理者，因此内核程序要执行一些特权指令<br>而应用程序出于安全考虑不能执行这些指令。</li><li>由很多内核程序组成操作系统内核（Kernel）</li></ul></li><li><p>特权指令与非特权指令</p><ul><li><p>指令是指CPU能识别、执行的最基本指令，指二进制机器指令</p><ul><li><img src="https://s1.ax1x.com/2023/08/18/pP39jFH.png" alt="pP39jFH.png"></li></ul></li><li><p>特权指令</p><ul><li>是指不允许用户直接使用的指令，只能由操作系统内核来使用</li><li>如I/O指令、置中断指令，内存清零等指令</li><li>核心态指令实际上包括系统调用类指令和一些针对时钟、中断和原语的操作指令</li></ul></li><li><p>非特权指令</p><ul><li>是指允许用户直接使用的指令，它不能直接访问系统中的软硬件资源，仅限于访问用户的地址空间，<br>这也是为了防止用户程序对系统造成破坏。如加减法指令</li></ul></li></ul></li><li><p>用户态与核心态</p><ul><li>用户态与内核态的概念<ul><li>将CPU的运行模式划分为用户态（目态）和核心态  (又称管态、内核态)</li><li>用户态上运行应用程序，只能执行非特权指令<br>操作系统内核程序运行在核心态，可以执行特权指令，也可以执行非特权指令</li><li>CPU中有一个寄存器叫程序状态字寄存器(PSW)，其中有个二进制位，1表示“内核态”，0表示“用户态”</li></ul></li><li>用户态与核心态之间的转换<ul><li>用户态→核心态：<strong>由“中断”引发，硬件自动完成变态过程，触发中断信号意味着操作系统将强行夺回CPU的使用权</strong><br><strong>但凡需要操作系统介入的地方，都会引发中断</strong>（此前在用户态中可能发生了系统调用，使用了访管指令）</li><li>核心态→用户态：<strong>执行一条特权指令（一般是中断返回指令），修改PSW的标志位为“用户态”，</strong><br><strong>这个动作意味着操作系统将主动让出CPU使用权</strong></li><li>操作系统会对引发中断的事件进行处理，处理完了再把CPU使用权交给别的应用程序</li></ul></li><li>在操作系统中由用户态转变为核心态的例子<ul><li>用户程序要求操作系统的服务，即系统调用。</li><li>发生$1$次中断。</li><li>用户程序中产生了一个错误状态。</li><li>用户程序中企图执行一条特权指令。</li></ul></li><li><strong>只能在核心态下运行的指令是：置时钟指令、广义指令（系统调用）、输入输出、中断指令</strong></li><li><strong>可以在用户态下运行的指令是：读时钟、取数、寄存器清零</strong></li></ul></li><li><p>操作系统的内核（✪）</p><ul><li><p>内核的定义</p><ul><li>内核是计算机上配置的底层<strong>软件</strong>，它管理着系统的各种资源，可以看作是连接应用程序和硬件的一座桥梁<br>是操作系统最基本最核心的部分</li></ul></li><li><p>内核的构成（内核中的指令操作工作在核心态）</p><ul><li><p>与硬件关系紧密的模块</p><ul><li><p>时钟管理：实现计时功能</p></li><li><p>中断机制：通过时钟与中断的管理，可以实现进程的切换<br><strong>现代操作系统是靠中断驱动的软件</strong></p></li><li><p>原语（一种特殊的程序）</p><ul><li><p>原语的概述</p><ul><li>处于操作系统的最底层，是最接近硬件的部分。</li><li><strong>这些程序的运行具有原子性，其操作只能一气呵成（出于系统安全性和便于管理考虑）</strong></li><li>这些程序的运行时间都较短，而且调用频繁。</li></ul></li><li><p>原语的定义</p><ul><li>定义原语的直接方法是关闭中断，让其所有动作不可分割地完成后再打开中断。</li><li>系统中的设备驱动、CPU切换、进程通信等功能中的部分操作都可定义为原语，使它们成为内核的组成部分</li></ul></li></ul></li></ul></li><li><p>对系统资源进行管理的功能：<br>这些管理工作更多的是对数据结构的操作，不会直接涉及硬件</p><ul><li>进程管理：进程状态管理、进程调度和分派、创建与撒销进程控制块等。</li><li>存储器管理：存储器的空间分配和回收、内存信息保护程序、代码对换程序等。</li><li>设备管理：缓冲区管理、设备分配和回收等。</li></ul></li></ul></li><li><p>操作系统的体系结构</p><ul><li>大(宏、单)内核<ul><li>将操作系统的主要功能模块都作为系统内核，运行在核心态</li><li>优点：高性能</li><li>缺点：内核代码庞大，结构混乱，难以维护</li></ul></li><li>微内核<ul><li>只把最基本的功能保留在内核</li><li>优点：内核功能少，结构清晰，方便维护</li><li>缺点：需要频繁地在核心态和用户态之间切换，性能低</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/18/pP3AawV.png" alt="pP3AawV.png"></li></ul></li><li>注：Ubuntu、CentOS的开发团队，其主要工作是实现非内核功能，而内核都是用了Linux内核</li></ul></li></ul></li></ul><h4 id="2-中断和异常的概念（✪）"><a href="#2-中断和异常的概念（✪）" class="headerlink" title="2.中断和异常的概念（✪）"></a>2.中断和异常的概念（✪）</h4><ul><li><p>中断的作用</p><ul><li>实现核心态与用户态的切换<ul><li>发生中断或异常时，运行用户态的CPU会立即进入核心态，这是通过硬件自动实现的</li><li>用一个特殊寄存器的一位来表示CPU所处的工作状态，0表示核心态，1表示用户态<br>若要进入核心态，则只需将该位置0即可</li></ul></li><li>释放程序对资源的占有（实现并发）</li></ul></li><li><p>中断在广义上的分类</p><ul><li><p>内中断（异常）</p><ul><li><p>内中断也称为异常，<strong>与当前执行的指令有关，中断信号来源于CPU内部</strong><br>异常不能被屏蔽，一旦出现，就应立即处理。</p></li><li><p>异常的分类</p><ul><li><p>软件中断</p><ul><li><p>故障(Fault)：由错误条件引起的，可能被内核程序修复。内核程序修复故障后会把CPU使用权还给应用程序，<br>让它继续执行下去。如：缺页故障</p></li><li><p><strong>陷入(Trap)</strong>：由<strong>陷入指令</strong>引发，是应用程序故意引发的，用于在<u>用户态</u>下调用操作系统内核程序，<br><strong>注：陷入指令（访管指令）在用户态中使用，属于非特权指令</strong></p></li></ul></li><li><p>硬件中断</p><ul><li>终止(Abort)：由致命错误引起，内核程序无法修复该错误，因此一般不再将CPU使用权还给引发终止的应用程序，<br>而是直接终止该应用程序。如：整数除0、非法使用特权指令</li></ul></li></ul></li></ul></li><li><p>外中断（狭义上的中断，属于硬件中断）</p><ul><li><strong>与当前执行的命令无关，中断信号来自CPU外部</strong></li><li>I/O中断：通常用于信息输入/输出<br>如设备发出的I/O结束中断，表示设备输入/输出处理已经完成。</li><li>时钟中断：表示一个固定的时间片已到，让处理机处理计时、启动定时运行的任务等。</li></ul></li></ul></li><li><p>中断机制的基本实现原理</p><ul><li><p>步骤</p><ul><li>检查中断信号<ul><li>内中断：CPU在执行指令时会检查是否有异常发生</li><li>外中断：每个指令周期末尾CPU都会检查是否有外中断信号需要处理</li></ul></li><li>找到相应的中断处理程序：根据中断信号的类型去查询“中断向量表”，以此来找到相应的中断处理程序在内存中的存放位置。</li></ul></li><li>在中断机制中CPU和操作系统的作用<ul><li><strong>进入中断的程序属于操作系统程序</strong></li><li><strong>中断操作中由操作系统完成的是：提供中断服务、初始化中断向量表、保存中断屏蔽字、保存通用寄存器的内容、执行系统调用服务例程</strong></li><li><strong>中断操作中由CPU完成的是：保存被中断程序的中断点和程序状态字，将CPU模式改为内核态</strong></li></ul></li><li><strong>中断处理需要保存程序断点/程序计数器（PC）、程序状态字寄存器（PSW），而子程序只需要保存断点/程序计数器（PC）</strong></li><li>定时器产生时钟中断后，由时钟中断服务程序更新的内容是<ul><li>内核中时钟变量的值</li><li>当前进程占用CPU的时间</li><li>当前进程在时间片内的剩余执行时间</li></ul></li></ul></li></ul><h4 id="3-系统调用（✪）"><a href="#3-系统调用（✪）" class="headerlink" title="3.系统调用（✪）"></a>3.系统调用（✪）</h4><ul><li>系统调用的定义<ul><li>操作系统提供给应用程序(程序员/编程人员)使用的接口，可以理解为一种可供应用程序调用的特殊函数，<br>由用户进程发起，应用程序可以通过系统调用来请求获得操作系统内核的服务</li><li>应用程序通过系统调用请求操作系统的服务。而系统中的各种共享资源都由操作系统内核统一掌管，<br>因此凡是与共享资源有关的操作(如存储分配、I/O操作、文件管理等)，都必须通过系统调用的方式向操作系统内核提出服务请求</li><li>系统调用由操作系统内核代为完成。这样可以保证系统的稳定性和安全性，防止用户进行非法操作。</li><li><strong>完全由操作系统自动完成的和不涉及请求操作系统服务的不属于系统调用</strong><ul><li>如进程调度，页置换由操作系统自动完成</li><li>生成随机整数，只需要普通的函数调用，不涉及请求操作系统的服务</li></ul></li></ul></li><li>系统调用与库函数的区别<ul><li><strong>普通应用程序可直接进行系统调用，也可使用库函数用程序，有的库函数涉及系统调用，有的不涉及</strong></li><li>编程语言向上提供库函数。有时会将系统调用封装成库函数，以隐藏系统调用的一些细节，使程序员编程更加方便。</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/18/pP3FhB6.png" alt="pP3FhB6.png"></li></ul></li></ul></li><li>系统调用的功能<ul><li><strong>设备管理：完成设备的请求/释放/启动等功能</strong></li><li><strong>文件管理：完成文件的读/写/创建/删除等功能</strong></li><li><strong>进程控制：完成进程的创建/撤销/阻塞/唤醒等功能</strong></li><li><strong>进程通信：完成进程之间的消息传递/信号传递等功能</strong></li><li><strong>内存管理：完成内存的分配/回收等功能</strong></li></ul></li><li>系统调用的过程（<strong>在用户态发生，在内核态执行</strong>）<ul><li>在用户态中使用传参指令：将系统调用需要的参数放到某些通用寄存器中</li><li>用户程序可以执行<strong>陷入指令(又称访管指令或trap指令)</strong>来发起系统调用，请求操作系统提供服务。</li><li>当需要管理程序服务时，系统则通过<strong>硬件中断</strong>机制进入核心态，运行管理程序<br>此时相当于应用程序把CPU的使用权主动交给操作系统内核程序(CPU状态会从用户态进入核心态)</li><li>在核心态中，操作系统内核程序再对系统调用请求做出相应处理。</li><li>处理完成后，操作系统内核程序又会把CPU的使用权还给用户程序(即CPU状态会从核心态回到用户态)</li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/04/pPklj2V.png" alt="pPklj2V.png"></li></ul></li></ul></li><li><p>系统调用的注意事项</p><ul><li>由用户态进入核心态，不仅状态需要切换，而且所用的堆栈也可能需要由用户堆栈切换为系统堆栈，<br>但这个系统堆栈也是属于该进程的。</li><li><strong>若程序的运行由用户态转到核心态，则会用到访管指令，访管指令是在用户态使用的，所以它不可能是特权指令。</strong></li><li><strong>系统调用中由操作系统完成的是：保存通用寄存器的内容、执行系统调用服务例程</strong></li><li><strong>系统调用中由CPU完成的是：保存断点和程序状态字、将CPU模式改为内核态</strong></li></ul></li></ul><h3 id="四-操作系统结构（✠）"><a href="#四-操作系统结构（✠）" class="headerlink" title="四.操作系统结构（✠）"></a>四.操作系统结构（✠）</h3><h4 id="1-分层结构"><a href="#1-分层结构" class="headerlink" title="1.分层结构"></a>1.分层结构</h4><ul><li>分层法的概述<ul><li>将操作系统分为若干层，最底层(层0)为硬件，最高层(层N)为用户接口，每层只能调用紧邻它的低层的功能和服务（单向依赖）</li></ul></li><li>分层法的优点<ul><li>便于系统的调试和验证，简化了系统的设计和实现</li><li>易扩充和易维护：各层之间调用接口清晰固定</li></ul></li><li>分层法的缺点<ul><li>仅可调用相邻低层，难以合理定义各层的边界</li><li>效率低，不可跨层调用，系统调用执行时间长<ul><li>操作系统每执行一个功能，通常要自上而下地穿越多层，<br>各层之间都有相应的层间通信机制，这无疑增加了额外的开销，导致系统效率降低。</li></ul></li></ul></li></ul><h4 id="2-模块化"><a href="#2-模块化" class="headerlink" title="2.模块化"></a>2.模块化</h4><ul><li>模块化的概述<ul><li>模块-接口法：将操作系统按功能划分为若干具有一定独立性的模块。</li><li>每个模块具有某方面的管理功能，并规定好各模块间的接口，使各模块之间能够通过接口进行通信</li></ul></li><li>模块化的优点<ul><li>模块间逻辑清晰易于维护，确定模块间接口后，即可多模块同时开发</li><li>支持动态加载新的内核模块（如：安装设备驱动程序、安装新的文件系统模块到内核），增强OS适应性</li><li>任何模块都可以直接调用其他模块，无需采用消息传递进行通信，效率高</li></ul></li><li>模块化的缺点<ul><li>模块间的接口定义未必合理、实用</li><li>模块间相互依赖，更难调试和验证</li></ul></li></ul><h4 id="3-宏内核与微内核"><a href="#3-宏内核与微内核" class="headerlink" title="3.宏内核与微内核"></a>3.宏内核与微内核</h4><ul><li><p>宏内核</p><ul><li>宏内核也称单内核或大内核，将所有的系统功能都放在内核里，通常也采用了“模块化“的设计思想</li><li>优点：<strong>性能高</strong>，内核内部各种功能都可以直接相互调用</li><li><p>缺点：内核庞大功能复杂，难以维护；大内核中某个功能模块出错，就可能导致整个系统崩溃</p></li><li><p>目前的主流操作系统均使用的宏内核的架构：Linux、UNIX</p></li></ul></li><li><p>微内核（机制与策略分离）</p><ul><li>只把中断、原语、进程通信等最核心的功能放入内核。进程管理、文件管理、设备管理等功能以用户进程的形式运行在用户态</li><li><p>优点</p><ul><li>内核小功能少、易于维护，内核可靠性高</li><li>内核外的某个功能模块出错不会导致整个系统崩溃（<strong>可靠性与安全性</strong>）</li></ul></li><li><p>缺点</p><ul><li><strong>性能低，需要频繁的切换用户态/核心态</strong>。</li><li>用户态下的各功能模块不可以直接相互调用，只能通过内核的”消息传递”来间接通信</li></ul></li><li><p>微内核的特点</p><ul><li><p>扩展性和灵活性</p><ul><li>许多功能从内核中分离出来，当要修改某些功能或增加新功能时，<br>只需在相应的服务器中修改或新增功能，或再增加一个专用的服务器，而无须改动内核代码。</li></ul></li><li><p>可靠性和安全性</p><ul><li>内核外的某个功能模块出错不会导致整个系统崩溃</li></ul></li><li><p>可移植性</p><ul><li>与CPU和I/O硬件有关的代码均放在内核中，而其他各种服务器均与硬件平台无关，<br>因而将操作系统移植到另一个平台上所需做的修改是比较小的。</li></ul></li><li><p>分布式计算</p><ul><li>客户和服务器之间、服务器和服务器之间的通信采用消息传递机制，<br>这就使得微内核系统能很好地支持分布式系统和网络系统。</li></ul></li></ul></li><li><p>虽然宏内核在桌面操作系统中取得了绝对的胜利，但是微内核在实时、工业、航空及军事应用中特别流行，<br>这些领域都是关键任务，需要有高度的可靠性。</p></li></ul></li><li><p>图片</p><ul><li><img src="https://s1.ax1x.com/2023/08/18/pP3EqgJ.png" alt="pP3EqgJ.png"></li></ul></li></ul><h3 id="五-操作系统引导（boot-✠）"><a href="#五-操作系统引导（boot-✠）" class="headerlink" title="五.操作系统引导（boot ✠）"></a>五.操作系统引导（boot ✠）</h3><ul><li><strong>操作系统引导程序(boot)位于磁盘，操作系统在初始化过程中需要创建中断向量表</strong></li><li><strong>活动分区又称主分区，即安装了操作系统的分区，通常在C盘，MBR(主引导记录可以确定活动分区)</strong></li><li><p>操作系统引导过程</p><ul><li>①<strong>激活CPU</strong>。激活的CPU读取ROM中的boot程序，开始执行BIOS的指令（自举程序）</li><li>②<strong>硬件自检</strong>。启动BIOS程序后，先进行硬件自检，检查硬件是否出现故障。如有故障，主板会发出不同含义的蜂鸣，启动中止;如无故障，屏幕会显示CPU、内存、硬盘等信息。</li><li>③<strong>加载带有操作系统的硬盘。将磁盘读入内存</strong></li><li>④<strong>加载主引导记录MBR</strong>。<strong>主引导记录MBR告诉CPU去硬盘的哪个主分区去找操作系统</strong>。</li><li>⑤<strong>扫描硬盘分区表，并加载硬盘活动分区</strong>。</li><li><strong>⑥加载分区引导记录PBR。其作用是寻找并激活分区根目录下用于引导操作系统的程序（启动管理器）。</strong></li><li>⑦加载启动管理器</li><li>⑧加载操作系统</li></ul></li><li><p>图片</p><ul><li><img src="https://s1.ax1x.com/2023/08/18/pP3eSpD.png" alt="pP3eSpD.png"></li></ul></li></ul><h3 id="六-虚拟机"><a href="#六-虚拟机" class="headerlink" title="六.虚拟机"></a>六.虚拟机</h3><ul><li>虚拟机：使用虚拟化技术，将一台物理机器虚拟化为多台虚拟机器(Virtual Machine,VM)<br>每个虚拟机器都可以独立运行一个操作系统</li><li>也可称为虚拟机管理程序/虚拟机监控程序Virtual Machine Monitor/Hypervisor</li><li><p>运行在两类虚拟机管理程序上的操作系统都称为客户操作系统，Guest OS<br>对于第二类虚拟机管理程序，运行在底层硬件上的操作系统称为宿主操作系统，Host OS</p></li><li><p>1.第一类VMM（裸金属架构）</p><ul><li>直接运行在硬件之上，能直接控制和分配物理资源</li><li>运行在最高特权级(Ring 0)，可以执行最高特权的指令是唯一一个运行在最高特权级的程序</li><li>在安装Guest OS时，VMM要在原本的硬盘上自行分配存储空间，类似于“外核”的分配方式，分配未经抽象的物理硬件</li><li>性能更好；虚拟机的可迁移性更差</li><li>可支持的虚拟机数量更多，不需要和Host OS竞争资源，相同的硬件资源可以支持更多的虚拟机</li></ul></li><li><p>2.第二类VMM（寄宿架构）</p><ul><li>它是一个依赖于Windows、Linux等操作系统分配和调度资源的程序，很像一个普通的进程<br>运行在Host OS之上，依赖于Host OS为其分配物理资源</li><li>第二类VMM分运行在用户态、部分运行在内核态。<br>GuestOS发出的系统调用会被VMM截获，并转化为VMM对HostOS的系统调用</li><li>GuestOS拥有自己的虚拟磁盘，该盘实际上是Host OS文件系统中的一个大文件。GuestOS分配到的内存是虚拟内存。<ul><li>首次启动时，第二类虚拟机管理程序像一台刚启动的计算机那样运转，期望找到的驱动器可以是虚拟设备。<br>然后将操作系统安装到虚拟磁盘上（其实只是宿主操作系统中的一个文件）</li></ul></li><li>性能更差，需要HostOS作为”中介”；可迁移性更好，只需导出虚拟机镜像文件即可迁移到另一台HostOS上，商业化应用更广泛</li><li>可支持的虚拟机数量更少，Host OS本身需要使用物理资源，HostOS上运行的其他进程也需要物理资源</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/18/pP3mrrj.png" alt="pP3mrrj.png"></li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;操作系统第一章-计算机系统概述&quot;&gt;&lt;a href=&quot;#操作系统第一章-计算机系统概述&quot; class=&quot;headerlink&quot; title=&quot;操作系统第一章 计算机系统概述&quot;&gt;&lt;/a&gt;操作系统第一章 计算机系统概述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;计算机学科基础：操作系统第一章计算机系统概述的学习笔记&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="计算机基础学习笔记" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="操作系统" scheme="http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>计算机网络第六章-应用层</title>
    <link href="http://example.com/2024/08/11/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AC%AC%E5%85%AD%E7%AB%A0-%E5%BA%94%E7%94%A8%E5%B1%82/"/>
    <id>http://example.com/2024/08/11/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AC%AC%E5%85%AD%E7%AB%A0-%E5%BA%94%E7%94%A8%E5%B1%82/</id>
    <published>2024-08-11T15:44:14.000Z</published>
    <updated>2024-08-11T16:58:51.187Z</updated>
    
    <content type="html"><![CDATA[<h2 id="计算机网络第六章-应用层"><a href="#计算机网络第六章-应用层" class="headerlink" title="计算机网络第六章 应用层"></a>计算机网络第六章 应用层</h2><blockquote><p>计算机学科基础：计算机网络第六章应用层的学习笔记</p></blockquote><span id="more"></span><h3 id="一-网络应用模型"><a href="#一-网络应用模型" class="headerlink" title="一.网络应用模型"></a>一.网络应用模型</h3><h4 id="1-客户-服务器模型（C-S）"><a href="#1-客户-服务器模型（C-S）" class="headerlink" title="1.客户/服务器模型（C/S）"></a>1.客户/服务器模型（C/S）</h4><ul><li>在客户/服务器(Client/Server,C/S)模型中，有一个总是打开的主机称为服务器，它服务于许多来自其他称为客户机的主机请求<ul><li>服务器的工作流程<ul><li>服务器处于接收请求的状态</li><li>客户机发出服务请求，并等待接收结果</li><li>服务器收到请求后，分析请求，进行必要的处理，得到结果并发送给客户机</li></ul></li></ul></li><li>客户程序必须知道服务器程序的地址，客户机上一般不需要特殊的硬件和复杂的操作系统。</li><li>而服务器上运行的软件则是专门用来提供某种服务的程序，可同时处理多个远程或本地客户的要求。<br>系统启动后即自动调用并一直不断地运行着，被动地等待并接收来自各地客户的请求。因此，服务器程序不需要知道客户程序的地址</li><li>客户/服务器模型的主要的特征<ul><li>客户是服务请求方，服务器是服务提供方<br>常见的使用客户/服务器模型的应用包括Web、文件传输协议(FTP)、远程登录和电子邮件等</li><li>网络中各计算机的地位不平等，服务器可以通过对用户权限的限制来达到管理客户机的目的，使它们不能随意存储/删除数据，<br>或进行其他受限的网络活动。整个网络的管理工作由少数服务器担当，因此网络的管理非常集中和方便</li><li>客户机相互之间不直接通信。例如，在Web应用中两个浏览器并不直接通信</li><li>可扩展性不佳。受服务器硬件和网络带宽的限制，服务器支持的客户机数有限</li></ul></li></ul><h4 id="2-P2P模型"><a href="#2-P2P模型" class="headerlink" title="2.P2P模型"></a>2.P2P模型</h4><ul><li><p>P2P模型的思想是整个网络中的传输内容不再被保存在中心服务器上，每个结点都同时具有下载、上传的功能，其权利和义务都是大体对等的。</p></li><li><p>在P2P模型中，各计算机没有固定的客户和服务器划分。相反，任意一对计算机一一称为对等方(Per)，直接相互通信<br>P2P模型从本质上来看仍然使用客户/服务器模式，每个结点既作为客户访问其他结点的资源，也作为服务器提供资源给其他结点访问。</p></li><li>P2P模型的优点<ul><li>减轻了服务器的计算压力，消除了对某个服务器的完全依赖，可以将任务分配到各个结点上，因此大大提高了系统效率和资源利用率<br>例如，播放流媒体时对服务器的压力过大，而通过P2P模型，可以利用大量的客户机来提供服务</li><li>多个客户机之间可以直接共享文档</li><li>可扩展性好，传统服务器有响应和带宽的限制，因此只能接受一定数量的请求</li><li>网络健壮性强，单个结点的失效不会影响其他部分的结点。</li></ul></li><li>P2P模型的缺点<ul><li>在获取服务的同时，还要给其他结点提供服务，因此会占用较多的内存，影响整机速度。<br>例如，经常进行P2P下载还会对硬盘造成较大的损伤。</li><li>据某互联网调研机构统计，当前P2P程序已占互联网50%~90%的流量，使网络变得非常拥塞，<br>因此各大ISP(互联网服务提供商，如电信、网通等）通常都对P2P应用持反对态度。</li></ul></li></ul><h4 id="3-常见应用层协议端口号"><a href="#3-常见应用层协议端口号" class="headerlink" title="3.常见应用层协议端口号"></a>3.常见应用层协议端口号</h4><ul><li><img src="https://s1.ax1x.com/2023/08/12/pPuSIX9.png" alt="pPuSIX9.png"></li></ul><h3 id="二-域名系统（DNS✪）"><a href="#二-域名系统（DNS✪）" class="headerlink" title="二.域名系统（DNS✪）"></a>二.域名系统（DNS✪）</h3><h4 id="1-DNS的概述"><a href="#1-DNS的概述" class="headerlink" title="1.DNS的概述"></a>1.DNS的概述</h4><ul><li>域名系统(DNS)是因特网使用的命名系统，用来把便于人们记忆的具有特定含义的主机名转换为便于机器处理的IP地址。<br>互联网上提供的主机一定要有IP地址，不一定要有域名</li><li>DNS系统采用客户/服务器模型，<strong>其协议运行在UDP之上，使用53号端口</strong></li><li>从概念上可将DNS分为3部分：层次域名空间、域名服务器和解析器。</li><li>一个域名可以对应多个主机或IP地址，一个主机或IP地址也可以对应多个域名</li><li>主机需要知道自己的本地域名服务器的IP地址，本地域名服务器需要知道根服务器的IP地址</li></ul><h4 id="2-层次域名空间"><a href="#2-层次域名空间" class="headerlink" title="2.层次域名空间"></a>2.层次域名空间</h4><ul><li>因特网采用层次树状结构的命名方法。采用这种命名方法，任何一个连接到因特网的主机或路由器，都有一个唯一的层次结构名称，即域名(Domain Name)。</li><li>域(Domain)是名字空间中一个可被管理的划分。域还可以划分为子域，而子域还可以继续划分为子域的子域，<br>这样就形成了顶级域、二级域、三级域等。每个域名都由标号序列组成，而各标号之间用点(“.”)隔开<img src="https://s1.ax1x.com/2023/08/11/pPn9vu9.png" alt="pPn9vu9.png"></li><li>关于域名中的标号有以下几点需要注意<ul><li>标号中的英文不区分大小写</li><li>标号中除连字符(-)外不能使用其他的标点符号</li><li>每个标号不超过63个字符，多标号组成的完整域名最长不超过255个字符</li><li>级别最低的域名写在最左边，级别最高的顶级域名写在最右边</li></ul></li><li>项级域名的分类(TLD)<ul><li>国家（地区）顶级域名：国家和某些地区的域名，如“.cn”表示中国，“.us”表示美国，“.uk”表示英国</li><li>通用顶级域名：常见的有“.com”（公司）、“.net”（网络服务机构）、“.org”（非营利性组织）和“.gov”（国家或政府部门）等</li><li>基础结构域名。这种顶级域名只有一个，即arpa，用于反向域名解析，因此又称反向域名</li></ul></li><li>在域名系统中，每个域分别由不同的组织进行管理。每个组织都可以将它的域再分成一定数目的子域，并将这些子域委托给其他组织去管理</li></ul><h4 id="3-域名服务器"><a href="#3-域名服务器" class="headerlink" title="3.域名服务器"></a>3.域名服务器</h4><ul><li><p>域名服务器的概述</p><ul><li>因特网的域名系统被设计成一个联机分布式的数据库系统，并采用客户/服务器模型。</li><li>域名到IP地址的解析是由运行在域名服务器上的程序完成的，一个服务器所负责管辖的范围称为区(不以“域”为单位)<br>各单位根据具体情况来划分自己管辖范围的区，但在一个区中的所有结点必须是能够连通的，<br>每个区设置相应的权限域名服务器，用来保存该区中的所有主机的域名到IP地址的映射。</li><li>每个域名服务器不但能够进行一些域名到IP地址的解析，而且还必须具有连向其他域名服务器的信息<br>当自己不能进行域名到IP地址的转换时，能够知道到什么地方去找其他域名服务器</li><li>DNS使用了大量的域名服务器，它们以层次方式组织，采用分布式设计的DNS<br>没有一台域名服务器具有因特网上所有主机的映射，相反，该映射分布在所有的DNS上。</li></ul></li><li><p>域名服务器的分类</p><ul><li>根域名服务器<ul><li>根域名服务器是最高层次的域名服务器，所有的根域名服务器都知道所有的顶级域名服务器的IP地址</li><li><strong>根域名服务器也是最重要的域名服务器，不管是哪个本地域名服务器，若要对因特网上任何一个域名进行解析，只要自己无法解析，就<u>首先</u>要求助于根域名服务器</strong></li><li>因特网上有13个根域名服务器，尽管我们将这13个根域名服务器中的每个都视为单个服务器，但每个“服务器”实际上是冗余服务器的集群，以提供安全性和可靠性</li><li>根域名服务器用来管辖顶级域(如.com)，通常它并不直接把待查询的域名直接转换成IP地址，<br>而是告诉本地域名服务器下一步应当找哪个顶级域名服务器进行查询</li></ul></li><li>顶级域名服务器<ul><li>这些域名服务器负责管理在该顶级域名服务器注册的所有二级域名。</li><li>收到DNS查询请求时，就给出相应的回答(可能是最后的结果，也可能是下一步应当查找的域名服务器的IP地址)</li></ul></li><li>授权域名服务器（权限域名服务器）<ul><li>每台主机都必须在授权域名服务器处登记。为了更可靠工作，一台主机最好至少有两个授权域名服务器。</li><li>许多域名服务器都同时充当本地域名服务器和授权域名服务器。<strong>授权域名服务器总能将其管辖的主机名转换为该主机的IP地址</strong></li></ul></li><li>本地域名服务器<ul><li>当一台主机发出DNS查询请求时，这个查询请求报文就发送给该主机的本地域名服务器</li><li>在Windows系统中配置“本地连接”时，就需要填写DNS地址，这个地址就是本地DNS（域名服务器）的地址</li></ul></li><li>层次图片<ul><li><img src="https://s1.ax1x.com/2023/08/11/pPnPJzD.png" alt="pPnPJzD.png"></li></ul></li></ul></li></ul><h4 id="4-域名解析过程-♚"><a href="#4-域名解析过程-♚" class="headerlink" title="4.域名解析过程(♚)"></a>4.域名解析过程(♚)</h4><ul><li><p>两种查询方式</p><ul><li>递归查询（主机向本地域名服务器的查询）<ul><li>如果本地主机所询问的本地域名服务器不知道被查询域名的IP地址，<br>那么本地域名服务器就以DNS客户的身份，向根域名服务器继续发出查询请求报文</li></ul></li><li>迭代查询（本地域名服务器向根域名服务器的查询）<ul><li>当根域名服务器收到本地域名服务器发出的迭代查询请求报文时，要么给出所要查询的IP地址，<br>要么告诉本地域名服务器：“你下一步应当向哪个顶级域名服务器进行查询”，然后让本地域名服务器向这个顶级域名服务器进行后续的查询</li><li>顶级域名服务器收到查询报文后，要么给出所要查询的IP地址，要么告诉本地域名服务器下一步应向哪个权限域名服务器查询。最后，知道所要解析的域名的IP地址后，把这个结果返回给发起查询的主机。</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/11/pPnF08f.png" alt="pPnF08f.png"></li></ul></li></ul></li><li><p>高速缓存</p><ul><li>为了提高DNS的查询效率，并减少因特网上的DNS查询报文数量，在域名服务器中广泛地使用了高速缓存。</li><li>当一个DNS服务器接收到DNS查询结果时，它能将该DNS信息缓存在高速缓存中。</li><li>当另一个相同的域名查询到达该DNS服务器时，该服务器就能够直接提供所要求的IP地址，而不需要再去向其他DNS服务器询问。</li><li>因为主机名和IP地址之间的映射不是永久的，所以DNS服务器将在一段时间后丢弃高速缓存中的信息。</li></ul></li><li>例题<ul><li>例1：最少情况：当本地域名服务器中有该域名的DNS信息时，不需要查询任何其他域名服务器，最少发出0次DNS查询<br>最多情况：因为均采用迭代查询方式，在最坏情况下，本地域名服务器需要依次迭代地向根域名服务器、顶级域名服务器(.com)、权限域名服务器(xyz.com)、权限域名服务器(abc.xyz.com)发出DNS查询请求，因此最多发出4次DNS查询。<ul><li><img src="https://s1.ax1x.com/2023/08/11/pPnAmfx.png" alt="pPnAmfx.png"></li></ul></li><li>例2：最短本地域名服务器已有缓存，只需访问其即可（10），此时再进行浏览（10）总共20；最长需要本地服务器依次访问根服务器、顶级域名服务器、域名服务器（30），加上之前访问本地服务器的时间以及主机向网站进行浏览的时间，共50<ul><li><img src="https://s1.ax1x.com/2023/08/11/pPnAK1K.png" alt="pPnAK1K.png"></li></ul></li></ul></li></ul><h3 id="三-文件传输系统（FTP）"><a href="#三-文件传输系统（FTP）" class="headerlink" title="三.文件传输系统（FTP）"></a>三.文件传输系统（FTP）</h3><h4 id="1-FTP的工作原理"><a href="#1-FTP的工作原理" class="headerlink" title="1.FTP的工作原理"></a>1.FTP的工作原理</h4><ul><li>FTP提供交互式的访问，<strong>允许客户指明文件的类型与格式，并允许文件具有存取权限</strong>。<br>它屏蔽了各计算机系统的细节，因而适合于在异构网络中的任意计算机之间传送文件</li><li>FTP采用客户/服务器的工作方式，它使用TCP可靠的传输服务。一个FTP服务器进程可同时为多个客户进程提供服务。</li><li>FTP的服务器进程由两大部分组成：一个主进程，负责接收新的请求，另外有若干从属进程，负责处理单个请求。</li><li>FTP的功能<ul><li>提供不同种类主机系统(硬、软件体系等都可以不同)之间的文件传输能力</li><li>以用户权限管理的方式提供用户对远程FTP服务器上的文件管理能力</li><li>以匿名FTP的方式提供公用文件共享的能力</li></ul></li><li>FTP服务器的工作步骤<ul><li>打开熟知端口21（控制端口），使客户进程能够连接上</li><li>等待客户进程发连接请求</li><li>启动从属进程来处理客户进程发来的请求。主进程与从属进程并发执行，从属进程对客户进程的请求处理完毕后即终止</li><li>回到等待状态，继续接收其他客户进程的请求。</li><li>FTP服务器必须在整个会话期间保留用户的状态信息。特别是服务器必须把指定的用户账户与控制连接联系起来，<br>服务器必须追踪用户在远程目录树上的当前位置</li></ul></li></ul><h4 id="2-控制连接与数据连接"><a href="#2-控制连接与数据连接" class="headerlink" title="2.控制连接与数据连接"></a>2.控制连接与数据连接</h4><ul><li><p>FTP在工作时使用两个并行的TCP连接：一个是控制连接(服务器端口号21)，<br>一个是数据连接(服务器端口号20)。使用两个不同的端口号可以使协议更容易实现。</p></li><li><p>控制连接（端口号21）</p><ul><li>服务器监听21号端口，等待客户连接，建立在这个端口上的连接称为控制连接</li><li>控制连接用来传输控制信息(如连接请求、传送请求等)，并且控制信息都以7位ASCII格式传送，<br>FTP客户发出的传送请求，通过控制连接发送给服务器端的控制进程</li><li>控制连接并不用来传送文件，但在传输文件时也可以使用控制连接（如客户在传输中途发一个中止传输的命令），<br><strong>因此控制连接在整个会话期间一直保持打开状态</strong>。</li></ul></li><li>数据连接（端口号20）<ul><li>服务器端的控制进程在接收到FTP客户发来的文件传输请求后，就创建“数据传送进程”和“数据连接”。</li><li>数据连接用来连接客户端和服务器端的数据传送进程，数据传送进程实际完成文件的传送，<br><strong>在传送完毕后关闭“数据传送连接”并结束运行</strong>。</li><li>数据连接的两种传输模式<ul><li>主动模式PORT<ul><li>客户端连接到服务器的21端口，登录成功后要读取数据时，客户端随机开放一个端口，并发送命令告知服务器</li><li>服务器收到PORT命令和端口号后，通过20端口和客户端开放的端口连接，发送数据。</li></ul></li><li>被动模式PASV模式<ul><li>客户端要读取数据时，发送PASV命令到服务器，服务器在本地随机开放一个端口，并告知客户端，<br>客户端再连接到服务器开放的端口进行数据传输。</li></ul></li></ul></li></ul></li><li><strong>无论是用PORT模式还是PASV模式，选择权在客户端</strong></li><li><strong>主动模式传送数据是“服务器”连接到“客户端”的端口，被动模式传送数据是“客户端”连接到“服务器”的端口。</strong></li><li><strong>TCP的20和21端口均为<u>服务器</u>的端口，客户进程的端口是客户进程自己提供的</strong></li><li>带外传送与NFS<ul><li>因为FTP使用了一个分离的控制连接，所以也称FTP的控制信息是带外(Out-of-band)传送的。</li><li>使用FTP时，若要修改服务器上的文件，则需要先将此文件传送到本地主机，然后再将修改后的文件副本传送到原服务器，来回传送耗费很多时间。</li><li>网络文件系统(NFS)采用另一种思路，它允许进程打开一个远程文件，并能在该文件的某个特定位置开始读写数据。<br>此时NFS可使用户复制一个大文件中的一个很小的片段，而不需要复制整个大文件</li></ul></li></ul><h3 id="四-电子邮件"><a href="#四-电子邮件" class="headerlink" title="四.电子邮件"></a>四.电子邮件</h3><h4 id="1-电子邮件系统的组成结构"><a href="#1-电子邮件系统的组成结构" class="headerlink" title="1.电子邮件系统的组成结构"></a>1.电子邮件系统的组成结构</h4><ul><li>电子邮件是一种异步通信方式，通信时不需要双方同时在场。电子邮件把邮件发送到收件人使用的邮件服务器，<br>并放在其中的收件人邮箱中，收件人可以随时上网到自己使用的邮件服务器进行读取。</li><li>电子邮件系统的主要的组成构件<ul><li>用户代理(User Agent)<ul><li>用户与电子邮件系统的接口。用户代理向用户提供一个很友好的接口来发送和接收邮件，<br>用户代理至少应当具有撰写、显示和邮件处理的功能。</li><li>通常情况下，用户代理就是一个运行在PC上的程序（电子邮件客户端软件），常见的有Outlook和Foxmail等。</li></ul></li><li>邮件服务器<ul><li>它的功能是发送和接收邮件，同时还要向发信人报告邮件传送的情况（已交付、被拒绝、丢失等）。</li><li>邮件服务器采用客户/服务器方式工作，但它必须能够同时充当客户和服务器。<ul><li>例如，当邮件服务器A向邮件服务器B发送邮件时，A就作为SMTP客户，而B是SMTP服务器</li><li>反之，当B向A发送邮件时，B就是SMTP客户，而A就是SMTP服务器</li></ul></li></ul></li><li>邮件发送协议和读取协议<ul><li>SMTP（邮箱发送协议）<ul><li><strong>用于用户代理向邮件服务器发送邮件或在邮件服务器之间发送邮件</strong></li><li>SMTP用的是“推”(Push)的通信方式，即用户代理向邮件服务器发送邮件及在邮件服务器之间发送邮件时，<br>SMTP客户将邮件“推”送到SMTP服务器</li></ul></li><li>POP3（邮件读取协议）<ul><li>用于用户代理从邮件服务器读取邮件</li><li>POP3用的是“拉”(Pull)的通信方式，即用户读取邮件时，用户代理向邮件服务器发出请求，“拉”取用户邮箱中的邮件。</li></ul></li><li>在浏览器中时，HTTP协议也可以发送或接收邮件</li></ul></li></ul></li><li>电子邮件的收发过程<ul><li>发信人调用用户代理来撰写和编辑要发送的邮件。用户代理用SMTP把邮件传送给发送端邮件服务器</li><li>发送端邮件服务器将邮件放入邮件缓存队列中，等待发送</li><li>运行在发送端邮件服务器的SMTP客户进程，发现邮件缓存中有待发送的邮件，<br>就向运行在接收端邮件服务器的SMTP服务器进程发起建立TCP连接</li><li>TCP连接建立后，SMTP客户进程开始向远程SMTP服务器进程发送邮件。<br>当所有待发送邮件发完后，SMTP就关闭所建立的TCP连接</li><li>运行在接收端邮件服务器中的SMTP服务器进程收到邮件后，将邮件放入收信人的用户邮箱，等待收信人在方便时进行读取</li><li>收信人打算收信时，调用用户代理，使用POP3(或IMAP)协议将自己的邮件从接收端邮件服务器的用户邮箱中取回（如果邮箱中有来信的话）</li></ul></li></ul><h4 id="2-电子邮件格式与MIME"><a href="#2-电子邮件格式与MIME" class="headerlink" title="2.电子邮件格式与MIME"></a>2.电子邮件格式与MIME</h4><ul><li>电子邮件格式<ul><li>一个电子邮件分为信封和内容两大部分，邮件内容又分为首部和主体两部分</li><li>邮件内容的首部包含一些首部行，每个首部行由一个关键字后跟冒号再后跟值组成。有些关键字是必需的，有些则是可选的。<br>最重要的关键字是To和Subject.</li><li>To是必需的关键字，后面填入一个或多个收件人的电子邮件地址。电子邮件地址的规定格式为：收件人邮箱名@邮箱所在主机的域名，如abc@cskaoyan.com，其中收信人邮箱名即用户名，在邮件服务器上唯一</li><li>Subject是可选关键字，是邮件的主题，反映了邮件的主要内容</li><li>还有一个必填的关键字是From，但它通常由邮件系统自动填入</li></ul></li><li>多用途网际邮件扩充（MIME）<ul><li>由于SMTP只能传送<strong>7位的ASCI码</strong>邮件，许多其他非英语国家的文字就无法传送，<br>且无法传送可执行文件及其他二进制对象，因此提出了MIME</li><li>MIME并未改动SMTP或取代它。MIME的意图是继续使用目前的格式，但增加了邮件主体的结构，并定义了传送非ASCII码的编码规则，可以转换格式</li><li>MIME主要包括以下三部分内容<ul><li>5个新的邮件首部字段，包括MME版本、内容描述、内容标识、传送编码和内容类型</li><li>定义了许多邮件内容的格式，对多媒体电子邮件的表示方法进行了标准化</li><li>定义了传送编码，可对任何内容格式进行转换，而不会被邮件系统改变</li></ul></li></ul></li></ul><h4 id="3-SMTP和POP3"><a href="#3-SMTP和POP3" class="headerlink" title="3.SMTP和POP3"></a>3.SMTP和POP3</h4><ul><li>简单邮件传输协议 (SMTP)<ul><li>SMTP用的是TCP连接，<strong>端口号为25</strong>，是一种提供可靠且有效的电子邮件传输的协议，<br>它控制两个相互通信的SMTP进程交换信息。</li><li>由于SMTP使用客户/服务器方式，因此负责发送邮件的SMTP进程就是SMTP客户，而负责接收邮件的SMTP进程就是SMTP服务<br>器。</li></ul></li><li>邮局协议(POP)<ul><li>在传输层使用TCP，<strong>端口号为110</strong>，使用客户/服务器的工作方式，是一个非常简单但切能有限的邮件读取协议，<br>现任使用的是它的第3个版本POP3.</li><li>POP3采用的是“拉”(Pull)的通信方式，当用户读取邮件时，用户代理向邮件服务器发出请求，“拉”取用户邮箱中的邮件。<br>接收方的用户代理上必须运行POP客户程序，而接收方的邮件服务器上则运行POP服务器程序。</li><li>POP有两种工作方式：“下载并保留”和“下载并删除”。<ul><li>在“下载并保留”方式下，用户从邮件服务器上读取邮件后，邮件依然会保存在邮件服务器上，用户可再次从服务器上读取该邮件</li><li>而使用“下载并删除”方式时，邮件一旦被读取，就被从邮件服务器上删除，用户不能再次从服务器上读取。</li></ul></li></ul></li></ul><h3 id="五-万维网（www✪）"><a href="#五-万维网（www✪）" class="headerlink" title="五.万维网（www✪）"></a>五.万维网（www✪）</h3><h4 id="1-www的概念与组成结构"><a href="#1-www的概念与组成结构" class="headerlink" title="1.www的概念与组成结构"></a>1.www的概念与组成结构</h4><ul><li><p>万维网是无数个网络站点和网页的集合，它们在一起构成了因特网最主要的部分<br>（因特网也包括电子邮件、Usenet和新闻组）。</p></li><li><p>万维网是一个分布式、联机式的信息存储空间，资源由一个全域“统一资源定位符”(URL)标识</p></li><li>这些资源通过超文本传输协议(HTTP)传送给使用者，而后者通过单击链接来获取资源。</li><li><strong>万维网以客户/服务器方式工作。浏览器是在用户主机上的万维网客户程序，而万维网文档所驻留的主机则运行服务器程序，这台主机称为万维网服务器。</strong><br><strong>客户程序向服务器程序发出请求，服务器程序向客户程序送回客户所要的万维网文档</strong></li><li>万维网的内核部分<ul><li>统一资源定位符(URL)<ul><li>负责标识万维网上的各种文档，并使每个文档在整个万维网的范围内具有唯一的标识符URL</li><li>URL的一般形式是：&lt;协议&gt;://&lt;主机&gt;:&lt;端口&gt;/&lt;路径&gt;</li><li>&lt;协议&gt;指用什么协议来获取万维网文档，常见的协议有http、fp等</li><li>&lt;主机&gt;是存放资源的主机在因特网中的域名或IP地址</li><li>&lt;端口&gt;和&lt;路径&gt;有时可省略。在URL中不区分大小写。</li></ul></li><li>超文本传输协议(HTTP)<ul><li>一个应用层协议，它使用TCP连接进行可靠的传输</li><li>HTTP是万维网客户程序和服务器程序之间交互所必须严格遵守的协议</li></ul></li><li>超文本标记语言(HTML)<ul><li>一种文档结构的标记语言，它使用一些约定的标记对页面上的各种信息(包括文字、声音、图像、视频等)、格式进行描述</li><li>万维网页面的设计者可以很方便地用一个超链接从本页面的某处链接到因特网上的任何一个万维网页面，<br>并能够在自己的计算机屏幕上显示这些页面。</li></ul></li></ul></li></ul><h4 id="2-超文本传输协议（HTTP）"><a href="#2-超文本传输协议（HTTP）" class="headerlink" title="2.超文本传输协议（HTTP）"></a>2.超文本传输协议（HTTP）</h4><ul><li>HTTP的操作过程<ul><li>用户点击超链接或输网址后的过程<ul><li>1.浏览器分析URL</li><li>2.浏览器向DNS请求解析URL的IP地址</li><li>3.DNS解析出IP地址</li><li>4.浏览器与服务器建立TCP连接<ul><li>每个万维网站点都有一个服务器进程，它不断地监听TCP的端口80（默认），<br>当监听到连接请求后便与浏览器建立TCP连接</li></ul></li><li>5.浏览器发出HTTP请求</li><li>6.服务器响应HTTP请求并发送相关文件给浏览器</li><li>7.释放TCP连接</li><li>8.浏览器解释文件并将web页显示给用户</li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/11/pPnG99U.png" alt="pPnG99U.png"></li></ul></li></ul></li><li>HTTP的特点<ul><li>HTTP使用TCP作为传输层协议，保证了数据的可靠传输。但HTTP本身是无连接的，通信的双方在交换HTTP报文之前不需要先建立HTTP连接。</li><li>HTTP是无状态的，可使用Cookie（存储在用户主机中的文本文件），通过识别码记录一段时间内某用户的访问记录</li><li>HTTP的两种连接方式<ul><li>非持久连接（HTTP/1.0只能使用非持续连接）<ul><li>对于非持久连接，每个网页元素对象 (如JPEG图形、Flash等) 的传输都需要单独建立一个TCP连接<br><strong>第三次握手的报文段中捎带了客户对万维网文档的请求</strong></li><li>请求一个万维网文档所需的时间是该文档的传输时间（与文档大小成正比）加上两倍往返时间RTT</li><li>每个对象引用都导致2×RTT的开销，此外每次建立新的TCP连接都要分配缓存和变量，使万维网服务器的负担很重。</li><li>打开一个包含100张图片的web界面，需要打开和关闭101次TCP连接</li></ul></li><li>持久连接（持续连接为HTTP/1.1的默认方式）<ul><li>万维网服务器在发送响应后仍然保持这条连接，使同一个客户（浏览器）和该服务器可以继续在这条连接上传送后续的HTTP请求和响应报文</li><li>分为了非流水线方式和流水线方式<ul><li>非流水线方式<ul><li>对于非流水线方式，客户在收到前一个响应后才能发出下一个请求（相当于浏览器要接收确认）<br>服务器发送完一个对象后，其TCP连接就处于空闲状态，浪费了服务器资源</li></ul></li><li>流水线方式<ul><li><strong>HTTP/1.1的默认方式是使用流水线的持久连接</strong>，客户每遇到一个对象引用就立即发出一个请求，<br>因而客户可以逐个地连续发出对各个引用对象的请求。</li><li>如果所有的请求和响应都是连续发送的，那么所有引用的对象共计经历1个RTT延迟，<br>而不是像非流水线方式那样，每个引用都必须有1个RTT延迟</li><li>这种方式减少了TCP连接中的空闲时间，提高了效率。</li></ul></li></ul></li></ul></li><li>图片<ul><li><img src="https://s1.ax1x.com/2023/08/11/pPnJiPf.png" alt="pPnJiPf.png"></li></ul></li></ul></li></ul></li><li>HTTP的报文结构<ul><li>HTTP是面向文本的，因此报文中的每个字段都是一些ASCII码串，并且每个字段的长度都是不确定的。</li><li>分为请求报文（从客户向服务器发送的请求报文）和响应报文（从服务器到客户的回答）</li><li><img src="https://s1.ax1x.com/2023/08/11/pPnh6XQ.png" alt="pPnh6XQ.png"></li></ul></li><li>例题<ul><li><img src="https://s1.ax1x.com/2023/08/12/pPuSZe1.png" alt="pPuSZe1.png"><ul><li>答案：本题还要考虑慢开始的拥塞控制算法<img src="https://s1.ax1x.com/2023/08/12/pPuSdfS.png" alt="pPuSdfS.png"></li></ul></li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;计算机网络第六章-应用层&quot;&gt;&lt;a href=&quot;#计算机网络第六章-应用层&quot; class=&quot;headerlink&quot; title=&quot;计算机网络第六章 应用层&quot;&gt;&lt;/a&gt;计算机网络第六章 应用层&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;计算机学科基础：计算机网络第六章应用层的学习笔记&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="计算机基础学习笔记" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机网络" scheme="http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
